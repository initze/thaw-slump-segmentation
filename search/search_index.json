{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Thaw Slump Segmentation","text":"<p>Package Reference</p> <p>Scripts</p>"},{"location":"scripts/","title":"Scripts","text":"<p>Usecase 2 Data Preprocessing Script</p> <p>Usecase 2 Data Preprocessing Script</p> <p>Usecase 2 Training Script</p>"},{"location":"scripts/#src.thaw_slump_segmentation.scripts.setup_raw_data.STATUS","title":"<code>STATUS = {0: 'failed', 1: 'success', 2: 'skipped'}</code>  <code>module-attribute</code>","text":""},{"location":"scripts/#src.thaw_slump_segmentation.scripts.setup_raw_data.SUCCESS_STATES","title":"<code>SUCCESS_STATES = ['rename', 'label', 'ndvi', 'tcvis', 'rel_dem', 'slope', 'mask', 'move']</code>  <code>module-attribute</code>","text":""},{"location":"scripts/#src.thaw_slump_segmentation.scripts.setup_raw_data.is_ee_initialized","title":"<code>is_ee_initialized = False</code>  <code>module-attribute</code>","text":""},{"location":"scripts/#src.thaw_slump_segmentation.scripts.setup_raw_data.main","title":"<code>main()</code>","text":"Source code in <code>src/thaw_slump_segmentation/scripts/setup_raw_data.py</code> <pre><code>def main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--gdal_bin', default=None, help='Path to gdal binaries (ignored if --skip_gdal is passed)')\n    parser.add_argument('--gdal_path', default=None, help='Path to gdal scripts (ignored if --skip_gdal is passed)')\n    parser.add_argument('--n_jobs', default=-1, type=int, help='number of parallel joblib jobs')\n    parser.add_argument('--nolabel', action='store_false', help='Set flag to do preprocessing without label file')\n    parser.add_argument('--data_dir', default='data', type=Path, help='Path to data processing dir')\n    parser.add_argument('--log_dir', default='logs', type=Path, help='Path to log dir')\n\n    args = parser.parse_args()\n\n    setup_raw_data(\n        gdal_bin=args.gdal_bin,\n        gdal_path=args.gdal_path,\n        n_jobs=args.n_jobs,\n        label=args.nolabel,\n        data_dir=args.data_dir,\n        log_dir=args.log_dir,\n    )\n</code></pre>"},{"location":"scripts/#src.thaw_slump_segmentation.scripts.setup_raw_data.preprocess_directory","title":"<code>preprocess_directory(image_dir, data_dir, aux_dir, backup_dir, log_path, gdal_bin, gdal_path, label_required=True)</code>","text":"<p>Preprocess a folder of a source imagery dataset to use in the inference processing. Notably this method generates additional data files to be used as auxiliary input of the inference model: DEM derived data (relative elevation and slope), NDVI and the landsat trend data (tcvis).</p> <p>In particular, the extend of the imagery in image_dir is used to generate a regional excerpt of the other data from the aux_dir as well as Google Earth Engine and the input data itself. The regional excerpts are stored as GeoTIFF alongside the source data in the data_dir folder.</p> <p>Additionally, some data is copied to <code>backup_dir</code>.</p> <p>GDAL is heavily used in the preprocessing, so the paths to a working gdal environment are required,  usually the platform binaries or installed via conda.</p> <p>To accesss data in Google Earth Engine this method will authenticate with the Google Earth Engine API, so an active account at GEE is required.</p> <p>The method returns a dictionary describing the success states of each processing stage. The keys are: * rename * label * ndvi * tcvis * rel_dem * slope * mask * move</p> <p>Parameters:</p> Name Type Description Default <code>image_dir</code> <code>Path</code> <p>The source folder</p> required <code>data_dir</code> <code>Path</code> <p>The folder where to store the preprocessed dataset</p> required <code>aux_dir</code> <code>Path</code> <p>source of auxiliary data, for now the DEM VRT files</p> required <code>backup_dir</code> <code>Path</code> <p>path to a folder where to store backups of the input data</p> required <code>log_path</code> <code>Path</code> <p>path where to write logging</p> required <code>gdal_bin</code> <code>Path</code> <p>Path to a folder with the gdal_binaries (gdalwarp etc.)</p> required <code>gdal_path</code> <code>_type_</code> <p>Path to a folder with gdal_scripts (gdal_retile.py etc)</p> required <code>label_required</code> <code>bool</code> <p>If label data should be baked into the source data. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>a dictionary with the results of the individual processing stages</p> Source code in <code>src/thaw_slump_segmentation/scripts/setup_raw_data.py</code> <pre><code>def preprocess_directory(image_dir, data_dir, aux_dir, backup_dir, log_path, gdal_bin, gdal_path, label_required=True):\n    \"\"\"Preprocess a folder of a source imagery dataset to use in the inference processing.\n    Notably this method generates additional data files to be used as auxiliary input of the inference\n    model: DEM derived data (relative elevation and slope), NDVI and the landsat trend data (tcvis).\n\n    In particular, the extend of the imagery in image_dir is used to generate a regional excerpt of the\n    other data from the aux_dir as well as Google Earth Engine and the input data itself. The regional\n    excerpts are stored as GeoTIFF alongside the source data in the data_dir folder.\n\n    Additionally, some data is copied to `backup_dir`.\n\n    GDAL is heavily used in the preprocessing, so the paths to a working gdal environment are required, \n    usually the platform binaries or installed via conda.\n\n    To accesss data in Google Earth Engine this method will authenticate with the Google Earth Engine API,\n    so an active account at GEE is required.\n\n    The method returns a dictionary describing the success states of each processing stage. The keys are:\n    * rename\n    * label\n    * ndvi\n    * tcvis\n    * rel_dem\n    * slope\n    * mask\n    * move\n\n    Args:\n        image_dir (Path): The source folder\n        data_dir (Path): The folder where to store the preprocessed dataset\n        aux_dir (Path): source of auxiliary data, for now the DEM VRT files\n        backup_dir (Path): path to a folder where to store backups of the input data\n        log_path (Path): path where to write logging\n        gdal_bin (Path): Path to a folder with the gdal_binaries (gdalwarp etc.)\n        gdal_path (_type_): Path to a folder with gdal_scripts (gdal_retile.py etc)\n        label_required (bool, optional): If label data should be baked into the source data. Defaults to True.\n\n    Returns:\n        dict: a dictionary with the results of the individual processing stages\n    \"\"\"\n\n    # Mock old args object\n    gdal.initialize(bin=gdal_bin, path=gdal_path)\n\n    init_logging(log_path)\n    image_name = os.path.basename(image_dir)\n    thread_logger = get_logger(f'setup_raw_data.{image_name}')\n    data_pre_processing.earthengine._logger = get_logger(f'setup_raw_data.{image_name}.ee')\n    data_pre_processing.utils._logger = get_logger(f'setup_raw_data.{image_name}.utils')\n\n    global is_ee_initialized\n    if not is_ee_initialized:\n        try:\n            thread_logger.debug('Initializing Earth Engine')\n            ee.Initialize()\n        except Exception:\n            thread_logger.warn('Initializing Earth Engine failed, trying to authenticate')\n            ee.Authenticate()\n            ee.Initialize()\n        is_ee_initialized = True\n    success_state = dict(rename=0, label=0, ndvi=0, tcvis=0, rel_dem=0, slope=0, mask=0, move=0)\n    thread_logger.info(f'Starting preprocessing {image_name}')\n\n    pre_cleanup(image_dir)\n\n    success_state['rename'] = rename_clip_to_standard(image_dir)\n\n    if not has_projection(image_dir):\n        thread_logger.error('Input File has no valid Projection!')\n        return\n\n    if label_required:\n        success_state['label'] = vector_to_raster_mask(image_dir)\n    else:\n        success_state['label'] = 2\n\n    success_state['ndvi'] = make_ndvi_file(image_dir)\n\n    ee_image_tcvis = ee.ImageCollection('users/ingmarnitze/TCTrend_SR_2000-2019_TCVIS').mosaic()\n    success_state['tcvis'] = get_tcvis_from_gee(image_dir, ee_image_tcvis, out_filename='tcvis.tif', resolution=3)\n\n    success_state['rel_dem'] = aux_data_to_tiles(\n        image_dir, aux_dir / 'ArcticDEM' / 'elevation.vrt', 'relative_elevation.tif'\n    )\n\n    success_state['slope'] = aux_data_to_tiles(image_dir, aux_dir / 'ArcticDEM' / 'slope.vrt', 'slope.tif')\n\n    success_state['mask'] = mask_input_data(image_dir, data_dir)\n\n    # backup_dir_full = os.path.join(backup_dir, os.path.basename(image_dir))\n    if backup_dir is not None:\n        backup_dir_full = backup_dir / image_dir.name\n        success_state['move'] = move_files(image_dir, backup_dir_full)\n\n    for status in SUCCESS_STATES:\n        thread_logger.info(status + ': ' + STATUS[success_state[status]])\n    return success_state\n</code></pre>"},{"location":"scripts/#src.thaw_slump_segmentation.scripts.setup_raw_data.setup_raw_data","title":"<code>setup_raw_data(gdal_bin='/usr/bin', gdal_path='/usr/bin', n_jobs=-1, label=False, data_dir=Path('data'), log_dir=Path('logs'))</code>","text":"Source code in <code>src/thaw_slump_segmentation/scripts/setup_raw_data.py</code> <pre><code>def setup_raw_data(\n    gdal_bin: Annotated[str, typer.Option('--gdal_bin', help='Path to gdal binaries', envvar='GDAL_BIN')] = '/usr/bin',\n    gdal_path: Annotated[\n        str, typer.Option('--gdal_path', help='Path to gdal scripts', envvar='GDAL_PATH')\n    ] = '/usr/bin',\n    n_jobs: Annotated[int, typer.Option('--n_jobs', help='number of parallel joblib jobs, pass 0 to disable joblib')] = -1,\n    label: Annotated[\n        bool, typer.Option('--label/--nolabel', help='Set flag to do preprocessing with label file')\n    ] = False,\n    data_dir: Annotated[Path, typer.Option('--data_dir', help='Path to data processing dir')] = Path('data'),\n    log_dir: Annotated[Path, typer.Option('--log_dir', help='Path to log dir')] = Path('logs'),\n):\n    INPUT_DATA_DIR = data_dir / 'input'\n    BACKUP_DIR = data_dir / 'backup'\n    DATA_DIR = data_dir / 'tiles'\n    AUX_DIR = data_dir / 'auxiliary'\n\n    timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n    log_path = Path(log_dir) / f'setup_raw_data-{timestamp}.log'\n    if not Path(log_dir).exists():\n        os.mkdir(Path(log_dir))\n    init_logging(log_path)\n    logger = get_logger('setup_raw_data')\n    logger.info('###########################')\n    logger.info('# Starting Raw Data Setup #')\n    logger.info('###########################')\n\n    logger.info(f\"images to preprocess in {INPUT_DATA_DIR}\")\n    dir_list = check_input_data(INPUT_DATA_DIR)\n    [logger.info(f\" * '{f.relative_to(INPUT_DATA_DIR)}'\") for f in dir_list ]\n    if len(dir_list) &gt; 0:\n        if n_jobs == 0:\n            for image_dir in dir_list:\n                preprocess_directory(image_dir, DATA_DIR, AUX_DIR, BACKUP_DIR, log_path, gdal_bin, gdal_path, label)\n        else:\n            Parallel(n_jobs=n_jobs)(\n                delayed(preprocess_directory)(\n                    image_dir, DATA_DIR, AUX_DIR, BACKUP_DIR, log_path, gdal_bin, gdal_path, label\n                )\n                for image_dir in dir_list\n        )\n    else:\n        logger.error('Empty Input Data Directory! No Data available to process!')\n</code></pre>"},{"location":"scripts/#src.thaw_slump_segmentation.scripts.prepare_data.RASTERFILTER","title":"<code>RASTERFILTER = '*_SR*.tif'</code>  <code>module-attribute</code>","text":""},{"location":"scripts/#src.thaw_slump_segmentation.scripts.prepare_data.VECTORFILTER","title":"<code>VECTORFILTER = '*.shp'</code>  <code>module-attribute</code>","text":""},{"location":"scripts/#src.thaw_slump_segmentation.scripts.prepare_data.args","title":"<code>args = parser.parse_args()</code>  <code>module-attribute</code>","text":""},{"location":"scripts/#src.thaw_slump_segmentation.scripts.prepare_data.parser","title":"<code>parser = argparse.ArgumentParser(description='Make data ready for training', formatter_class=argparse.ArgumentDefaultsHelpFormatter)</code>  <code>module-attribute</code>","text":""},{"location":"scripts/#src.thaw_slump_segmentation.scripts.prepare_data.do_gdal_calls","title":"<code>do_gdal_calls(DATASET, xsize, ysize, overlap, aux_data=['ndvi', 'tcvis', 'slope', 'relative_elevation'], logger=None)</code>","text":"Source code in <code>src/thaw_slump_segmentation/scripts/prepare_data.py</code> <pre><code>def do_gdal_calls(\n    DATASET,\n    xsize: int,\n    ysize: int,\n    overlap: int,\n    aux_data=['ndvi', 'tcvis', 'slope', 'relative_elevation'],\n    logger=None,\n):\n    rasterfile = glob_file(DATASET, RASTERFILTER, logger)\n    image_name = rasterfile.name[0:-7]\n\n    maskfile = DATASET / f'{image_name}_mask.tif'\n\n    tile_dir_data = DATASET / 'tiles' / 'data'\n    tile_dir_mask = DATASET / 'tiles' / 'mask'\n\n    # Create parents on the first data folder\n    tile_dir_data.mkdir(exist_ok=True, parents=True)\n    tile_dir_mask.mkdir(exist_ok=True)\n\n\n    # Retile data, mask\n    # log_run(f'python {gdal.retile} -ps {XSIZE} {YSIZE} -overlap {OVERLAP} -targetDir {tile_dir_data} {rasterfile}', logger)\n    # log_run(f'python {gdal.retile} -ps {XSIZE} {YSIZE} -overlap {OVERLAP} -targetDir {tile_dir_mask} {maskfile}', logger)\n    log_run(f'{gdal.retile} -ps {xsize} {ysize} -overlap {overlap} -targetDir {tile_dir_data} {rasterfile}', logger)\n    log_run(f'{gdal.retile} -ps {xsize} {ysize} -overlap {overlap} -targetDir {tile_dir_mask} {maskfile}', logger)\n\n    # Retile additional data\n    for aux in aux_data:\n        auxfile = DATASET / f'{aux}.tif'\n        tile_dir_aux = DATASET / 'tiles' / aux\n        tile_dir_aux.mkdir(exist_ok=True)\n        # log_run(f'python {gdal.retile} -ps {XSIZE} {YSIZE} -overlap {OVERLAP} -targetDir {tile_dir_aux} {auxfile}', logger)\n        log_run(f'{gdal.retile} -ps {xsize} {ysize} -overlap {overlap} -targetDir {tile_dir_aux} {auxfile}', logger)\n</code></pre>"},{"location":"scripts/#src.thaw_slump_segmentation.scripts.prepare_data.get_planet_product_type","title":"<code>get_planet_product_type(img_path)</code>","text":"<p>return if file is scene or OrthoTile</p> Source code in <code>src/thaw_slump_segmentation/scripts/prepare_data.py</code> <pre><code>def get_planet_product_type(img_path):\n    \"\"\"\n    return if file is scene or OrthoTile\"\"\"\n    split = img_path.stem.split('_')\n    # check if 4th last imagename segment is BGRN\n    if split[-4] == 'BGRN':\n        pl_type = 'OrthoTile'\n    elif len(split) == 6 and len(split[2]) == 6:\n        pl_type = \"Sentinel2\"\n    else:\n        pl_type = 'Scene'\n\n    return pl_type\n</code></pre>"},{"location":"scripts/#src.thaw_slump_segmentation.scripts.prepare_data.glob_file","title":"<code>glob_file(DATASET, filter_string, logger=None)</code>","text":"Source code in <code>src/thaw_slump_segmentation/scripts/prepare_data.py</code> <pre><code>def glob_file(DATASET, filter_string, logger=None):\n    candidates = list(DATASET.glob(f'{filter_string}'))\n    if len(candidates) == 1:\n        logger.debug(f'Found file: {candidates[0]}')\n        return candidates[0]\n    else:\n        raise ValueError(f'Found {len(candidates)} candidates.' 'Please make selection more specific!')\n</code></pre>"},{"location":"scripts/#src.thaw_slump_segmentation.scripts.prepare_data.main_function","title":"<code>main_function(dataset, log_path, h5dir, xsize, ysize, overlap, threshold, skip_gdal, gdal_bin, gdal_path)</code>","text":"Source code in <code>src/thaw_slump_segmentation/scripts/prepare_data.py</code> <pre><code>def main_function(\n    dataset, log_path, h5dir: Path, xsize: int, ysize: int, overlap: int, threshold: float, skip_gdal: bool, gdal_bin:str, gdal_path:str\n):\n    init_logging(log_path)\n    thread_logger = get_logger(f'prepare_data.{dataset.name}')\n    thread_logger.info(f'Starting preparation on dataset {dataset}')\n    if not skip_gdal:\n        gdal.initialize(bin=gdal_bin, path=gdal_bin)\n        thread_logger.info('Doing GDAL Calls')\n        do_gdal_calls(dataset, xsize, ysize, overlap, logger=thread_logger)\n    else:\n        thread_logger.info('Skipping GDAL Calls')\n\n    tifs = list(sorted(dataset.glob('tiles/data/*.tif')))\n    if len(tifs) == 0:\n        thread_logger.warning(f'No tiles found for {dataset}, skipping this directory.')\n        return\n\n    h5_path = h5dir / f'{dataset.name}.h5'\n    info_dir = h5dir / dataset.name\n    info_dir.mkdir(parents=True)\n\n    thread_logger.info(f'Creating H5File at {h5_path}')\n    h5 = h5py.File(\n        h5_path,\n        'w',\n        rdcc_nbytes=2 * (1 &lt;&lt; 30),  # 2 GiB\n        rdcc_nslots=200003,\n    )\n    channel_numbers = dict(planet=4, ndvi=1, tcvis=3, relative_elevation=1, slope=1)\n\n    datasets = dict()\n    for dataset_name, nchannels in channel_numbers.items():\n        ds = h5.create_dataset(\n            dataset_name,\n            dtype=np.float32,\n            shape=(len(tifs), nchannels, xsize, ysize),\n            maxshape=(len(tifs), nchannels, xsize, ysize),\n            chunks=(1, nchannels, xsize, ysize),\n            compression='lzf',\n            scaleoffset=3,\n        )\n        datasets[dataset_name] = ds\n\n    datasets['mask'] = h5.create_dataset(\n        'mask',\n        dtype=np.uint8,\n        shape=(len(tifs), 1, xsize, ysize),\n        maxshape=(len(tifs), 1, xsize, ysize),\n        chunks=(1, 1, xsize, ysize),\n        compression='lzf',\n    )\n\n    # Convert data to HDF5 storage for efficient data loading\n    i = 0\n    bad_tiles = 0\n    for img in tifs:\n        tile = {}\n        with rio.open(img) as raster:\n            tile['planet'] = raster.read()\n\n        if (tile['planet'] == 0).all(axis=0).mean() &gt; threshold:\n            bad_tiles += 1\n            continue\n\n        with rio.open(mask_from_img(img)) as raster:\n            tile['mask'] = raster.read()\n        assert tile['mask'].max() &lt;= 1, \"Mask can't contain values &gt; 1\"\n\n        for other in channel_numbers:\n            if other == 'planet':\n                continue  # We already did this!\n            with rio.open(other_from_img(img, other)) as raster:\n                data = raster.read()\n            if data.shape[0] &gt; channel_numbers[other]:\n                # This is for tcvis mostly\n                data = data[: channel_numbers[other]]\n            tile[other] = data\n\n        # gdal_retile leaves narrow stripes at the right and bottom border,\n        # which are filtered out here:\n        is_narrow = False\n        for tensor in tile.values():\n            if tensor.shape[-2:] != (xsize, ysize):\n                is_narrow = True\n                break\n        if is_narrow:\n            bad_tiles += 1\n            continue\n\n        for t in tile:\n            datasets[t][i] = tile[t]\n\n        make_info_picture(tile, info_dir / f'{i}.jpg')\n        i += 1\n\n    for t in datasets:\n        datasets[t].resize(i, axis=0)\n</code></pre>"},{"location":"scripts/#src.thaw_slump_segmentation.scripts.prepare_data.make_info_picture","title":"<code>make_info_picture(tile, filename)</code>","text":"<p>Make overview picture</p> Source code in <code>src/thaw_slump_segmentation/scripts/prepare_data.py</code> <pre><code>def make_info_picture(tile, filename):\n    \"Make overview picture\"\n    rgbn = np.clip(tile['planet'][:4, :, :].transpose(1, 2, 0) / 3000 * 255, 0, 255).astype(np.uint8)\n    tcvis = np.clip(tile['tcvis'].transpose(1, 2, 0), 0, 255).astype(np.uint8)\n\n    rgb = rgbn[:, :, :3]\n    nir = rgbn[:, :, [3, 2, 1]]\n    mask = (tile['mask'][[0, 0, 0]].transpose(1, 2, 0) * 255).astype(np.uint8)\n\n    img = np.concatenate(\n        [\n            np.concatenate([rgb, nir], axis=1),\n            np.concatenate([tcvis, mask], axis=1),\n        ]\n    )\n    imsave(filename, img)\n</code></pre>"},{"location":"scripts/#src.thaw_slump_segmentation.scripts.prepare_data.mask_from_img","title":"<code>mask_from_img(img_path)</code>","text":"<p>Given an image path, return path for the mask</p> Source code in <code>src/thaw_slump_segmentation/scripts/prepare_data.py</code> <pre><code>def mask_from_img(img_path):\n    \"\"\"\n    Given an image path, return path for the mask\n    \"\"\"\n    # change for\n    product_type = get_planet_product_type(img_path)\n    base = img_path.parent.parent\n\n    if product_type == 'Scene':\n        date, time, *block, platform, _, sr, row, col = img_path.stem.split('_')\n        block = '_'.join(block)\n        mask_path = base / 'mask' / f'{date}_{time}_{block}_mask_{row}_{col}.tif'\n    elif product_type == \"Sentinel2\":\n        date, processing_date, tile_id, _, row, col = img_path.stem.split('_')\n        mask_path = base / 'mask' / f'{date}_{processing_date}_{tile_id}_mask_{row}_{col}.tif'\n    else:\n        block, tile, date, sensor, bgrn, sr, row, col = img_path.stem.split('_')\n        mask_path = base / 'mask' / f'{block}_{tile}_{date}_{sensor}_mask_{row}_{col}.tif'\n\n    assert mask_path.exists()\n\n    return mask_path\n</code></pre>"},{"location":"scripts/#src.thaw_slump_segmentation.scripts.prepare_data.other_from_img","title":"<code>other_from_img(img_path, other)</code>","text":"<p>Given an image path, return paths for mask and tcvis</p> Source code in <code>src/thaw_slump_segmentation/scripts/prepare_data.py</code> <pre><code>def other_from_img(img_path, other):\n    \"\"\"\n    Given an image path, return paths for mask and tcvis\n    \"\"\"\n    product_type = get_planet_product_type(img_path)\n    if product_type == 'Scene':\n        date, time, *block, platform, _, sr, row, col = img_path.stem.split('_')\n    elif product_type == \"Sentinel2\":\n        date, processing_date, tile_id, _, row, col = img_path.stem.split('_')\n    else:\n        block, tile, date, sensor, bgrn, sr, row, col = img_path.stem.split('_')\n\n    base = img_path.parent.parent\n\n    path = base / other / f'{other}_{row}_{col}.tif'\n    assert path.exists()\n\n    return path\n</code></pre>"},{"location":"scripts/#src.thaw_slump_segmentation.scripts.prepare_data.prepare_data","title":"<code>prepare_data(data_dir=Path('data'), log_dir=Path('logs'), skip_gdal=False, gdal_bin='/usr/bin', gdal_path='/usr/bin', n_jobs=-1, nodata_threshold=50, tile_size='256x256', tile_overlap=25)</code>","text":"<p>Make data ready for training</p> Source code in <code>src/thaw_slump_segmentation/scripts/prepare_data.py</code> <pre><code>def prepare_data(\n    data_dir: Annotated[Path, typer.Option('--data_dir', help='Path to data processing dir')] = Path('data'),\n    log_dir: Annotated[Path, typer.Option('--log_dir', help='Path to log dir')] = Path('logs'),\n    skip_gdal: Annotated[\n        bool,\n        typer.Option('--skip_gdal/--use_gdal', help='Skip the Gdal conversion stage (if it has already been done)'),\n    ] = False,\n    gdal_bin: Annotated[\n        str,\n        typer.Option('--gdal_bin', help='Path to gdal binaries (ignored if --skip_gdal is passed)', envvar='GDAL_BIN'),\n    ] = '/usr/bin',\n    gdal_path: Annotated[\n        str,\n        typer.Option('--gdal_path', help='Path to gdal scripts (ignored if --skip_gdal is passed)', envvar='GDAL_PATH'),\n    ] = '/usr/bin',\n    n_jobs: Annotated[int, typer.Option('--n_jobs', help='number of parallel joblib jobs')] = -1,\n    nodata_threshold: Annotated[\n        float, typer.Option('--nodata_threshold', help='Throw away data with at least this % of nodata pixels')\n    ] = 50,\n    tile_size: Annotated[\n        str, typer.Option('--tile_size', help=\"Tiling size in pixels e.g. '256x256'\", callback=tile_size_callback)\n    ] = '256x256',\n    tile_overlap: Annotated[int, typer.Option('--tile_overlap', help='Overlap of the tiles in pixels')] = 25,\n):\n    \"\"\"Make data ready for training\"\"\"\n\n    # Tiling Settings\n    if isinstance(tile_size, tuple):\n        xsize, ysize = tile_size\n    else:\n        xsize, ysize = map(int, tile_size.split('x'))\n    overlap = tile_overlap\n\n    timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n    log_path = log_dir / f'prepare_data-{timestamp}.log'\n    log_dir.mkdir(exist_ok=True)\n    init_logging(log_path)\n    logger = get_logger('prepare_data')\n    logger.info('#############################')\n    logger.info('# Starting Data Preparation #')\n    logger.info('#############################')\n\n    threshold = nodata_threshold / 100\n\n    DATA_ROOT = data_dir\n    DATA_DIR = DATA_ROOT / 'tiles'\n    h5dir = DATA_ROOT / 'h5'\n    h5dir.mkdir(exist_ok=True)\n\n    # All folders that contain the big raster (...AnalyticsML_SR.tif) are assumed to be a dataset\n    datasets = [raster.parent for raster in DATA_DIR.glob('*/' + RASTERFILTER)]\n\n    overwrite_conflicts = []\n    for dataset in datasets:\n        check_dir = h5dir / dataset.name\n        if check_dir.exists():\n            overwrite_conflicts.append(check_dir)\n\n    if overwrite_conflicts:\n        logger.warning(f\"Found old data directories: {', '.join(dir.name for dir in overwrite_conflicts)}.\")\n        decision = input('Delete and recreate them [d], skip them [s] or abort [a]? ').lower()\n        if decision == 'd':\n            logger.info('User chose to delete old directories.')\n            for old_dir in overwrite_conflicts:\n                shutil.rmtree(old_dir)\n        elif decision == 's':\n            logger.info('User chose to skip old directories.')\n            already_done = [d.name for d in overwrite_conflicts]\n            datasets = [d for d in datasets if d.name not in already_done]\n        else:\n            # When in doubt, don't overwrite/change anything to be safe\n            logger.error('Aborting due to conflicts with existing data directories.')\n            sys.exit(1)\n\n    Parallel(n_jobs=n_jobs)(\n        delayed(main_function)(dataset, log_path, h5dir, xsize, ysize, overlap, threshold, skip_gdal, gdal_bin, gdal_path)\n        for dataset in datasets\n    )\n</code></pre>"},{"location":"scripts/#src.thaw_slump_segmentation.scripts.prepare_data.read_and_assert_imagedata","title":"<code>read_and_assert_imagedata(image_path)</code>","text":"Source code in <code>src/thaw_slump_segmentation/scripts/prepare_data.py</code> <pre><code>def read_and_assert_imagedata(image_path):\n    with rio.open(image_path) as raster:\n        if raster.count &lt;= 3:\n            data = raster.read()\n        else:\n            data = raster.read()[:3]\n        # Assert data can safely be coerced to int16\n        assert data.max() &lt; 2**15\n        return data\n</code></pre>"},{"location":"scripts/#src.thaw_slump_segmentation.scripts.prepare_data.tile_size_callback","title":"<code>tile_size_callback(value)</code>","text":"Source code in <code>src/thaw_slump_segmentation/scripts/prepare_data.py</code> <pre><code>def tile_size_callback(value: str):\n    try:\n        x, y = map(int, value.split('x'))\n    except ValueError:\n        raise typer.BadParameter('Tile size must be in the format \"256x256\"')\n    return x, y\n</code></pre>"},{"location":"scripts/#src.thaw_slump_segmentation.scripts.train.args","title":"<code>args = parser.parse_args()</code>  <code>module-attribute</code>","text":""},{"location":"scripts/#src.thaw_slump_segmentation.scripts.train.parser","title":"<code>parser = argparse.ArgumentParser(description='Training script', formatter_class=argparse.ArgumentDefaultsHelpFormatter)</code>  <code>module-attribute</code>","text":""},{"location":"scripts/#src.thaw_slump_segmentation.scripts.train.Engine","title":"<code>Engine</code>","text":"Source code in <code>src/thaw_slump_segmentation/scripts/train.py</code> <pre><code>class Engine:\n    def __init__(\n        self,\n        config: Path,\n        data_dir: Path,\n        name: str,\n        log_dir: Path,\n        resume: str,\n        summary: bool,\n        wandb_project: str,\n        wandb_name: str,\n    ):\n        self.config = yaml.load(config.open(), Loader=yaml_custom.SaneYAMLLoader)\n        self.DATA_ROOT = data_dir\n        # Logging setup\n        timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n        if name:\n            log_dir_name = f'{name}_{timestamp}'\n        else:\n            log_dir_name = timestamp\n        self.log_dir = Path(log_dir) / log_dir_name\n        self.log_dir.mkdir(exist_ok=False)\n\n        init_logging(self.log_dir / 'train.log')\n        self.logger = get_logger('train')\n\n        self.data_sources = DataSources(self.config['data_sources'])\n        self.config['model']['input_channels'] = sum(src.channels for src in self.data_sources)\n\n        m = self.config['model']\n        self.model = create_model(\n            arch=m['architecture'],\n            encoder_name=m['encoder'],\n            encoder_weights=None if m['encoder_weights'] == 'random' else m['encoder_weights'],\n            classes=1,\n            in_channels=m['input_channels'],\n        )\n\n        # make parallel\n        self.model = nn.DataParallel(self.model)\n\n        if resume:\n            self.config['resume'] = resume\n\n        if 'resume' in self.config and self.config['resume']:\n            checkpoint = Path(self.config['resume'])\n            if not checkpoint.exists():\n                raise ValueError(f\"There is no Checkpoint at {self.config['resume']} to resume from!\")\n            if checkpoint.is_dir():\n                # Load last checkpoint in run dir\n                ckpt_nums = [int(ckpt.stem) for ckpt in checkpoint.glob('checkpoints/*.pt')]\n                last_ckpt = max(ckpt_nums)\n                self.config['resume'] = str(checkpoint / 'checkpoints' / f'{last_ckpt:02d}.pt')\n            self.logger.info(f\"Resuming training from checkpoint {self.config['resume']}\")\n            self.model.load_state_dict(torch.load(self.config['resume']))\n\n        self.dev = torch.device('cpu') if not torch.cuda.is_available() else torch.device('cuda')\n        self.logger.info(f'Training on {self.dev} device')\n\n        self.model = self.model.to(self.dev)\n        self.model = torch.compile(self.model, mode='max-autotune', fullgraph=True)\n\n        self.learning_rate = self.config['learning_rate']\n        self.opt = torch.optim.AdamW(self.model.parameters(), lr=self.learning_rate)\n        self.setup_lr_scheduler()\n\n        self.board_idx = 0\n        self.epoch = 0\n\n        if summary:\n            from torchsummary import summary\n\n            summary(self.model, [(self.config['model']['input_channels'], 256, 256)])\n            sys.exit(0)\n\n        self.dataset_cache = {}\n\n        self.vis_predictions = None\n        self.vis_loader, self.vis_names = get_vis_loader(\n            self.config['visualization_tiles'],\n            batch_size=self.config['batch_size'],\n            data_sources=self.data_sources,\n            data_root=self.DATA_ROOT,\n        )\n\n        # Write the config YML to the run-folder\n        self.config['run_info'] = {\n            'timestamp': timestamp,\n            'git_head': subprocess.check_output(['git', 'describe'], encoding='utf8').strip(),\n        }\n        with open(self.log_dir / 'config.yml', 'w') as f:\n            yaml.dump(self.config, f)\n\n        self.checkpoints = self.log_dir / 'checkpoints'\n        self.checkpoints.mkdir()\n\n        # Weights and Biases initialization\n        print('wandb project:', wandb_project)\n        print('wandb name:', wandb_name)\n        print('config:', self.config)\n        print('entity:', 'ml4earth')\n        wandb.init(project=wandb_project, name=wandb_name, config=self.config, entity='ingmarnitze_team')\n\n    def run(self):\n        for phase in self.config['schedule']:\n            self.logger.info(f'Starting phase \"{phase[\"phase\"]}\"')\n\n            self.phase_should_log_images = phase.get('log_images', False)\n            self.setup_metrics(phase)\n            self.current_key = None\n            for epoch in range(phase['epochs']):\n                # Epoch setup\n                self.epoch = epoch\n                self.loss_function = create_loss(scoped_get('loss_function', phase, self.config)).to(self.dev)\n\n                for step in phase['steps']:\n                    command, key = parse_step(step)\n                    self.current_key = key\n\n                    if command == 'train_on':\n                        # Training step\n                        data_loader = self.get_dataloader(key)\n                        self.train_epoch(data_loader)\n                    elif command == 'validate_on':\n                        # Validation step\n                        data_loader = self.get_dataloader(key)\n                        self.val_epoch(data_loader)\n                    elif command == 'log_images':\n                        self.logger.warn(\n                            \"Step 'log_images' is deprecated. Please use 'log_images' in the phase instead.\"\n                        )\n                        continue\n                    else:\n                        raise ValueError(f\"Unknown command '{command}'\")\n\n                    # Reset the metrics after each step\n                    if self.current_key:\n                        self.metrics[self.current_key].reset()\n                        gc.collect()\n                        torch.cuda.empty_cache()\n\n                # Log images after each epoch\n                if self.phase_should_log_images:\n                    self.log_images()\n\n                if self.scheduler:\n                    print('before step:', self.scheduler.get_last_lr())\n                    self.scheduler.step()\n                    print('after step:', self.scheduler.get_last_lr())\n\n    def setup_metrics(self, phase: dict):\n        # Setup Metrics for this phase\n        # Check if the phase has a log_images command -&gt; Relevant for memory management of metrics (PRC, ROC and Confusion Matrix)\n        nthresholds = (\n            100  # Make sure that the thresholds for the PRC-based metrics are equal to benefit from grouped computing\n        )\n        # Make sure that the matching args are the same for all instance metrics\n        matching_threshold = 0.5\n        matching_metric = 'iou'\n        boundary_dilation = 0.02\n\n        metrics = MetricCollection(\n            {\n                'Accuracy': Accuracy(task='binary', validate_args=False),\n                'Precision': Precision(task='binary', validate_args=False),\n                'Specificity': Specificity(task='binary', validate_args=False),\n                'Recall': Recall(task='binary', validate_args=False),\n                'F1Score': F1Score(task='binary', validate_args=False),\n                'JaccardIndex': JaccardIndex(task='binary', validate_args=False),\n                'AUROC': AUROC(task='binary', thresholds=nthresholds, validate_args=False),\n                'AveragePrecision': AveragePrecision(task='binary', thresholds=nthresholds, validate_args=False),\n                # Calibration errors: https://arxiv.org/abs/1909.10155, they take a lot of memory!\n                #'ExpectedCalibrationError': CalibrationError(task='binary', norm='l1'),\n                #'RootMeanSquaredCalibrationError': CalibrationError(task='binary', norm='l2'),\n                #'MaximumCalibrationError': CalibrationError(task='binary', norm='max'),\n                'CohenKappa': CohenKappa(task='binary', validate_args=False),\n                'HammingDistance': HammingDistance(task='binary', validate_args=False),\n                'HingeLoss': HingeLoss(task='binary', validate_args=False),\n                # MCC raised a weired error one time, skipping to be save\n                # 'MatthewsCorrCoef': MatthewsCorrCoef(task='binary'),\n            }\n        )\n        self.metrics = {}\n        self.rocs = {}\n        self.prcs = {}\n        self.confmats = {}\n        self.instance_prcs = {}\n        self.instance_confmats = {}\n\n        if self.phase_should_log_images:\n            self.metric_tracker = defaultdict(list)\n\n        for step in phase['steps']:\n            command, key = parse_step(step)\n            if not key:\n                continue\n\n            self.metrics[key] = metrics.clone(f'{key}/').to(self.dev)\n            if command == 'validate_on':\n                # We don't want to log the confusion matrix, PRC and ROC curve for every step in the training loop\n                # We assign them seperately to have easy access to them in the log_images phase but still benefit from the MetricCollection in terms of compute_groups and update, compute &amp; reset calls\n                self.rocs[key] = ROC(task='binary', thresholds=nthresholds, validate_args=False).to(self.dev)\n                self.prcs[key] = PrecisionRecallCurve(task='binary', thresholds=nthresholds, validate_args=False).to(\n                    self.dev\n                )\n                self.confmats[key] = ConfusionMatrix(task='binary', normalize='true', validate_args=False).to(self.dev)\n                self.instance_prcs[key] = BinaryInstancePrecisionRecallCurve(\n                    thresholds=nthresholds,\n                    matching_threshold=matching_threshold,\n                    matching_metric=matching_metric,\n                    validate_args=False,\n                ).to(self.dev)\n                self.instance_confmats[key] = BinaryInstanceConfusionMatrix(\n                    matching_threshold=matching_threshold, matching_metric=matching_metric, validate_args=False\n                ).to(self.dev)\n                self.metrics[key].add_metrics(\n                    {\n                        'ROC': self.rocs[key],\n                        'PRC': self.prcs[key],\n                        'ConfusionMatrix': self.confmats[key],\n                        'Instance-PRC': self.instance_prcs[key],\n                        'Instance-ConfusionMatrix': self.instance_confmats[key],\n                    }\n                )\n                # We don't want to log the instance metrics for every step in the training loop, because they are memory intensive (and not very useful for training)\n                self.metrics[key].add_metrics(\n                    {\n                        'Instance-Accuracy': BinaryInstanceAccuracy(\n                            matching_threshold=matching_threshold, matching_metric=matching_metric, validate_args=False\n                        ).to(self.dev),\n                        'Instance-Precision': BinaryInstancePrecision(\n                            matching_threshold=matching_threshold, matching_metric=matching_metric, validate_args=False\n                        ).to(self.dev),\n                        'Instance-Recall': BinaryInstanceRecall(\n                            matching_threshold=matching_threshold, matching_metric=matching_metric, validate_args=False\n                        ).to(self.dev),\n                        'Instance-F1Score': BinaryInstanceF1Score(\n                            matching_threshold=matching_threshold, matching_metric=matching_metric, validate_args=False\n                        ).to(self.dev),\n                        'Instance-AveragePrecision': BinaryInstanceAveragePrecision(\n                            thresholds=nthresholds,\n                            matching_threshold=matching_threshold,\n                            matching_metric=matching_metric,\n                            validate_args=False,\n                        ).to(self.dev),\n                        'BoundaryIoU': BinaryBoundaryIoU(dilation=boundary_dilation, validate_args=False).to(self.dev),\n                    }\n                )\n\n        # Create the plot directory\n        if self.phase_should_log_images:\n            phase_name = phase['phase'].lower()\n            self.metric_plot_dir = self.log_dir / f'metrics_plots_{phase_name}'\n            self.metric_plot_dir.mkdir(exist_ok=True)\n\n    def get_dataloader(self, name):\n        if name in self.dataset_cache:\n            return self.dataset_cache[name]\n\n        if name in self.config['datasets']:\n            ds_config = self.config['datasets'][name]\n            if 'batch_size' not in ds_config:\n                ds_config['batch_size'] = self.config['batch_size']\n            ds_config['num_workers'] = self.config['data_threads']\n            ds_config['augment_types'] = self.config['datasets']\n            ds_config['data_sources'] = self.data_sources\n            ds_config['data_root'] = self.DATA_ROOT\n            self.dataset_cache[name] = get_loader(**ds_config)\n        else:\n            func, arg = re.search(r'(\\w+)\\((\\w+)\\)', name).groups()\n            if func == 'slump_tiles':\n                ds_config = self.config['datasets'][arg]\n                if 'batch_size' not in ds_config:\n                    ds_config['batch_size'] = self.config['batch_size']\n                ds_config['num_workers'] = self.config['data_threads']\n                ds_config['data_sources'] = self.data_sources\n                ds_config['data_root'] = self.DATA_ROOT\n                self.dataset_cache[name] = get_slump_loader(**ds_config)\n\n        return self.dataset_cache[name]\n\n    def train_epoch(self, train_loader):\n        self.logger.info(f'Epoch {self.epoch} - Training Started')\n        progress = tqdm(train_loader)\n        self.model.train(True)\n        epoch_loss = {\n            'Loss': [],\n            'Deep Supervision Loss': [],\n        }\n        for iteration, (img, target) in enumerate(progress):\n            self.board_idx += img.shape[0]\n\n            img = img.to(self.dev, torch.float)\n            target = target.to(self.dev, torch.long, non_blocking=True)\n\n            self.opt.zero_grad()\n            y_hat = self.model(img)\n\n            if isinstance(y_hat, (tuple, list)):\n                # Deep Supervision\n                deep_super_losses = [self.loss_function(pred.squeeze(1), target) for pred in y_hat]\n                y_hat = y_hat[0].squeeze(1)\n                loss = sum(deep_super_losses)\n                for dsl in deep_super_losses:\n                    epoch_loss['Loss'].append(dsl.detach())\n                epoch_loss['Deep Supervision Loss'].append(loss.detach())\n            else:\n                loss = self.loss_function(y_hat, target)\n                epoch_loss['Loss'].append(loss.detach())\n\n            loss.backward()\n            self.opt.step()\n\n            with torch.no_grad():\n                self.metrics[self.current_key].update(y_hat.squeeze(1), target)\n\n        # Compute epochs loss and metrics\n        epoch_loss = {k: torch.stack(v).mean().item() for k, v in epoch_loss.items() if v}\n        wandb.log({'train/Loss': epoch_loss}, step=self.board_idx)\n        epoch_metrics = self.log_metrics()\n        self.logger.info(f'Epoch {self.epoch} - Loss {epoch_loss} Training Metrics: {epoch_metrics}')\n        self.log_csv(epoch_metrics, epoch_loss)\n\n        # Update progress bar\n        progress.set_postfix(epoch_metrics)\n\n        # Save model Checkpoint\n        torch.save(self.model.state_dict(), self.checkpoints / f'{self.epoch:02d}.pt')\n\n    def val_epoch(self, val_loader):\n        self.logger.info(f'Epoch {self.epoch} - Validation of {self.current_key} Started')\n        self.model.train(False)\n        with torch.no_grad():\n            epoch_loss = []\n            for iteration, (img, target) in enumerate(val_loader):\n                img = img.to(self.dev, torch.float)\n                target = target.to(self.dev, torch.long, non_blocking=True)\n                y_hat = self.model(img).squeeze(1)\n\n                loss = self.loss_function(y_hat, target)\n                epoch_loss.append(loss.detach())\n                self.metrics[self.current_key].update(y_hat, target)\n\n            # Compute epochs loss and metrics\n            epoch_loss = torch.stack(epoch_loss).mean().item()\n            epoch_metrics = self.log_metrics()\n        wandb.log({'val/Loss': epoch_loss}, step=self.board_idx)\n        self.logger.info(f'Epoch {self.epoch} - Loss {epoch_loss} Validation Metrics: {epoch_metrics}')\n        self.log_csv(epoch_metrics, epoch_loss)\n\n        # Plot roc, prc and confusion matrix to disk and wandb\n        fig_roc, _ = self.rocs[self.current_key].plot(score=True)\n        fig_prc, _ = self.prcs[self.current_key].plot(score=True)\n        fig_confmat, _ = self.confmats[self.current_key].plot(cmap='Blues')\n        fig_instance_prc, _ = self.instance_prcs[self.current_key].plot(score=True)\n        fig_instance_confmat, _ = self.instance_confmats[self.current_key].plot(cmap='Blues')\n\n        # We need to wrap the figures into a wandb.Image to save storage -&gt; Maybe we can find a better solution in the future, e.g. using plotly\n        wandb.log({f'{self.current_key}/ROC': wandb.Image(fig_roc)}, step=self.board_idx)\n        wandb.log({f'{self.current_key}/PRC': wandb.Image(fig_prc)}, step=self.board_idx)\n        wandb.log({f'{self.current_key}/Confusion Matrix': wandb.Image(fig_confmat)}, step=self.board_idx)\n        wandb.log({f'{self.current_key}/Instance-PRC': wandb.Image(fig_instance_prc)}, step=self.board_idx)\n        wandb.log(\n            {f'{self.current_key}/Instance-Confusion Matrix': wandb.Image(fig_instance_confmat)}, step=self.board_idx\n        )\n\n        if self.phase_should_log_images:\n            fig_roc.savefig(self.metric_plot_dir / f'{self.current_key}_roc_curve.png')\n            fig_prc.savefig(self.metric_plot_dir / f'{self.current_key}_precision_recall_curve.png')\n            fig_confmat.savefig(self.metric_plot_dir / f'{self.current_key}_confusion_matrix.png')\n            fig_instance_prc.savefig(self.metric_plot_dir / f'{self.current_key}_instance_precision_recall_curve.png')\n            fig_instance_confmat.savefig(self.metric_plot_dir / f'{self.current_key}_instance_confusion_matrix.png')\n        fig_roc.clear()\n        fig_prc.clear()\n        fig_confmat.clear()\n        fig_instance_prc.clear()\n        fig_instance_confmat.clear()\n        plt.close('all')\n\n    def log_metrics(self) -&gt; dict:\n        epoch_metrics = self.metrics[self.current_key].compute()\n        # We need to filter our ROC, PRC and Confusion Matrix from the metrics because they are not scalar metrics\n\n        scalar_epoch_metrics = {\n            k: v.item() for k, v in epoch_metrics.items() if isinstance(v, torch.Tensor) and v.numel() == 1\n        }\n\n        # Log to WandB\n        wandb.log(scalar_epoch_metrics, step=self.board_idx)\n\n        # Plot the metrics to disk\n        if self.phase_should_log_images:\n            scalar_epoch_metrics_tensor = {\n                k: v for k, v in epoch_metrics.items() if isinstance(v, torch.Tensor) and v.numel() == 1\n            }\n            self.metric_tracker[self.current_key].append(scalar_epoch_metrics_tensor)\n            fig, ax = plt.subplots(figsize=(20, 10))\n            self.metrics[self.current_key].plot(self.metric_tracker[self.current_key], together=True, ax=ax)\n            fig.savefig(self.metric_plot_dir / f'{self.current_key}_metrics.png')\n            fig.clear()\n            plt.close('all')\n\n        return scalar_epoch_metrics\n\n    def log_csv(self, epoch_metrics: dict, epoch_loss: int | dict):\n        # Log to CSV\n        logfile = self.log_dir / f'{self.current_key}.csv'\n\n        if isinstance(epoch_loss, dict):\n            logstr = (\n                f'{self.epoch},'\n                + ','.join(f'{val}' for key, val in epoch_metrics.items())\n                + ','\n                + ','.join(f'{val}' for key, val in epoch_loss.items())\n            )\n        else:\n            logstr = f'{self.epoch},' + ','.join(f'{val}' for key, val in epoch_metrics.items()) + f',{epoch_loss}'\n\n        if not logfile.exists():\n            # Print header upon first log print\n            header = 'Epoch,' + ','.join(f'{key}' for key, val in epoch_metrics.items()) + ',Loss'\n            with logfile.open('w') as f:\n                print(header, file=f)\n                print(logstr, file=f)\n        else:\n            with logfile.open('a') as f:\n                print(logstr, file=f)\n\n    def log_images(self):\n        self.logger.debug(f'Epoch {self.epoch} - Image Logging')\n        self.model.train(False)\n        with torch.no_grad():\n            preds = []\n            for vis_imgs, vis_masks in self.vis_loader:\n                preds.append(self.model(vis_imgs.to(self.dev)).cpu().squeeze(1))\n            preds = torch.cat(preds).unsqueeze(1)\n            if self.vis_predictions is None:\n                self.vis_predictions = preds\n            else:\n                self.vis_predictions = torch.cat([self.vis_predictions, preds], dim=1)\n        (self.log_dir / 'tile_predictions').mkdir(exist_ok=True)\n        for i, tile in enumerate(self.vis_names):\n            filename = self.log_dir / 'tile_predictions' / f'{tile}.jpg'\n            showexample(\n                self.vis_loader.dataset[i], self.vis_predictions[i], filename, self.data_sources, step=self.board_idx\n            )\n        plt.close('all')\n\n    def setup_lr_scheduler(self):\n        # Scheduler\n        if 'learning_rate_scheduler' not in self.config.keys():\n            print('running without learning rate scheduler')\n            self.scheduler = None\n        elif self.config['learning_rate_scheduler'] == 'StepLR':\n            if 'lr_step_size' not in self.config.keys():\n                step_size = 10\n            else:\n                step_size = self.config['lr_step_size']\n            if 'lr_gamma' not in self.config.keys():\n                gamma = 0.1\n            else:\n                gamma = self.config['lr_gamma']\n            self.scheduler = torch.optim.lr_scheduler.StepLR(self.opt, step_size=step_size, gamma=gamma)\n            print(f\"running with 'StepLR' learning rate scheduler with step_size = {step_size} and gamma = {gamma}\")\n        elif self.config['learning_rate_scheduler'] == 'ExponentialLR':\n            if 'lr_gamma' not in self.config.keys():\n                gamma = 0.9\n            else:\n                gamma = self.config['lr_gamma']\n            self.scheduler = torch.optim.lr_scheduler.ExponentialLR(self.opt, gamma=gamma)\n            print(f\"running with 'ExponentialLR' learning rate scheduler with gamma = {gamma}\")\n</code></pre>"},{"location":"scripts/#src.thaw_slump_segmentation.scripts.train.Engine.DATA_ROOT","title":"<code>DATA_ROOT = data_dir</code>  <code>instance-attribute</code>","text":""},{"location":"scripts/#src.thaw_slump_segmentation.scripts.train.Engine.board_idx","title":"<code>board_idx = 0</code>  <code>instance-attribute</code>","text":""},{"location":"scripts/#src.thaw_slump_segmentation.scripts.train.Engine.checkpoints","title":"<code>checkpoints = self.log_dir / 'checkpoints'</code>  <code>instance-attribute</code>","text":""},{"location":"scripts/#src.thaw_slump_segmentation.scripts.train.Engine.config","title":"<code>config = yaml.load(config.open(), Loader=yaml_custom.SaneYAMLLoader)</code>  <code>instance-attribute</code>","text":""},{"location":"scripts/#src.thaw_slump_segmentation.scripts.train.Engine.data_sources","title":"<code>data_sources = DataSources(self.config['data_sources'])</code>  <code>instance-attribute</code>","text":""},{"location":"scripts/#src.thaw_slump_segmentation.scripts.train.Engine.dataset_cache","title":"<code>dataset_cache = {}</code>  <code>instance-attribute</code>","text":""},{"location":"scripts/#src.thaw_slump_segmentation.scripts.train.Engine.dev","title":"<code>dev = torch.device('cpu') if not torch.cuda.is_available() else torch.device('cuda')</code>  <code>instance-attribute</code>","text":""},{"location":"scripts/#src.thaw_slump_segmentation.scripts.train.Engine.epoch","title":"<code>epoch = 0</code>  <code>instance-attribute</code>","text":""},{"location":"scripts/#src.thaw_slump_segmentation.scripts.train.Engine.learning_rate","title":"<code>learning_rate = self.config['learning_rate']</code>  <code>instance-attribute</code>","text":""},{"location":"scripts/#src.thaw_slump_segmentation.scripts.train.Engine.log_dir","title":"<code>log_dir = Path(log_dir) / log_dir_name</code>  <code>instance-attribute</code>","text":""},{"location":"scripts/#src.thaw_slump_segmentation.scripts.train.Engine.logger","title":"<code>logger = get_logger('train')</code>  <code>instance-attribute</code>","text":""},{"location":"scripts/#src.thaw_slump_segmentation.scripts.train.Engine.model","title":"<code>model = torch.compile(self.model, mode='max-autotune', fullgraph=True)</code>  <code>instance-attribute</code>","text":""},{"location":"scripts/#src.thaw_slump_segmentation.scripts.train.Engine.opt","title":"<code>opt = torch.optim.AdamW(self.model.parameters(), lr=self.learning_rate)</code>  <code>instance-attribute</code>","text":""},{"location":"scripts/#src.thaw_slump_segmentation.scripts.train.Engine.vis_predictions","title":"<code>vis_predictions = None</code>  <code>instance-attribute</code>","text":""},{"location":"scripts/#src.thaw_slump_segmentation.scripts.train.Engine.__init__","title":"<code>__init__(config, data_dir, name, log_dir, resume, summary, wandb_project, wandb_name)</code>","text":"Source code in <code>src/thaw_slump_segmentation/scripts/train.py</code> <pre><code>def __init__(\n    self,\n    config: Path,\n    data_dir: Path,\n    name: str,\n    log_dir: Path,\n    resume: str,\n    summary: bool,\n    wandb_project: str,\n    wandb_name: str,\n):\n    self.config = yaml.load(config.open(), Loader=yaml_custom.SaneYAMLLoader)\n    self.DATA_ROOT = data_dir\n    # Logging setup\n    timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n    if name:\n        log_dir_name = f'{name}_{timestamp}'\n    else:\n        log_dir_name = timestamp\n    self.log_dir = Path(log_dir) / log_dir_name\n    self.log_dir.mkdir(exist_ok=False)\n\n    init_logging(self.log_dir / 'train.log')\n    self.logger = get_logger('train')\n\n    self.data_sources = DataSources(self.config['data_sources'])\n    self.config['model']['input_channels'] = sum(src.channels for src in self.data_sources)\n\n    m = self.config['model']\n    self.model = create_model(\n        arch=m['architecture'],\n        encoder_name=m['encoder'],\n        encoder_weights=None if m['encoder_weights'] == 'random' else m['encoder_weights'],\n        classes=1,\n        in_channels=m['input_channels'],\n    )\n\n    # make parallel\n    self.model = nn.DataParallel(self.model)\n\n    if resume:\n        self.config['resume'] = resume\n\n    if 'resume' in self.config and self.config['resume']:\n        checkpoint = Path(self.config['resume'])\n        if not checkpoint.exists():\n            raise ValueError(f\"There is no Checkpoint at {self.config['resume']} to resume from!\")\n        if checkpoint.is_dir():\n            # Load last checkpoint in run dir\n            ckpt_nums = [int(ckpt.stem) for ckpt in checkpoint.glob('checkpoints/*.pt')]\n            last_ckpt = max(ckpt_nums)\n            self.config['resume'] = str(checkpoint / 'checkpoints' / f'{last_ckpt:02d}.pt')\n        self.logger.info(f\"Resuming training from checkpoint {self.config['resume']}\")\n        self.model.load_state_dict(torch.load(self.config['resume']))\n\n    self.dev = torch.device('cpu') if not torch.cuda.is_available() else torch.device('cuda')\n    self.logger.info(f'Training on {self.dev} device')\n\n    self.model = self.model.to(self.dev)\n    self.model = torch.compile(self.model, mode='max-autotune', fullgraph=True)\n\n    self.learning_rate = self.config['learning_rate']\n    self.opt = torch.optim.AdamW(self.model.parameters(), lr=self.learning_rate)\n    self.setup_lr_scheduler()\n\n    self.board_idx = 0\n    self.epoch = 0\n\n    if summary:\n        from torchsummary import summary\n\n        summary(self.model, [(self.config['model']['input_channels'], 256, 256)])\n        sys.exit(0)\n\n    self.dataset_cache = {}\n\n    self.vis_predictions = None\n    self.vis_loader, self.vis_names = get_vis_loader(\n        self.config['visualization_tiles'],\n        batch_size=self.config['batch_size'],\n        data_sources=self.data_sources,\n        data_root=self.DATA_ROOT,\n    )\n\n    # Write the config YML to the run-folder\n    self.config['run_info'] = {\n        'timestamp': timestamp,\n        'git_head': subprocess.check_output(['git', 'describe'], encoding='utf8').strip(),\n    }\n    with open(self.log_dir / 'config.yml', 'w') as f:\n        yaml.dump(self.config, f)\n\n    self.checkpoints = self.log_dir / 'checkpoints'\n    self.checkpoints.mkdir()\n\n    # Weights and Biases initialization\n    print('wandb project:', wandb_project)\n    print('wandb name:', wandb_name)\n    print('config:', self.config)\n    print('entity:', 'ml4earth')\n    wandb.init(project=wandb_project, name=wandb_name, config=self.config, entity='ingmarnitze_team')\n</code></pre>"},{"location":"scripts/#src.thaw_slump_segmentation.scripts.train.Engine.get_dataloader","title":"<code>get_dataloader(name)</code>","text":"Source code in <code>src/thaw_slump_segmentation/scripts/train.py</code> <pre><code>def get_dataloader(self, name):\n    if name in self.dataset_cache:\n        return self.dataset_cache[name]\n\n    if name in self.config['datasets']:\n        ds_config = self.config['datasets'][name]\n        if 'batch_size' not in ds_config:\n            ds_config['batch_size'] = self.config['batch_size']\n        ds_config['num_workers'] = self.config['data_threads']\n        ds_config['augment_types'] = self.config['datasets']\n        ds_config['data_sources'] = self.data_sources\n        ds_config['data_root'] = self.DATA_ROOT\n        self.dataset_cache[name] = get_loader(**ds_config)\n    else:\n        func, arg = re.search(r'(\\w+)\\((\\w+)\\)', name).groups()\n        if func == 'slump_tiles':\n            ds_config = self.config['datasets'][arg]\n            if 'batch_size' not in ds_config:\n                ds_config['batch_size'] = self.config['batch_size']\n            ds_config['num_workers'] = self.config['data_threads']\n            ds_config['data_sources'] = self.data_sources\n            ds_config['data_root'] = self.DATA_ROOT\n            self.dataset_cache[name] = get_slump_loader(**ds_config)\n\n    return self.dataset_cache[name]\n</code></pre>"},{"location":"scripts/#src.thaw_slump_segmentation.scripts.train.Engine.log_csv","title":"<code>log_csv(epoch_metrics, epoch_loss)</code>","text":"Source code in <code>src/thaw_slump_segmentation/scripts/train.py</code> <pre><code>def log_csv(self, epoch_metrics: dict, epoch_loss: int | dict):\n    # Log to CSV\n    logfile = self.log_dir / f'{self.current_key}.csv'\n\n    if isinstance(epoch_loss, dict):\n        logstr = (\n            f'{self.epoch},'\n            + ','.join(f'{val}' for key, val in epoch_metrics.items())\n            + ','\n            + ','.join(f'{val}' for key, val in epoch_loss.items())\n        )\n    else:\n        logstr = f'{self.epoch},' + ','.join(f'{val}' for key, val in epoch_metrics.items()) + f',{epoch_loss}'\n\n    if not logfile.exists():\n        # Print header upon first log print\n        header = 'Epoch,' + ','.join(f'{key}' for key, val in epoch_metrics.items()) + ',Loss'\n        with logfile.open('w') as f:\n            print(header, file=f)\n            print(logstr, file=f)\n    else:\n        with logfile.open('a') as f:\n            print(logstr, file=f)\n</code></pre>"},{"location":"scripts/#src.thaw_slump_segmentation.scripts.train.Engine.log_images","title":"<code>log_images()</code>","text":"Source code in <code>src/thaw_slump_segmentation/scripts/train.py</code> <pre><code>def log_images(self):\n    self.logger.debug(f'Epoch {self.epoch} - Image Logging')\n    self.model.train(False)\n    with torch.no_grad():\n        preds = []\n        for vis_imgs, vis_masks in self.vis_loader:\n            preds.append(self.model(vis_imgs.to(self.dev)).cpu().squeeze(1))\n        preds = torch.cat(preds).unsqueeze(1)\n        if self.vis_predictions is None:\n            self.vis_predictions = preds\n        else:\n            self.vis_predictions = torch.cat([self.vis_predictions, preds], dim=1)\n    (self.log_dir / 'tile_predictions').mkdir(exist_ok=True)\n    for i, tile in enumerate(self.vis_names):\n        filename = self.log_dir / 'tile_predictions' / f'{tile}.jpg'\n        showexample(\n            self.vis_loader.dataset[i], self.vis_predictions[i], filename, self.data_sources, step=self.board_idx\n        )\n    plt.close('all')\n</code></pre>"},{"location":"scripts/#src.thaw_slump_segmentation.scripts.train.Engine.log_metrics","title":"<code>log_metrics()</code>","text":"Source code in <code>src/thaw_slump_segmentation/scripts/train.py</code> <pre><code>def log_metrics(self) -&gt; dict:\n    epoch_metrics = self.metrics[self.current_key].compute()\n    # We need to filter our ROC, PRC and Confusion Matrix from the metrics because they are not scalar metrics\n\n    scalar_epoch_metrics = {\n        k: v.item() for k, v in epoch_metrics.items() if isinstance(v, torch.Tensor) and v.numel() == 1\n    }\n\n    # Log to WandB\n    wandb.log(scalar_epoch_metrics, step=self.board_idx)\n\n    # Plot the metrics to disk\n    if self.phase_should_log_images:\n        scalar_epoch_metrics_tensor = {\n            k: v for k, v in epoch_metrics.items() if isinstance(v, torch.Tensor) and v.numel() == 1\n        }\n        self.metric_tracker[self.current_key].append(scalar_epoch_metrics_tensor)\n        fig, ax = plt.subplots(figsize=(20, 10))\n        self.metrics[self.current_key].plot(self.metric_tracker[self.current_key], together=True, ax=ax)\n        fig.savefig(self.metric_plot_dir / f'{self.current_key}_metrics.png')\n        fig.clear()\n        plt.close('all')\n\n    return scalar_epoch_metrics\n</code></pre>"},{"location":"scripts/#src.thaw_slump_segmentation.scripts.train.Engine.run","title":"<code>run()</code>","text":"Source code in <code>src/thaw_slump_segmentation/scripts/train.py</code> <pre><code>def run(self):\n    for phase in self.config['schedule']:\n        self.logger.info(f'Starting phase \"{phase[\"phase\"]}\"')\n\n        self.phase_should_log_images = phase.get('log_images', False)\n        self.setup_metrics(phase)\n        self.current_key = None\n        for epoch in range(phase['epochs']):\n            # Epoch setup\n            self.epoch = epoch\n            self.loss_function = create_loss(scoped_get('loss_function', phase, self.config)).to(self.dev)\n\n            for step in phase['steps']:\n                command, key = parse_step(step)\n                self.current_key = key\n\n                if command == 'train_on':\n                    # Training step\n                    data_loader = self.get_dataloader(key)\n                    self.train_epoch(data_loader)\n                elif command == 'validate_on':\n                    # Validation step\n                    data_loader = self.get_dataloader(key)\n                    self.val_epoch(data_loader)\n                elif command == 'log_images':\n                    self.logger.warn(\n                        \"Step 'log_images' is deprecated. Please use 'log_images' in the phase instead.\"\n                    )\n                    continue\n                else:\n                    raise ValueError(f\"Unknown command '{command}'\")\n\n                # Reset the metrics after each step\n                if self.current_key:\n                    self.metrics[self.current_key].reset()\n                    gc.collect()\n                    torch.cuda.empty_cache()\n\n            # Log images after each epoch\n            if self.phase_should_log_images:\n                self.log_images()\n\n            if self.scheduler:\n                print('before step:', self.scheduler.get_last_lr())\n                self.scheduler.step()\n                print('after step:', self.scheduler.get_last_lr())\n</code></pre>"},{"location":"scripts/#src.thaw_slump_segmentation.scripts.train.Engine.setup_lr_scheduler","title":"<code>setup_lr_scheduler()</code>","text":"Source code in <code>src/thaw_slump_segmentation/scripts/train.py</code> <pre><code>def setup_lr_scheduler(self):\n    # Scheduler\n    if 'learning_rate_scheduler' not in self.config.keys():\n        print('running without learning rate scheduler')\n        self.scheduler = None\n    elif self.config['learning_rate_scheduler'] == 'StepLR':\n        if 'lr_step_size' not in self.config.keys():\n            step_size = 10\n        else:\n            step_size = self.config['lr_step_size']\n        if 'lr_gamma' not in self.config.keys():\n            gamma = 0.1\n        else:\n            gamma = self.config['lr_gamma']\n        self.scheduler = torch.optim.lr_scheduler.StepLR(self.opt, step_size=step_size, gamma=gamma)\n        print(f\"running with 'StepLR' learning rate scheduler with step_size = {step_size} and gamma = {gamma}\")\n    elif self.config['learning_rate_scheduler'] == 'ExponentialLR':\n        if 'lr_gamma' not in self.config.keys():\n            gamma = 0.9\n        else:\n            gamma = self.config['lr_gamma']\n        self.scheduler = torch.optim.lr_scheduler.ExponentialLR(self.opt, gamma=gamma)\n        print(f\"running with 'ExponentialLR' learning rate scheduler with gamma = {gamma}\")\n</code></pre>"},{"location":"scripts/#src.thaw_slump_segmentation.scripts.train.Engine.setup_metrics","title":"<code>setup_metrics(phase)</code>","text":"Source code in <code>src/thaw_slump_segmentation/scripts/train.py</code> <pre><code>def setup_metrics(self, phase: dict):\n    # Setup Metrics for this phase\n    # Check if the phase has a log_images command -&gt; Relevant for memory management of metrics (PRC, ROC and Confusion Matrix)\n    nthresholds = (\n        100  # Make sure that the thresholds for the PRC-based metrics are equal to benefit from grouped computing\n    )\n    # Make sure that the matching args are the same for all instance metrics\n    matching_threshold = 0.5\n    matching_metric = 'iou'\n    boundary_dilation = 0.02\n\n    metrics = MetricCollection(\n        {\n            'Accuracy': Accuracy(task='binary', validate_args=False),\n            'Precision': Precision(task='binary', validate_args=False),\n            'Specificity': Specificity(task='binary', validate_args=False),\n            'Recall': Recall(task='binary', validate_args=False),\n            'F1Score': F1Score(task='binary', validate_args=False),\n            'JaccardIndex': JaccardIndex(task='binary', validate_args=False),\n            'AUROC': AUROC(task='binary', thresholds=nthresholds, validate_args=False),\n            'AveragePrecision': AveragePrecision(task='binary', thresholds=nthresholds, validate_args=False),\n            # Calibration errors: https://arxiv.org/abs/1909.10155, they take a lot of memory!\n            #'ExpectedCalibrationError': CalibrationError(task='binary', norm='l1'),\n            #'RootMeanSquaredCalibrationError': CalibrationError(task='binary', norm='l2'),\n            #'MaximumCalibrationError': CalibrationError(task='binary', norm='max'),\n            'CohenKappa': CohenKappa(task='binary', validate_args=False),\n            'HammingDistance': HammingDistance(task='binary', validate_args=False),\n            'HingeLoss': HingeLoss(task='binary', validate_args=False),\n            # MCC raised a weired error one time, skipping to be save\n            # 'MatthewsCorrCoef': MatthewsCorrCoef(task='binary'),\n        }\n    )\n    self.metrics = {}\n    self.rocs = {}\n    self.prcs = {}\n    self.confmats = {}\n    self.instance_prcs = {}\n    self.instance_confmats = {}\n\n    if self.phase_should_log_images:\n        self.metric_tracker = defaultdict(list)\n\n    for step in phase['steps']:\n        command, key = parse_step(step)\n        if not key:\n            continue\n\n        self.metrics[key] = metrics.clone(f'{key}/').to(self.dev)\n        if command == 'validate_on':\n            # We don't want to log the confusion matrix, PRC and ROC curve for every step in the training loop\n            # We assign them seperately to have easy access to them in the log_images phase but still benefit from the MetricCollection in terms of compute_groups and update, compute &amp; reset calls\n            self.rocs[key] = ROC(task='binary', thresholds=nthresholds, validate_args=False).to(self.dev)\n            self.prcs[key] = PrecisionRecallCurve(task='binary', thresholds=nthresholds, validate_args=False).to(\n                self.dev\n            )\n            self.confmats[key] = ConfusionMatrix(task='binary', normalize='true', validate_args=False).to(self.dev)\n            self.instance_prcs[key] = BinaryInstancePrecisionRecallCurve(\n                thresholds=nthresholds,\n                matching_threshold=matching_threshold,\n                matching_metric=matching_metric,\n                validate_args=False,\n            ).to(self.dev)\n            self.instance_confmats[key] = BinaryInstanceConfusionMatrix(\n                matching_threshold=matching_threshold, matching_metric=matching_metric, validate_args=False\n            ).to(self.dev)\n            self.metrics[key].add_metrics(\n                {\n                    'ROC': self.rocs[key],\n                    'PRC': self.prcs[key],\n                    'ConfusionMatrix': self.confmats[key],\n                    'Instance-PRC': self.instance_prcs[key],\n                    'Instance-ConfusionMatrix': self.instance_confmats[key],\n                }\n            )\n            # We don't want to log the instance metrics for every step in the training loop, because they are memory intensive (and not very useful for training)\n            self.metrics[key].add_metrics(\n                {\n                    'Instance-Accuracy': BinaryInstanceAccuracy(\n                        matching_threshold=matching_threshold, matching_metric=matching_metric, validate_args=False\n                    ).to(self.dev),\n                    'Instance-Precision': BinaryInstancePrecision(\n                        matching_threshold=matching_threshold, matching_metric=matching_metric, validate_args=False\n                    ).to(self.dev),\n                    'Instance-Recall': BinaryInstanceRecall(\n                        matching_threshold=matching_threshold, matching_metric=matching_metric, validate_args=False\n                    ).to(self.dev),\n                    'Instance-F1Score': BinaryInstanceF1Score(\n                        matching_threshold=matching_threshold, matching_metric=matching_metric, validate_args=False\n                    ).to(self.dev),\n                    'Instance-AveragePrecision': BinaryInstanceAveragePrecision(\n                        thresholds=nthresholds,\n                        matching_threshold=matching_threshold,\n                        matching_metric=matching_metric,\n                        validate_args=False,\n                    ).to(self.dev),\n                    'BoundaryIoU': BinaryBoundaryIoU(dilation=boundary_dilation, validate_args=False).to(self.dev),\n                }\n            )\n\n    # Create the plot directory\n    if self.phase_should_log_images:\n        phase_name = phase['phase'].lower()\n        self.metric_plot_dir = self.log_dir / f'metrics_plots_{phase_name}'\n        self.metric_plot_dir.mkdir(exist_ok=True)\n</code></pre>"},{"location":"scripts/#src.thaw_slump_segmentation.scripts.train.Engine.train_epoch","title":"<code>train_epoch(train_loader)</code>","text":"Source code in <code>src/thaw_slump_segmentation/scripts/train.py</code> <pre><code>def train_epoch(self, train_loader):\n    self.logger.info(f'Epoch {self.epoch} - Training Started')\n    progress = tqdm(train_loader)\n    self.model.train(True)\n    epoch_loss = {\n        'Loss': [],\n        'Deep Supervision Loss': [],\n    }\n    for iteration, (img, target) in enumerate(progress):\n        self.board_idx += img.shape[0]\n\n        img = img.to(self.dev, torch.float)\n        target = target.to(self.dev, torch.long, non_blocking=True)\n\n        self.opt.zero_grad()\n        y_hat = self.model(img)\n\n        if isinstance(y_hat, (tuple, list)):\n            # Deep Supervision\n            deep_super_losses = [self.loss_function(pred.squeeze(1), target) for pred in y_hat]\n            y_hat = y_hat[0].squeeze(1)\n            loss = sum(deep_super_losses)\n            for dsl in deep_super_losses:\n                epoch_loss['Loss'].append(dsl.detach())\n            epoch_loss['Deep Supervision Loss'].append(loss.detach())\n        else:\n            loss = self.loss_function(y_hat, target)\n            epoch_loss['Loss'].append(loss.detach())\n\n        loss.backward()\n        self.opt.step()\n\n        with torch.no_grad():\n            self.metrics[self.current_key].update(y_hat.squeeze(1), target)\n\n    # Compute epochs loss and metrics\n    epoch_loss = {k: torch.stack(v).mean().item() for k, v in epoch_loss.items() if v}\n    wandb.log({'train/Loss': epoch_loss}, step=self.board_idx)\n    epoch_metrics = self.log_metrics()\n    self.logger.info(f'Epoch {self.epoch} - Loss {epoch_loss} Training Metrics: {epoch_metrics}')\n    self.log_csv(epoch_metrics, epoch_loss)\n\n    # Update progress bar\n    progress.set_postfix(epoch_metrics)\n\n    # Save model Checkpoint\n    torch.save(self.model.state_dict(), self.checkpoints / f'{self.epoch:02d}.pt')\n</code></pre>"},{"location":"scripts/#src.thaw_slump_segmentation.scripts.train.Engine.val_epoch","title":"<code>val_epoch(val_loader)</code>","text":"Source code in <code>src/thaw_slump_segmentation/scripts/train.py</code> <pre><code>def val_epoch(self, val_loader):\n    self.logger.info(f'Epoch {self.epoch} - Validation of {self.current_key} Started')\n    self.model.train(False)\n    with torch.no_grad():\n        epoch_loss = []\n        for iteration, (img, target) in enumerate(val_loader):\n            img = img.to(self.dev, torch.float)\n            target = target.to(self.dev, torch.long, non_blocking=True)\n            y_hat = self.model(img).squeeze(1)\n\n            loss = self.loss_function(y_hat, target)\n            epoch_loss.append(loss.detach())\n            self.metrics[self.current_key].update(y_hat, target)\n\n        # Compute epochs loss and metrics\n        epoch_loss = torch.stack(epoch_loss).mean().item()\n        epoch_metrics = self.log_metrics()\n    wandb.log({'val/Loss': epoch_loss}, step=self.board_idx)\n    self.logger.info(f'Epoch {self.epoch} - Loss {epoch_loss} Validation Metrics: {epoch_metrics}')\n    self.log_csv(epoch_metrics, epoch_loss)\n\n    # Plot roc, prc and confusion matrix to disk and wandb\n    fig_roc, _ = self.rocs[self.current_key].plot(score=True)\n    fig_prc, _ = self.prcs[self.current_key].plot(score=True)\n    fig_confmat, _ = self.confmats[self.current_key].plot(cmap='Blues')\n    fig_instance_prc, _ = self.instance_prcs[self.current_key].plot(score=True)\n    fig_instance_confmat, _ = self.instance_confmats[self.current_key].plot(cmap='Blues')\n\n    # We need to wrap the figures into a wandb.Image to save storage -&gt; Maybe we can find a better solution in the future, e.g. using plotly\n    wandb.log({f'{self.current_key}/ROC': wandb.Image(fig_roc)}, step=self.board_idx)\n    wandb.log({f'{self.current_key}/PRC': wandb.Image(fig_prc)}, step=self.board_idx)\n    wandb.log({f'{self.current_key}/Confusion Matrix': wandb.Image(fig_confmat)}, step=self.board_idx)\n    wandb.log({f'{self.current_key}/Instance-PRC': wandb.Image(fig_instance_prc)}, step=self.board_idx)\n    wandb.log(\n        {f'{self.current_key}/Instance-Confusion Matrix': wandb.Image(fig_instance_confmat)}, step=self.board_idx\n    )\n\n    if self.phase_should_log_images:\n        fig_roc.savefig(self.metric_plot_dir / f'{self.current_key}_roc_curve.png')\n        fig_prc.savefig(self.metric_plot_dir / f'{self.current_key}_precision_recall_curve.png')\n        fig_confmat.savefig(self.metric_plot_dir / f'{self.current_key}_confusion_matrix.png')\n        fig_instance_prc.savefig(self.metric_plot_dir / f'{self.current_key}_instance_precision_recall_curve.png')\n        fig_instance_confmat.savefig(self.metric_plot_dir / f'{self.current_key}_instance_confusion_matrix.png')\n    fig_roc.clear()\n    fig_prc.clear()\n    fig_confmat.clear()\n    fig_instance_prc.clear()\n    fig_instance_confmat.clear()\n    plt.close('all')\n</code></pre>"},{"location":"scripts/#src.thaw_slump_segmentation.scripts.train.parse_step","title":"<code>parse_step(step)</code>","text":"Source code in <code>src/thaw_slump_segmentation/scripts/train.py</code> <pre><code>def parse_step(step) -&gt; tuple[str, str | None]:\n    if isinstance(step, dict):\n        assert len(step) == 1\n        ((command, key),) = step.items()\n    else:\n        command, key = step, None\n    return command, key\n</code></pre>"},{"location":"scripts/#src.thaw_slump_segmentation.scripts.train.scoped_get","title":"<code>scoped_get(key, *scopestack)</code>","text":"Source code in <code>src/thaw_slump_segmentation/scripts/train.py</code> <pre><code>def scoped_get(key, *scopestack):\n    for scope in scopestack:\n        value = scope.get(key)\n        if value is not None:\n            return value\n    raise ValueError(f'Could not find \"{key}\" in any scope.')\n</code></pre>"},{"location":"scripts/#src.thaw_slump_segmentation.scripts.train.train","title":"<code>train(name, data_dir=Path('data'), log_dir=Path('logs'), config=Path('config.yml'), resume=None, summary=False, wandb_project='thaw-slump-segmentation', wandb_name=None)</code>","text":"<p>Training script</p> Source code in <code>src/thaw_slump_segmentation/scripts/train.py</code> <pre><code>def train(\n    name: Annotated[\n        str,\n        typer.Option(\n            '--name',\n            '-n',\n            prompt=True,\n            help='Give this run a name, so that it will be logged into logs/&lt;NAME&gt;_&lt;timestamp&gt;.',\n        ),\n    ],\n    data_dir: Annotated[Path, typer.Option('--data_dir', help='Path to data processing dir')] = Path('data'),\n    log_dir: Annotated[Path, typer.Option('--log_dir', help='Path to log dir')] = Path('logs'),\n    config: Annotated[Path, typer.Option('--config', '-c', help='Specify run config to use.')] = Path('config.yml'),\n    resume: Annotated[\n        str,\n        typer.Option(\n            '--resume',\n            '-r',\n            help='Resume from the specified checkpoint. Can be either a run-id (e.g. \"2020-06-29_18-12-03\") to select the last. Overrides the resume option in the config file if given.',\n        ),\n    ] = None,\n    summary: Annotated[bool, typer.Option('--summary', '-s', help='Only print model summary and return.')] = False,\n    wandb_project: Annotated[\n        str, typer.Option('--wandb_project', '-wp', help='Set a project name for weights and biases')\n    ] = 'thaw-slump-segmentation',\n    wandb_name: Annotated[\n        str, typer.Option('--wandb_name', '-wn', help='Set a run name for weights and biases')\n    ] = None,\n):\n    \"\"\"Training script\"\"\"\n    engine = Engine(config, data_dir, name, log_dir, resume, summary, wandb_project, wandb_name)\n    engine.run()\n</code></pre>"},{"location":"scripts/#src.thaw_slump_segmentation.scripts.process_02_inference.main","title":"<code>main()</code>","text":"Source code in <code>src/thaw_slump_segmentation/scripts/process_02_inference.py</code> <pre><code>def main():\n    # ### Settings\n\n    # Add argument definitions\n    parser = argparse.ArgumentParser(\n        description='Script to run auto inference for RTS', formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n    parser.add_argument(\n        '--code_dir',\n        type=Path,\n        default=Path('/isipd/projects/p_aicore_pf/initze/code/aicore_inference'),\n        help='Local code directory',\n    )\n    parser.add_argument(\n        '--raw_data_dir',\n        type=Path,\n        nargs='+',\n        default=[\n            Path('/isipd/projects/p_aicore_pf/initze/data/planet/planet_data_inference_grid/scenes'),\n            Path('/isipd/projects/p_aicore_pf/initze/data/planet/planet_data_inference_grid/tiles'),\n        ],\n        help='Location of raw data',\n    )\n    parser.add_argument(\n        '--processing_dir',\n        type=Path,\n        default=Path('/isipd/projects/p_aicore_pf/initze/processing'),\n        help='Location for data processing',\n    )\n    parser.add_argument(\n        '--inference_dir',\n        type=Path,\n        default=Path('/isipd/projects/p_aicore_pf/initze/processed/inference'),\n        help='Target directory for inference results',\n    )\n    parser.add_argument(\n        '--model_dir',\n        type=Path,\n        default=Path('/isipd/projects/p_aicore_pf/initze/models/thaw_slumps'),\n        help='Target directory for models',\n    )\n    parser.add_argument(\n        '--model', type=str, default='RTS_v6_tcvis', help=\"Model name, examples ['RTS_v6_tcvis', 'RTS_v6_notcvis']\"\n    )\n    parser.add_argument('--use_gpu', nargs='+', type=int, default=[0], help='List of GPU IDs to use, space separated')\n    parser.add_argument('--runs_per_gpu', type=int, default=5, help='Number of runs per GPU')\n    parser.add_argument('--max_images', type=int, default=None, help='Maximum number of images to process (optional)')\n    parser.add_argument('--skip_vrt', action='store_true', help='set to skip DEM vrt creation')\n    parser.add_argument('--skip_vector_save', action='store_true', help='set to skip output vector creation')\n\n    # TODO, make flag to skip vrt\n    args = parser.parse_args()\n\n    process_02_inference(\n        code_dir=args.code_dir,\n        raw_data_dir=args.raw_data_dir,\n        processing_dir=args.processing_dir,\n        inference_dir=args.inference_dir,\n        model_dir=args.model_dir,\n        model=args.model,\n        use_gpu=args.use_gpu,\n        runs_per_gpu=args.runs_per_gpu,\n        max_images=args.max_images,\n        skip_vrt=args.skip_vrt,\n        skip_vector_save=args.skip_vector_save,\n    )\n</code></pre>"},{"location":"scripts/#src.thaw_slump_segmentation.scripts.process_02_inference.process_02_inference","title":"<code>process_02_inference(code_dir=Path('/isipd/projects/p_aicore_pf/initze/code/aicore_inference'), raw_data_dir=[Path('/isipd/projects/p_aicore_pf/initze/data/planet/planet_data_inference_grid/scenes'), Path('/isipd/projects/p_aicore_pf/initze/data/planet/planet_data_inference_grid/tiles')], processing_dir=Path('/isipd/projects/p_aicore_pf/initze/processing'), inference_dir=Path('/isipd/projects/p_aicore_pf/initze/processed/inference'), model_dir=Path('/isipd/projects/p_aicore_pf/initze/models/thaw_slumps'), model='RTS_v6_tcvis', use_gpu=[0], runs_per_gpu=5, max_images=None, skip_vrt=False, skip_vector_save=False)</code>","text":"<p>Script to run auto inference for RTS</p> Source code in <code>src/thaw_slump_segmentation/scripts/process_02_inference.py</code> <pre><code>def process_02_inference(\n    code_dir: Annotated[Path, typer.Option(help='Local code directory')] = Path(\n        '/isipd/projects/p_aicore_pf/initze/code/aicore_inference'\n    ),\n    raw_data_dir: Annotated[List[Path], typer.Option(help='Location of raw data')] = [\n        Path('/isipd/projects/p_aicore_pf/initze/data/planet/planet_data_inference_grid/scenes'),\n        Path('/isipd/projects/p_aicore_pf/initze/data/planet/planet_data_inference_grid/tiles'),\n    ],\n    processing_dir: Annotated[Path, typer.Option(help='Location for data processing')] = Path(\n        '/isipd/projects/p_aicore_pf/initze/processing'\n    ),\n    inference_dir: Annotated[Path, typer.Option(help='Target directory for inference results')] = Path(\n        '/isipd/projects/p_aicore_pf/initze/processed/inference'\n    ),\n    model_dir: Annotated[Path, typer.Option(help='Target directory for models')] = Path(\n        '/isipd/projects/p_aicore_pf/initze/models/thaw_slumps'\n    ),\n    model: Annotated[\n        str, typer.Option(help=\"Model name, examples ['RTS_v6_tcvis', 'RTS_v6_notcvis']\")\n    ] = 'RTS_v6_tcvis',\n    use_gpu: Annotated[List[int], typer.Option(help='List of GPU IDs to use, space separated')] = [0],\n    runs_per_gpu: Annotated[int, typer.Option(help='Number of runs per GPU')] = 5,\n    max_images: Annotated[int, typer.Option(help='Maximum number of images to process (optional)')] = None,\n    skip_vrt: Annotated[bool, typer.Option(help='set to skip DEM vrt creation')] = False,\n    skip_vector_save: Annotated[bool, typer.Option(help='set to skip output vector creation')] = False,\n):\n    \"\"\"Script to run auto inference for RTS\"\"\"\n    # ### List all files with properties\n    # TODO: run double for both paths\n    print('Checking processing status!')\n    # read processing status for raw data list\n    # TODO: check here - produces very large output when double checking\n    df_processing_status_list = [\n        get_processing_status(rdd, processing_dir, inference_dir, model) for rdd in raw_data_dir\n    ]\n\n    # get df for preprocessing\n    df_final = pd.concat(df_processing_status_list).drop_duplicates()\n\n    # TODO: move to function\n    # print basic information\n    total_images = int(len(df_final))\n    preprocessed_images = int(df_final.preprocessed.sum())\n    preprocessing_images = int(total_images - preprocessed_images)\n    finished_images = int(df_final.inference_finished.sum())\n    print(f'Number of images: {total_images}')\n    print(f'Number of preprocessed images: {preprocessed_images}')\n    print(f'Number of images for preprocessing: {preprocessing_images}')\n    print(f'Number of images for inference: {preprocessed_images - finished_images}')\n    print(f'Number of finished images: {finished_images}')\n\n    # TODO: images with processing status True but Inference False are crappy\n\n    if total_images == finished_images:\n        print('No processing needed: all images are already processed!')\n        return 0\n\n    ## Preprocessing\n    # #### Update Arctic DEM data\n    if skip_vrt:\n        print('Skipping Elevation VRT creation!')\n    else:\n        print('Updating Elevation VRTs!')\n        dem_data_dir = Path('/isipd/projects/p_aicore_pf/initze/data/ArcticDEM')\n        vrt_target_dir = Path('/isipd/projects/p_aicore_pf/initze/processing/auxiliary/ArcticDEM')\n        update_DEM2(dem_data_dir=dem_data_dir, vrt_target_dir=vrt_target_dir)\n\n    # #### Copy data for Preprocessing\n    # make better documentation\n\n    df_preprocess = df_final[df_final.preprocessed == False]\n    print(f'Number of images to preprocess: {len(df_preprocess)}')\n\n    # TODO make better check\n    # Cleanup processing directories to avoid incomplete processing\n    input_dir_dslist = list((processing_dir / 'input').glob('*'))\n    if len(input_dir_dslist) &gt; 0:\n        print(f\"Cleaning up {(processing_dir / 'input')}\")\n        for d in input_dir_dslist:\n            if len(list(d.glob('*'))) &lt; 4:\n                print('Delete', d)\n                shutil.rmtree(d)\n    else:\n        print('Processing directory is ready, nothing to do!')\n\n    # Copy Data\n    _ = df_preprocess.swifter.apply(lambda x: copy_unprocessed_files(x, processing_dir), axis=1)\n\n    # #### Run Preprocessing\n    import warnings\n\n    warnings.filterwarnings('ignore')\n\n    N_JOBS = 40\n    print(f'Preprocessing {len(df_preprocess)} images')  # fix this\n    if len(df_preprocess) &gt; 0:\n        pp_string = f'setup_raw_data --data_dir {processing_dir} --n_jobs {N_JOBS} --nolabel'\n        os.system(pp_string)\n\n    # ## Processing/Inference\n    # rerun processing status\n    df_processing_status2 = pd.concat(\n        [get_processing_status(rdd, processing_dir, inference_dir, model) for rdd in raw_data_dir]\n    ).drop_duplicates()\n    # Filter to images that are not preprocessed yet\n    df_process = df_final[df_final.inference_finished == False]\n    # update overview and filter accordingly - really necessary?\n    df_process_final = (\n        df_process.set_index('name')\n        .join(df_processing_status2[df_processing_status2['preprocessed']][['name']].set_index('name'), how='inner')\n        .reset_index(drop=False)\n        .iloc[:max_images]\n    )\n    # validate if images are correctly preprocessed\n    df_process_final['preprocessing_valid'] = (\n        df_process_final.apply(lambda x: len(list(x['path'].glob('*'))), axis=1) &gt;= 5\n    )\n    # final filtering process to remove incorrectly preprocessed data\n    df_process_final = df_process_final[df_process_final['preprocessing_valid']]\n\n    # TODO: check for empty files and processing\n    print(f'Number of images: {len(df_process_final)}')\n\n    # #### Parallel runs\n    # Make splits to distribute the processing\n    n_splits = len(use_gpu) * runs_per_gpu\n    df_split = np.array_split(df_process_final, n_splits)\n    gpu_split = use_gpu * runs_per_gpu\n\n    # for split in df_split:\n    #    print(f'Number of images: {len(split)}')\n\n    print('Run inference!')\n    # ### Parallel Inference execution\n    _ = Parallel(n_jobs=n_splits)(\n        delayed(run_inference)(\n            df_split[split],\n            model=model,\n            processing_dir=processing_dir,\n            inference_dir=inference_dir,\n            model_dir=model_dir,\n            gpu=gpu_split[split],\n            run=True,\n        )\n        for split in range(n_splits)\n    )\n    # #### Merge output files\n\n    if not skip_vector_save:\n        if len(df_process_final) &gt; 0:\n            # read all files which following the above defined threshold\n            flist = list((inference_dir / model).glob('*/*pred_binarized.shp'))\n            len(flist)\n\n            # Save output vectors to merged file\n            # load them in parallel\n            out = Parallel(n_jobs=6)(delayed(load_and_parse_vector)(f) for f in tqdm(flist[:]))\n\n            # merge them and save to geopackage file\n            merged_gdf = gpd.pd.concat(out)\n            save_file = inference_dir / model / f'{model}_merged.gpkg'\n\n            # check if file already exists, create backup file if exists\n            if save_file.exists():\n                # Get the current timestamp\n                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n                # Create the backup file name\n                save_file_bk = inference_dir / model / f'{model}_merged_bk_{timestamp}.gpkg'\n                print(f'Creating backup of file {save_file} to {save_file_bk}')\n                shutil.move(save_file, save_file_bk)\n\n            # save to files\n            print(f'Saving vectors to {save_file}')\n            merged_gdf.to_file(save_file)\n    else:\n        print('Skipping output vector creation!')\n</code></pre>"},{"location":"scripts/#src.thaw_slump_segmentation.scripts.process_03_ensemble.main","title":"<code>main()</code>","text":"Source code in <code>src/thaw_slump_segmentation/scripts/process_03_ensemble.py</code> <pre><code>def main():\n    # Add argument definitions\n    parser = argparse.ArgumentParser(\n        description='Script to run auto inference for RTS', formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n    parser.add_argument(\n        '--raw_data_dir',\n        type=Path,\n        default=Path('/isipd/projects/p_aicore_pf/initze/data/planet/planet_data_inference_grid/tiles'),\n        help='Location of raw data',\n    )\n    parser.add_argument(\n        '--processing_dir',\n        type=Path,\n        default=Path('/isipd/projects/p_aicore_pf/initze/processing'),\n        help='Location for data processing',\n    )\n    parser.add_argument(\n        '--inference_dir',\n        type=Path,\n        default=Path('/isipd/projects/p_aicore_pf/initze/processed/inference'),\n        help='Target directory for inference results',\n    )\n    parser.add_argument(\n        '--model_dir',\n        type=Path,\n        default=Path('/isipd/projects/p_aicore_pf/initze/models/thaw_slumps'),\n        help='Target directory for models',\n    )\n    parser.add_argument('--ensemble_name', type=str, default='RTS_v6_ensemble_v2', help='Target directory for models')\n    parser.add_argument(\n        '--model_names',\n        type=str,\n        nargs='+',\n        default=['RTS_v6_tcvis', 'RTS_v6_notcvis'],\n        help=\"Model name, examples ['RTS_v6_tcvis', 'RTS_v6_notcvis']\",\n    )\n    parser.add_argument('--gpu', type=int, default=0, help='GPU IDs to use for edge cleaning')\n    parser.add_argument('--n_jobs', type=int, default=15, help='number of CPU jobs for ensembling')\n    parser.add_argument(\n        '--n_vector_loaders', type=int, default=6, help='number of parallel vector loaders for final merge'\n    )\n    parser.add_argument('--max_images', type=int, default=None, help='Maximum number of images to process (optional)')\n    parser.add_argument(\n        '--vector_output_format',\n        type=str,\n        nargs='+',\n        default=['gpkg', 'parquet'],\n        help='Output format extension of ensembled vector files',\n    )\n    parser.add_argument(\n        '--ensemble_thresholds',\n        type=float,\n        nargs='+',\n        default=[0.4, 0.45, 0.5],\n        help='Thresholds for polygonized outputs of the ensemble, needs to be string, see examples',\n    )\n    parser.add_argument(\n        '--ensemble_border_size', type=int, default=10, help='Number of pixels to remove around the border and no data'\n    )\n    parser.add_argument('--ensemble_mmu', type=int, default=32, help='Minimum mapping unit of output objects in pixels')\n    parser.add_argument('--save_binary', action='store_true', help='set to keep intermediate binary rasters')\n    parser.add_argument('--save_probability', action='store_true', help='set to keep intermediate probability rasters')\n    parser.add_argument('--try_gpu', action='store_true', help='set to try image processing with gpu')\n    parser.add_argument(\n        '--force_vector_merge',\n        action='store_true',\n        help='force merging of output vectors even if no new ensemble tiles were processed',\n    )\n    parser.add_argument('--filter_water', action='store_true', help='set to remove polygons over water')\n\n    args = parser.parse_args()\n\n    process_03_ensemble(\n        raw_data_dir=args.raw_data_dir,\n        processing_dir=args.processing_dir,\n        inference_dir=args.inference_dir,\n        model_dir=args.model_dir,\n        ensemble_name=args.ensemble_name,\n        model_names=args.model_names,\n        gpu=args.gpu,\n        n_jobs=args.n_jobs,\n        n_vector_loaders=args.n_vector_loaders,\n        max_images=args.max_images,\n        vector_output_format=args.vector_output_format,\n        ensemble_thresholds=args.ensemble_thresholds,\n        ensemble_border_size=args.ensemble_border_size,\n        ensemble_mmu=args.ensemble_mmu,\n        try_gpu=args.try_gpu,\n        force_vector_merge=args.force_vector_merge,\n        save_binary=args.save_binary,\n        save_probability=args.save_probability,\n        filter_water=args.filter_water,\n    )\n</code></pre>"},{"location":"scripts/#src.thaw_slump_segmentation.scripts.process_03_ensemble.process_03_ensemble","title":"<code>process_03_ensemble(raw_data_dir=Path('/isipd/projects/p_aicore_pf/initze/data/planet/planet_data_inference_grid/tiles'), processing_dir=Path('/isipd/projects/p_aicore_pf/initze/processing'), inference_dir=Path('/isipd/projects/p_aicore_pf/initze/processed/inference'), model_dir=Path('/isipd/projects/p_aicore_pf/initze/models/thaw_slumps'), ensemble_name='RTS_v6_ensemble_v2', model_names=['RTS_v6_tcvis', 'RTS_v6_notcvis'], gpu=0, n_jobs=15, n_vector_loaders=6, max_images=None, vector_output_format=['gpkg', 'parquet'], ensemble_thresholds=[0.4, 0.45, 0.5], ensemble_border_size=10, ensemble_mmu=32, try_gpu=False, force_vector_merge=False, save_binary=False, save_probability=False, filter_water=False)</code>","text":"Source code in <code>src/thaw_slump_segmentation/scripts/process_03_ensemble.py</code> <pre><code>def process_03_ensemble(\n    raw_data_dir: Annotated[Path, typer.Option(help='Location of raw data')] = Path(\n        '/isipd/projects/p_aicore_pf/initze/data/planet/planet_data_inference_grid/tiles'\n    ),\n    processing_dir: Annotated[Path, typer.Option(help='Location for data processing')] = Path(\n        '/isipd/projects/p_aicore_pf/initze/processing'\n    ),\n    inference_dir: Annotated[Path, typer.Option(help='Target directory for inference results')] = Path(\n        '/isipd/projects/p_aicore_pf/initze/processed/inference'\n    ),\n    model_dir: Annotated[Path, typer.Option(help='Target directory for models')] = Path(\n        '/isipd/projects/p_aicore_pf/initze/models/thaw_slumps'\n    ),\n    ensemble_name: Annotated[str, typer.Option(help='Target directory for models')] = 'RTS_v6_ensemble_v2',\n    model_names: Annotated[List[str], typer.Option(help=\"Model name, examples ['RTS_v6_tcvis', 'RTS_v6_notcvis']\")] = [\n        'RTS_v6_tcvis',\n        'RTS_v6_notcvis',\n    ],\n    gpu: Annotated[int, typer.Option(help='GPU IDs to use for edge cleaning')] = 0,\n    n_jobs: Annotated[int, typer.Option(help='number of CPU jobs for ensembling')] = 15,\n    n_vector_loaders: Annotated[int, typer.Option(help='number of parallel vector loaders for final merge')] = 6,\n    max_images: Annotated[int, typer.Option(help='Maximum number of images to process (optional)')] = None,\n    vector_output_format: Annotated[\n        List[str], typer.Option(help='Output format extension of ensembled vector files')\n    ] = ['gpkg', 'parquet'],\n    ensemble_thresholds: Annotated[\n        List[float],\n        typer.Option(help='Thresholds for polygonized outputs of the ensemble, needs to be string, see examples'),\n    ] = [0.4, 0.45, 0.5],\n    ensemble_border_size: Annotated[\n        int, typer.Option(help='Number of pixels to remove around the border and no data')\n    ] = 10,\n    ensemble_mmu: Annotated[int, typer.Option(help='Minimum mapping unit of output objects in pixels')] = 32,\n    try_gpu: Annotated[bool, typer.Option(help='set to try image processing with gpu')] = False,\n    force_vector_merge: Annotated[\n        bool, typer.Option(help='force merging of output vectors even if no new ensemble tiles were processed')\n    ] = False,\n    save_binary: Annotated[bool, typer.Option(help='set to keep intermediate binary rasters')] = False,\n    save_probability: Annotated[bool, typer.Option(help='set to keep intermediate probability rasters')] = False,\n    filter_water: Annotated[bool, typer.Option(help='set to remove polygons over water')] = False,\n):\n    ### Start\n    # check if cucim is available\n    try:\n        import cucim  # type: ignore  # noqa: F401\n\n        if try_gpu:\n            try_gpu = True\n            print('Running ensembling with GPU!')\n        else:\n            try_gpu = False\n            print('Running ensembling with CPU!')\n    except Exception as e:\n        try_gpu = False\n        print(f'Cucim import failed: {e}')\n\n    # setup all params\n    kwargs_ensemble = {\n        'ensemblename': ensemble_name,\n        'inference_dir': inference_dir,\n        'modelnames': model_names,\n        'binary_threshold': ensemble_thresholds,\n        'border_size': ensemble_border_size,\n        'minimum_mapping_unit': ensemble_mmu,\n        'save_binary': save_binary,\n        'save_probability': save_probability,\n        'try_gpu': try_gpu,  # currently default to CPU only\n        'gpu': gpu,\n    }\n\n    # Check for finalized products\n    get_processing_status(raw_data_dir, processing_dir, inference_dir, model=kwargs_ensemble['ensemblename'])\n    df_ensemble_status = get_processing_status_ensemble(\n        inference_dir,\n        model_input_names=kwargs_ensemble['modelnames'],\n        model_ensemble_name=kwargs_ensemble['ensemblename'],\n    )\n    # Check which need to be process - check for already processed and invalid files\n    process = df_ensemble_status[df_ensemble_status['process']]\n    n_images = len(process.iloc[:max_images])\n    # #### Run Ensemble Merging\n    if len(process) &gt; 0:\n        print(f'Start running ensemble for {n_images} images with {n_jobs} parallel jobs!')\n        print(f'Target ensemble name: {ensemble_name}')\n        print(f'Source model output {model_names}')\n        _ = Parallel(n_jobs=n_jobs)(\n            delayed(create_ensemble_v2)(image_id=process.iloc[row]['name'], **kwargs_ensemble)\n            for row in tqdm(range(n_images))\n        )\n    else:\n        print(f'Skipped ensembling, all files ready for {ensemble_name}!')\n\n    # # #### run parallelized batch\n\n    if (len(process) &gt; 0) or force_vector_merge:\n        # ### Merge vectors to complete dataset\n        # set probability levels: 'class_05' means 50%, 'class_045' means 45%. This is the regex to search for vector naming\n        # proba_strings = args.ensemble_thresholds\n        # TODO: needs to be 'class_04',\n        proba_strings = [f'class_{thresh}'.replace('.', '') for thresh in ensemble_thresholds]\n\n        for proba_string in proba_strings:\n            # read all files which follow the above defined threshold\n            flist = list((inference_dir / ensemble_name).glob(f'*/*_{proba_string}.gpkg'))\n            len(flist)\n            # load them in parallel\n            print(f'Loading results {proba_string}')\n            out = Parallel(n_jobs=6)(\n                delayed(load_and_parse_vector)(f, filter_water=filter_water) for f in tqdm(flist[:max_images])\n            )\n            # merge them and save to geopackage file\n            print('Merging results')\n            merged_gdf = gpd.pd.concat(out)\n\n            # if filter_water:\n            #     # water removal here\n            #     merged_gdf = filter_remove_water(merged_gdf)\n\n            for vector_format in vector_output_format:\n                # Save output to vector\n                save_file = inference_dir / ensemble_name / f'merged_{proba_string}.{vector_format}'\n\n                # make file backup if necessary\n                if save_file.exists():\n                    # Get the current timestamp\n                    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n                    # Create the backup file name\n                    save_file_bk = (\n                        inference_dir / ensemble_name / f'merged_{proba_string}_bk_{timestamp}.{vector_format}'\n                    )\n                    print(f'Creating backup of file {save_file} to {save_file_bk}')\n                    shutil.move(save_file, save_file_bk)\n\n                # save to files\n                print(f'Saving vectors to {save_file}')\n                if vector_format in ['shp', 'gpkg']:\n                    merged_gdf.to_file(save_file)\n                elif vector_format in ['parquet']:\n                    merged_gdf.to_parquet(save_file)\n                else:\n                    print(f'Unknown format {vector_format}!')\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/__main__/","title":"main","text":""},{"location":"reference/src/thaw_slump_segmentation/data_loading/","title":"Data loading","text":""},{"location":"reference/src/thaw_slump_segmentation/data_loading/#src.thaw_slump_segmentation.data_loading.DataSources","title":"<code>DataSources</code>","text":"Source code in <code>src/thaw_slump_segmentation/data_loading.py</code> <pre><code>class DataSources:\n    DataSource = namedtuple('DataSource',\n            ['name', 'channels', 'normalization_factors'])\n    Planet = DataSource('planet', 4, [3000, 3000, 3000, 3000])\n    NDVI = DataSource('ndvi', 1, [20000])\n    TCVIS = DataSource('tcvis', 3, [255, 255, 255])\n    RelativeElevation = DataSource('relative_elevation', 1, [30000])\n    Slope = DataSource('slope', 1, [90])\n    LIST = list(sorted([Planet, NDVI, TCVIS, RelativeElevation, Slope]))\n    NAME2SOURCE = {src.name: src for src in LIST}\n\n    def __init__(self, sources):\n        sources = (self._get_source(source) for source in sources)\n        # Sort Data Sources to be independent of the order given in the config\n        self.sources = tuple(sorted(sources))\n\n    def _get_source(self, source):\n        if type(source) is str:\n            return DataSources.NAME2SOURCE[source]\n        elif type(source) is DataSources.DataSource:\n            return source\n        else:\n            raise ValueError(f\"Can't convert object {source}\"\n                             f\"of type {type(source)} to DataSource\")\n\n    @staticmethod\n    def all():\n        return DataSources([src.name for src in DataSources.LIST])\n\n    def __iter__(self):\n        return self.sources.__iter__()\n\n    def index(self, element):\n        return self.sources.index(element)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/data_loading/#src.thaw_slump_segmentation.data_loading.DataSources.DataSource","title":"<code>DataSource = namedtuple('DataSource', ['name', 'channels', 'normalization_factors'])</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/data_loading/#src.thaw_slump_segmentation.data_loading.DataSources.LIST","title":"<code>LIST = list(sorted([Planet, NDVI, TCVIS, RelativeElevation, Slope]))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/data_loading/#src.thaw_slump_segmentation.data_loading.DataSources.NAME2SOURCE","title":"<code>NAME2SOURCE = {src.name: srcfor src in LIST}</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/data_loading/#src.thaw_slump_segmentation.data_loading.DataSources.NDVI","title":"<code>NDVI = DataSource('ndvi', 1, [20000])</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/data_loading/#src.thaw_slump_segmentation.data_loading.DataSources.Planet","title":"<code>Planet = DataSource('planet', 4, [3000, 3000, 3000, 3000])</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/data_loading/#src.thaw_slump_segmentation.data_loading.DataSources.RelativeElevation","title":"<code>RelativeElevation = DataSource('relative_elevation', 1, [30000])</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/data_loading/#src.thaw_slump_segmentation.data_loading.DataSources.Slope","title":"<code>Slope = DataSource('slope', 1, [90])</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/data_loading/#src.thaw_slump_segmentation.data_loading.DataSources.TCVIS","title":"<code>TCVIS = DataSource('tcvis', 3, [255, 255, 255])</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/data_loading/#src.thaw_slump_segmentation.data_loading.DataSources.sources","title":"<code>sources = tuple(sorted(sources))</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/data_loading/#src.thaw_slump_segmentation.data_loading.DataSources.__init__","title":"<code>__init__(sources)</code>","text":"Source code in <code>src/thaw_slump_segmentation/data_loading.py</code> <pre><code>def __init__(self, sources):\n    sources = (self._get_source(source) for source in sources)\n    # Sort Data Sources to be independent of the order given in the config\n    self.sources = tuple(sorted(sources))\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/data_loading/#src.thaw_slump_segmentation.data_loading.DataSources.__iter__","title":"<code>__iter__()</code>","text":"Source code in <code>src/thaw_slump_segmentation/data_loading.py</code> <pre><code>def __iter__(self):\n    return self.sources.__iter__()\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/data_loading/#src.thaw_slump_segmentation.data_loading.DataSources.all","title":"<code>all()</code>  <code>staticmethod</code>","text":"Source code in <code>src/thaw_slump_segmentation/data_loading.py</code> <pre><code>@staticmethod\ndef all():\n    return DataSources([src.name for src in DataSources.LIST])\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/data_loading/#src.thaw_slump_segmentation.data_loading.DataSources.index","title":"<code>index(element)</code>","text":"Source code in <code>src/thaw_slump_segmentation/data_loading.py</code> <pre><code>def index(self, element):\n    return self.sources.index(element)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/data_loading/#src.thaw_slump_segmentation.data_loading.get_dataset","title":"<code>get_dataset(dataset, data_sources=None, data_root=None, augment=False, transform=None, augment_types=None)</code>","text":"Source code in <code>src/thaw_slump_segmentation/data_loading.py</code> <pre><code>def get_dataset(dataset, data_sources=None, data_root=None, augment=False, transform=None, augment_types=None):\n    if data_sources is None:\n        # Use all data sources by default\n        data_sources = DataSources.all()\n    data_sources = DataSources(data_sources)\n\n    ds_path = Path(data_root) / 'h5' / (str(dataset) + '.h5')\n    dataset = H5Dataset(ds_path, data_sources=data_sources)\n    if augment:\n        dataset = Augment(dataset, augment_types=augment_types)\n    if transform is not None:\n        dataset = Transformed(dataset, transform)\n    return dataset\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/data_loading/#src.thaw_slump_segmentation.data_loading.get_loader","title":"<code>get_loader(scenes, batch_size, augment=False, augment_types=None, shuffle=False, num_workers=0, data_sources=None, data_root=None, transform=None)</code>","text":"Source code in <code>src/thaw_slump_segmentation/data_loading.py</code> <pre><code>def get_loader(scenes, batch_size, augment=False, augment_types=None, shuffle=False, num_workers=0, data_sources=None, data_root=None, transform=None):\n    if transform is None:\n        transform = make_scaling(data_sources)\n    scenes = [get_dataset(ds, data_sources=data_sources, data_root=data_root, augment=augment, augment_types=None, transform=transform) for ds in scenes]\n    concatenated = ConcatDataset(scenes)\n    return DataLoader(concatenated, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, pin_memory=True)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/data_loading/#src.thaw_slump_segmentation.data_loading.get_slump_loader","title":"<code>get_slump_loader(scenes, batch_size, augment=False, augment_types=None, shuffle=False, num_workers=0, data_sources=None, data_root=None, transform=None)</code>","text":"Source code in <code>src/thaw_slump_segmentation/data_loading.py</code> <pre><code>def get_slump_loader(scenes, batch_size, augment=False, augment_types=None, shuffle=False, num_workers=0, data_sources=None, data_root=None, transform=None):\n    logger = get_logger('data_loading')\n\n    if transform is None:\n        transform = make_scaling(data_sources)\n    filtered_sets = []\n    logger.info(\"Start calculating slump only dataset.\")\n    for scene in tqdm(scenes):\n        data = get_dataset(scene, data_sources=data_sources, data_root=data_root)\n        subset = [i for i in range(len(data)) if data[i][1].max() &gt; 0]\n        filtered_sets.append(Subset(data, subset))\n\n    dataset = ConcatDataset(filtered_sets)\n    if augment:\n        dataset = Augment(dataset, augment_types=augment_types)\n    dataset = Transformed(dataset, transform)\n    logger.info(\"Done calculating slump only dataset.\")\n    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, pin_memory=True)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/data_loading/#src.thaw_slump_segmentation.data_loading.get_vis_loader","title":"<code>get_vis_loader(vis_config, batch_size, data_sources=None, data_root=None)</code>","text":"Source code in <code>src/thaw_slump_segmentation/data_loading.py</code> <pre><code>def get_vis_loader(vis_config, batch_size, data_sources=None, data_root=None):\n    vis_names = []\n    vis_datasets = []\n    for scene, indices in vis_config.items():\n        dataset = get_dataset(scene, data_sources=data_sources, data_root=data_root)\n        vis_names += [f'{scene}-{i}' for i in indices]\n        filtered = Subset(dataset, indices)\n        vis_datasets.append(filtered)\n    vis_data = ConcatDataset(vis_datasets)\n    vis_data = Transformed(vis_data, make_scaling(data_sources))\n\n    loader = DataLoader(vis_data, batch_size=batch_size, shuffle=False, pin_memory=True)\n    return loader, vis_names\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/data_loading/#src.thaw_slump_segmentation.data_loading.make_scaling","title":"<code>make_scaling(data_sources=None)</code>","text":"Source code in <code>src/thaw_slump_segmentation/data_loading.py</code> <pre><code>def make_scaling(data_sources=None):\n    if data_sources is None:\n        # Use all data sources by default\n        data_sources = DataSources.all()\n    # Sort Data Sources to be independent of the order given in the config\n    data_sources = DataSources(data_sources)\n\n    factors = []\n    for source in data_sources:\n        factors += source.normalization_factors\n\n    normalize = 1 / torch.tensor(factors, dtype=torch.float32).reshape(-1, 1, 1)\n    return Scaling(normalize)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/loss_functions/","title":"Loss functions","text":""},{"location":"reference/src/thaw_slump_segmentation/loss_functions/#src.thaw_slump_segmentation.loss_functions.auto_ce","title":"<code>auto_ce(y_hat, y)</code>","text":"Source code in <code>src/thaw_slump_segmentation/loss_functions.py</code> <pre><code>def auto_ce(y_hat, y):\n    with torch.no_grad():\n        C = y_hat.shape[1]\n        counts = torch.stack([(y == i).float().mean() for i in range(C)])\n        weights = 1.0 / (C * counts)\n    return F.cross_entropy(y_hat, y.squeeze(1), weight=weights)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/loss_functions/#src.thaw_slump_segmentation.loss_functions.get_loss","title":"<code>get_loss(loss_args)</code>","text":"Source code in <code>src/thaw_slump_segmentation/loss_functions.py</code> <pre><code>def get_loss(loss_args):\n    loss_type = loss_args['type']\n    if loss_type in ('CrossEntropy') :\n        loss_class = torch.nn.BCEWithLogitsLoss\n        args = dict()\n        if 'weights' in loss_args:\n            args['weight'] = torch.tensor(loss_args['weights'])\n    elif loss_type == 'AutoCE':\n        return auto_ce\n    else:\n        print(f\"No Loss of type {loss_type} known\")\n\n    return loss_class(**args)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/main/","title":"Main","text":""},{"location":"reference/src/thaw_slump_segmentation/main/#src.thaw_slump_segmentation.main.cli","title":"<code>cli = typer.Typer(pretty_exceptions_show_locals=False)</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/main/#src.thaw_slump_segmentation.main.data_cli","title":"<code>data_cli = typer.Typer()</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/main/#src.thaw_slump_segmentation.main.process_cli","title":"<code>process_cli = typer.Typer()</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/main/#src.thaw_slump_segmentation.main.hello","title":"<code>hello(name)</code>","text":"Source code in <code>src/thaw_slump_segmentation/main.py</code> <pre><code>@cli.command()\ndef hello(name: str):\n    typer.echo(f'Hello, {name}!')\n    return f'Hello, {name}!'\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/postprocessing/","title":"Postprocessing","text":""},{"location":"reference/src/thaw_slump_segmentation/postprocessing/#src.thaw_slump_segmentation.postprocessing.CUCIM_AVAILABLE","title":"<code>CUCIM_AVAILABLE = True</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/postprocessing/#src.thaw_slump_segmentation.postprocessing.copy_unprocessed_files","title":"<code>copy_unprocessed_files(row, processing_dir, quiet=True)</code>","text":"<p>Copies unprocessed files from a source directory to a target directory.</p> <p>This function checks if a file exists in the target directory. If the file does not exist, it copies the file from the source to the target directory. If the file already exists in the target directory, it skips the copying process.</p> <p>Parameters: row (dict): A dictionary containing file information. It should have a key 'path' which corresponds to the file's path. processing_dir (Path): The target directory where the files should be copied to. It should be a pathlib.Path object. quiet (bool, optional): If set to True, the function will not print any output during its execution. If set to False, the function will print the status of the file copying process. Default is True.</p> <p>Returns: None</p> Source code in <code>src/thaw_slump_segmentation/postprocessing.py</code> <pre><code>def copy_unprocessed_files(row, processing_dir, quiet=True):\n    \"\"\"\n    Copies unprocessed files from a source directory to a target directory.\n\n    This function checks if a file exists in the target directory. If the file does not exist, it copies the file from the source to the target directory. If the file already exists in the target directory, it skips the copying process.\n\n    Parameters:\n    row (dict): A dictionary containing file information. It should have a key 'path' which corresponds to the file's path.\n    processing_dir (Path): The target directory where the files should be copied to. It should be a pathlib.Path object.\n    quiet (bool, optional): If set to True, the function will not print any output during its execution. If set to False, the function will print the status of the file copying process. Default is True.\n\n    Returns:\n    None\n    \"\"\"\n    inpath = row['path']\n    outpath = processing_dir / 'input' / inpath.name\n\n    if not outpath.exists():\n        if not quiet:\n            print(f'Start copying {inpath.name} to {outpath}')\n        shutil.copytree(inpath, outpath)\n    else:\n        if not quiet:\n            print(f'Skipped copying {inpath.name}')\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/postprocessing/#src.thaw_slump_segmentation.postprocessing.create_ensemble","title":"<code>create_ensemble(inference_dir, modelnames, ensemblename, image_id, binary_threshold=[0.3, 0.4, 0.5], delete_proba=True, delete_binary=True)</code>","text":"<p>Calculate the mean of two model predictions and write the output to disk.</p> <p>Args: modelnames (List[str]): A list of two model names. ensemblename (str): The name of the ensemble model. image_id (str): The ID of the image. binary_threshold (float): The binary threshold value.</p> <p>Returns: None</p> Source code in <code>src/thaw_slump_segmentation/postprocessing.py</code> <pre><code>def create_ensemble(\n    inference_dir: Path,\n    modelnames: List[str],\n    ensemblename: str,\n    image_id: str,\n    binary_threshold: list = [0.3, 0.4, 0.5],\n    delete_proba=True,\n    delete_binary=True,\n):\n    \"\"\"\n    Calculate the mean of two model predictions and write the output to disk.\n\n    Args:\n    modelnames (List[str]): A list of two model names.\n    ensemblename (str): The name of the ensemble model.\n    image_id (str): The ID of the image.\n    binary_threshold (float): The binary threshold value.\n\n    Returns:\n    None\n    \"\"\"\n    try:\n        # setup\n        outpath = inference_dir / ensemblename / image_id / f'{image_id}_{ensemblename}_proba.tif'\n        os.makedirs(outpath.parent, exist_ok=True)\n\n        # calculate\n        image1 = inference_dir / modelnames[0] / image_id / 'pred_probability.tif'\n        image2 = inference_dir / modelnames[1] / image_id / 'pred_probability.tif'\n\n        with rasterio.open(image1) as src1:\n            with rasterio.open(image2) as src2:\n                a1 = src1.read()\n                a2 = src2.read()\n\n            out_meta = src1.meta.copy()\n            out_meta_binary = out_meta.copy()\n            out_meta_binary['dtype'] = 'uint8'\n\n        out = np.mean([a1, a2], axis=0)\n        with rasterio.open(outpath, 'w', **out_meta) as target:\n            target.write(out)\n\n        # write binary raster\n        for threshold in binary_threshold:\n            thresh_str = str(threshold).replace('.', '')\n            outpath_class = Path(str(outpath).replace('proba', f'class_{thresh_str}'))\n            outpath_shp = outpath_class.with_suffix('.gpkg')\n\n            out_binary = out &gt;= threshold\n\n            with rasterio.open(outpath_class, 'w', **out_meta_binary, compress='deflate') as target:\n                target.write(out_binary)\n\n            # make vector\n            s_polygonize = f'gdal_polygonize.py {outpath_class} -q -mask {outpath_class} -f \"GPKG\" {outpath_shp}'\n            os.system(s_polygonize)\n            if delete_binary:\n                os.remove(outpath_class)\n\n        # delete files\n        if delete_proba:\n            os.remove(outpath)\n\n        return 0\n\n    except:\n        return 1\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/postprocessing/#src.thaw_slump_segmentation.postprocessing.create_ensemble_v2","title":"<code>create_ensemble_v2(inference_dir, modelnames, ensemblename, image_id, binary_threshold=[0.5], border_size=10, minimum_mapping_unit=32, save_binary=False, save_probability=False, try_gpu=True, gpu=0)</code>","text":"<p>Create an ensemble result from multiple model predictions, generate binary masks, and process the output.</p>"},{"location":"reference/src/thaw_slump_segmentation/postprocessing/#src.thaw_slump_segmentation.postprocessing.create_ensemble_v2--parameters","title":"Parameters:","text":"<p>inference_dir : Path     Path to the directory containing inference data.</p> List[str] <p>List of model names to be used for creating the ensemble.</p> str <p>Name of the ensemble result.</p> str <p>Identifier for the image being processed.</p> list, optional <p>List of binary threshold values, by default [0.5].</p> int, optional <p>Border size for edge masking and dilation, by default 10.</p> int, optional <p>Minimum size of mapping units to retain, by default 32.</p> bool, optional <p>Whether to delete the binary file after processing, by default True.</p> bool, optional <p>Whether to save the probability file after processing, by default False.</p>"},{"location":"reference/src/thaw_slump_segmentation/postprocessing/#src.thaw_slump_segmentation.postprocessing.create_ensemble_v2--returns","title":"Returns:","text":"<p>None</p> Source code in <code>src/thaw_slump_segmentation/postprocessing.py</code> <pre><code>def create_ensemble_v2(\n    inference_dir: Path,\n    modelnames: List[str],\n    ensemblename: str,\n    image_id: str,\n    binary_threshold: list = [0.5],\n    border_size: int = 10,\n    minimum_mapping_unit: int = 32,\n    save_binary: bool = False,\n    save_probability: bool = False,\n    try_gpu: bool = True,\n    gpu: int = 0,\n):\n    \"\"\"\n    Create an ensemble result from multiple model predictions, generate binary masks, and process the output.\n\n    Parameters:\n    ------------\n    inference_dir : Path\n        Path to the directory containing inference data.\n\n    modelnames : List[str]\n        List of model names to be used for creating the ensemble.\n\n    ensemblename : str\n        Name of the ensemble result.\n\n    image_id : str\n        Identifier for the image being processed.\n\n    binary_threshold : list, optional\n        List of binary threshold values, by default [0.5].\n\n    border_size : int, optional\n        Border size for edge masking and dilation, by default 10.\n\n    minimum_mapping_unit : int, optional\n        Minimum size of mapping units to retain, by default 32.\n\n    delete_binary : bool, optional\n        Whether to delete the binary file after processing, by default True.\n\n    save_probability : bool, optional\n        Whether to save the probability file after processing, by default False.\n\n    Returns:\n    ------------\n    None\n    \"\"\"\n\n    def calculate_mean_image(images):\n        ctr = 0\n        list_data = []\n\n        for image in images:\n            with rasterio.open(image) as src:\n                data = src.read()\n                if ctr == 0:\n                    out_meta = src.meta.copy()\n                    out_meta_binary = out_meta.copy()\n                    out_meta_binary['dtype'] = 'uint8'\n            list_data.append(data)\n            ctr += 1\n\n        mean_image = np.mean(list_data, axis=0)\n        return mean_image, out_meta_binary, out_meta\n\n    def dilate_data_mask(mask, size=10):\n        selem = disk(size)\n        return binary_dilation(mask, selem)\n\n    def dilate_data_mask_gpu(mask, size=10):\n        mask = cp.array(mask)\n        selem = disk_gpu(size)\n        return cp.asnumpy(binary_dilation_gpu(mask, selem))\n\n    def mask_edges(input_mask, size=10):\n        input_mask[:size, :] = True\n        input_mask[:, :size] = True\n        input_mask[-size:, :] = True\n        input_mask[:, -size:] = True\n        return input_mask\n\n    images = [inference_dir / model / image_id / 'pred_probability.tif' for model in modelnames]\n    for image in images:\n        if not image.exists():\n            print(f'{image.as_posix()} does not exist')\n            return None\n\n    try:\n        mean_image, out_meta_binary, out_meta_probability = calculate_mean_image(images)\n    except:\n        print(f'Read error of files {images}')\n        return None\n\n    # save probability layer, if specified\n    if save_probability:\n        outpath_proba = outpath = inference_dir / ensemblename / image_id / f'{image_id}_{ensemblename}_probability.tif'\n        os.makedirs(outpath_proba.parent, exist_ok=True)\n        with rasterio.open(outpath_proba, 'w', **out_meta_probability) as target:\n            target.write(mean_image)\n\n    for threshold in binary_threshold:\n        # get binary object mask\n        objects = mean_image[0] &gt;= threshold\n\n        # get individual numbered objects\n        labels = label(objects, connectivity=2)\n\n        # retrieve noData mask from image\n        mask = np.array(np.isnan(mean_image[0]), np.uint8)\n        # mask = np.array(np.isnan(mean_image), np.uint8)[0]\n        # grow nodata mask around no data\n        if CUCIM_AVAILABLE and try_gpu:\n            cp.cuda.Device(gpu).use()\n            dilated_mask = dilate_data_mask_gpu(mask, size=border_size)\n        else:\n            dilated_mask = dilate_data_mask(mask, size=border_size)\n        # remove fixed sizes along image edges\n        final_mask = mask_edges(dilated_mask, size=border_size)\n\n        # get label ids which are in the edge region\n        edge_labels = np.unique(labels * final_mask)\n        # check which labels intersect with new mask\n        isin_data_area = ~np.isin(labels, edge_labels)\n        # remove labels intersecting new noData and create final binary mask\n        out_binary = np.expand_dims((labels * isin_data_area &gt; 0), 0)\n        # remove small objects 32 px ~ 100m\u00b2\n        out_binary = remove_small_objects(out_binary, min_size=minimum_mapping_unit)\n\n        ### OUTPUTS\n        # set paths\n        thresh_str = str(threshold).replace('.', '')\n\n        outpath = inference_dir / ensemblename / image_id / f'{image_id}_{ensemblename}_class_{thresh_str}.tif'\n        # outpath_class = Path(str(outpath).replace('proba', f'class_{thresh_str}'))\n        outpath_shp = outpath.with_suffix('.gpkg')\n\n        # write binary file\n        os.makedirs(outpath.parent, exist_ok=True)\n        with rasterio.open(outpath, 'w', **out_meta_binary, compress='deflate') as target:\n            target.write(out_binary)\n\n        # make vector\n        s_polygonize = f'gdal_polygonize.py {outpath} -q -mask {outpath} -f \"GPKG\" {outpath_shp}'\n        os.system(s_polygonize)\n\n        if not save_binary:\n            os.remove(outpath)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/postprocessing/#src.thaw_slump_segmentation.postprocessing.create_ensemble_with_negative","title":"<code>create_ensemble_with_negative(inference_dir, modelnames, ensemblename, image_id, binary_threshold=[0.3, 0.4, 0.5], negative_modelname=None, negative_binary_threshold=0.8, delete_proba=True, delete_binary=True, erode_pixels=None)</code>","text":"<p>Calculate the ensemble prediction, including optional negative class, and write outputs to disk.</p> <p>Parameters:</p> Name Type Description Default <code>inference_dir</code> <code>Path</code> <p>Directory path for inference results.</p> required <code>modelnames</code> <code>List[str]</code> <p>List of two model names for ensemble.</p> required <code>ensemblename</code> <code>str</code> <p>Name of the ensemble model.</p> required <code>image_id</code> <code>str</code> <p>ID of the image.</p> required <code>binary_threshold</code> <code>List[float]</code> <p>List of binary threshold values.</p> <code>[0.3, 0.4, 0.5]</code> <code>negative_modelname</code> <code>str</code> <p>Name of the negative class model directory.</p> <code>None</code> <code>negative_binary_threshold</code> <code>float</code> <p>Binary threshold for negative class.</p> <code>0.8</code> <code>delete_proba</code> <code>bool</code> <p>Whether to delete the probability file after processing.</p> <code>True</code> <code>delete_binary</code> <code>bool</code> <p>Whether to delete binary files after creating vectors.</p> <code>True</code> <code>erode_pixels</code> <code>int</code> <p>Number of pixels for edge erosion.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>out_binary</code> <code>ndarray</code> <p>Binary result array.</p> Source code in <code>src/thaw_slump_segmentation/postprocessing.py</code> <pre><code>def create_ensemble_with_negative(\n    inference_dir: Path,\n    modelnames: List[str],\n    ensemblename: str,\n    image_id: str,\n    binary_threshold: list = [0.3, 0.4, 0.5],\n    negative_modelname: Optional[str] = None,\n    negative_binary_threshold: float = 0.8,\n    delete_proba: bool = True,\n    delete_binary: bool = True,\n    erode_pixels: Optional[int] = None,\n):\n    \"\"\"\n    Calculate the ensemble prediction, including optional negative class, and write outputs to disk.\n\n    Args:\n        inference_dir (Path): Directory path for inference results.\n        modelnames (List[str]): List of two model names for ensemble.\n        ensemblename (str): Name of the ensemble model.\n        image_id (str): ID of the image.\n        binary_threshold (List[float], optional): List of binary threshold values.\n        negative_modelname (str, optional): Name of the negative class model directory.\n        negative_binary_threshold (float, optional): Binary threshold for negative class.\n        delete_proba (bool, optional): Whether to delete the probability file after processing.\n        delete_binary (bool, optional): Whether to delete binary files after creating vectors.\n        erode_pixels (int, optional): Number of pixels for edge erosion.\n\n    Returns:\n        out_binary (numpy.ndarray): Binary result array.\n    \"\"\"\n    # setup\n    outpath = inference_dir / ensemblename / image_id / f'{image_id}_{ensemblename}_proba.tif'\n    os.makedirs(outpath.parent, exist_ok=True)\n\n    # calculate\n    image1 = inference_dir / modelnames[0] / image_id / 'pred_probability.tif'\n    image2 = inference_dir / modelnames[1] / image_id / 'pred_probability.tif'\n\n    try:\n        with rasterio.open(image1) as src1:\n            with rasterio.open(image2) as src2:\n                a1 = src1.read()\n                a2 = src2.read()\n\n            out_meta = src1.meta.copy()\n            out_meta_binary = out_meta.copy()\n            out_meta_binary['dtype'] = 'uint8'\n\n        # calculate mean of all datasets\n        out = np.mean([a1, a2], axis=0)\n\n        if erode_pixels:\n            # get mask\n            mask = np.array(np.isnan(a1), np.uint8)[0]\n            selem = disk(erode_pixels)\n            # Apply erosion to the mask\n            eroded_mask = dilation(mask, selem)\n\n            # apply eroded_mask\n            r, c = np.where(eroded_mask)\n            out[0, r, c] = np.nan\n\n            out[0, :erode_pixels, :] = np.nan\n            out[0, -erode_pixels:, :] = np.nan\n            out[0, :, :erode_pixels] = np.nan\n            out[0, :, -erode_pixels:] = np.nan\n\n        # optional if negative thresh\n        if negative_modelname:\n            image_negative = inference_dir / negative_modelname / image_id / 'pred_probability.tif'\n            with rasterio.open(image_negative) as src_negative:\n                a_negative = src_negative.read()\n                a_binary = a_negative &lt; negative_binary_threshold\n                out *= a_binary\n\n        with rasterio.open(outpath, 'w', **out_meta) as target:\n            target.write(out)\n\n        # write binary raster\n        for threshold in binary_threshold:\n            thresh_str = str(threshold).replace('.', '')\n            outpath_class = Path(str(outpath).replace('proba', f'class_{thresh_str}'))\n            outpath_shp = outpath_class.with_suffix('.gpkg')\n\n            out_binary = out &gt;= threshold\n            with rasterio.open(outpath_class, 'w', **out_meta_binary, compress='deflate') as target:\n                target.write(out_binary)\n\n            # make vector\n            s_polygonize = f'gdal_polygonize.py {outpath_class} -q -mask {outpath_class} -f \"GPKG\" {outpath_shp}'\n            os.system(s_polygonize)\n            if delete_binary:\n                os.remove(outpath_class)\n\n        # delete proba file\n        if delete_proba:\n            os.remove(outpath)\n\n        return out_binary\n\n    except:\n        return None\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/postprocessing/#src.thaw_slump_segmentation.postprocessing.filter_remove_water","title":"<code>filter_remove_water(gdf, threshold=0.2)</code>","text":"<p>Filter a GeoDataFrame to remove features with high water coverage based on ESRI Global Land Use Land Cover data.</p> <p>This function converts the input GeoDataFrame to an Earth Engine FeatureCollection, uses ESRI's Global LULC 10m dataset to create a binary water mask, calculates the mean water coverage for each feature, and filters out features exceeding the specified water coverage threshold.</p>"},{"location":"reference/src/thaw_slump_segmentation/postprocessing/#src.thaw_slump_segmentation.postprocessing.filter_remove_water--parameters","title":"Parameters:","text":"<p>gdf : geopandas.GeoDataFrame     Input GeoDataFrame containing the features to be filtered. threshold : float, optional     The maximum allowable proportion of water coverage for a feature to be retained.     Features with mean water coverage exceeding this threshold will be removed.     Default is 0.2 (20% water coverage).</p>"},{"location":"reference/src/thaw_slump_segmentation/postprocessing/#src.thaw_slump_segmentation.postprocessing.filter_remove_water--returns","title":"Returns:","text":"<p>geopandas.GeoDataFrame     A filtered GeoDataFrame containing only the features with water coverage     below or equal to the specified threshold.</p>"},{"location":"reference/src/thaw_slump_segmentation/postprocessing/#src.thaw_slump_segmentation.postprocessing.filter_remove_water--notes","title":"Notes:","text":"<ul> <li>This function requires Earth Engine to be initialized.</li> <li>It uses the ESRI Global Land Use Land Cover 10m dataset from Earth Engine.</li> <li>The water mask is created by identifying pixels with a value of 1 in the LULC dataset.</li> <li>The function assumes a scale of 10 meters for calculations, matching the LULC dataset resolution.</li> </ul>"},{"location":"reference/src/thaw_slump_segmentation/postprocessing/#src.thaw_slump_segmentation.postprocessing.filter_remove_water--example","title":"Example:","text":"<p>import geopandas as gpd import ee ee.Initialize() gdf = gpd.read_file('path/to/your/shapefile.shp') filtered_gdf = filter_remove_water(gdf, threshold=0.2)</p> Source code in <code>src/thaw_slump_segmentation/postprocessing.py</code> <pre><code>def filter_remove_water(gdf, threshold=0.2):\n    \"\"\"\n    Filter a GeoDataFrame to remove features with high water coverage based on ESRI Global Land Use Land Cover data.\n\n    This function converts the input GeoDataFrame to an Earth Engine FeatureCollection,\n    uses ESRI's Global LULC 10m dataset to create a binary water mask, calculates the\n    mean water coverage for each feature, and filters out features exceeding the specified\n    water coverage threshold.\n\n    Parameters:\n    -----------\n    gdf : geopandas.GeoDataFrame\n        Input GeoDataFrame containing the features to be filtered.\n    threshold : float, optional\n        The maximum allowable proportion of water coverage for a feature to be retained.\n        Features with mean water coverage exceeding this threshold will be removed.\n        Default is 0.2 (20% water coverage).\n\n    Returns:\n    --------\n    geopandas.GeoDataFrame\n        A filtered GeoDataFrame containing only the features with water coverage\n        below or equal to the specified threshold.\n\n    Notes:\n    ------\n    - This function requires Earth Engine to be initialized.\n    - It uses the ESRI Global Land Use Land Cover 10m dataset from Earth Engine.\n    - The water mask is created by identifying pixels with a value of 1 in the LULC dataset.\n    - The function assumes a scale of 10 meters for calculations, matching the LULC dataset resolution.\n\n    Example:\n    --------\n    &gt;&gt;&gt; import geopandas as gpd\n    &gt;&gt;&gt; import ee\n    &gt;&gt;&gt; ee.Initialize()\n    &gt;&gt;&gt; gdf = gpd.read_file('path/to/your/shapefile.shp')\n    &gt;&gt;&gt; filtered_gdf = filter_remove_water(gdf, threshold=0.2)\n    \"\"\"\n\n    # convert gdf to ee FC\n    rts_ee = ee.FeatureCollection(gdf.__geo_interface__)\n    # load gee layer\n    esri_lulc2020 = ee.ImageCollection('projects/sat-io/open-datasets/landcover/ESRI_Global-LULC_10m')\n    # filter to rts footprint\n    filtered = esri_lulc2020.filterBounds(rts_ee).mosaic()\n    # get binary water mask\n    water_layer = filtered.eq(1)\n    data_mask = water_layer.mask().eq(0)\n    # create final water mask : either water or nodata (assumed that no data is over the sea)\n    water_mask = water_layer.unmask().Or(data_mask)\n    # reduce regions and get value\n    reduced = ee.Image.reduceRegions(water_mask, rts_ee, reducer=ee.Reducer.mean(), scale=10)\n    # convert to gdf\n    gdf_out = ee.data.computeFeatures({'expression': reduced, 'fileFormat': 'GEOPANDAS_GEODATAFRAME'})\n    # filter to no water\n    gdf_filtered = gdf.loc[gdf_out.query(f'mean &lt;= {threshold}').index]\n\n    return gdf_filtered\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/postprocessing/#src.thaw_slump_segmentation.postprocessing.get_PS_products_type","title":"<code>get_PS_products_type(name)</code>","text":"Source code in <code>src/thaw_slump_segmentation/postprocessing.py</code> <pre><code>def get_PS_products_type(name):\n    if len(name.split('_')) == 3:\n        return 'PSScene'\n    elif len(name.split('_')) == 4:\n        if len(name.split('_')[0]) == 8:\n            return 'PSScene'\n        else:\n            return 'PSOrthoTile'\n    else:\n        None\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/postprocessing/#src.thaw_slump_segmentation.postprocessing.get_datasets","title":"<code>get_datasets(path, depth=0, preprocessed=False)</code>","text":"Source code in <code>src/thaw_slump_segmentation/postprocessing.py</code> <pre><code>def get_datasets(path, depth=0, preprocessed=False):\n    dirs = listdirs2(path, depth=depth)\n    df = pd.DataFrame(data=dirs, columns=['path'])\n    if len(df) &gt; 0:\n        df['name'] = df.apply(lambda x: x['path'].name, axis=1)\n        df['preprocessed'] = preprocessed\n        df['PS_product_type'] = df.apply(lambda x: get_PS_products_type(x['name']), axis=1)\n        df['image_date'] = df.apply(lambda x: get_date_from_PSfilename(x['name']), axis=1)\n        df['tile_id'] = df.apply(lambda x: x['name'].split('_')[1], axis=1)\n        return df\n    else:\n        return pd.DataFrame(columns=['path', 'name', 'preprocessed', 'PS_product_type', 'image_date', 'tile_id'])\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/postprocessing/#src.thaw_slump_segmentation.postprocessing.get_date_from_PSfilename","title":"<code>get_date_from_PSfilename(name)</code>","text":"Source code in <code>src/thaw_slump_segmentation/postprocessing.py</code> <pre><code>def get_date_from_PSfilename(name):\n    date = name.split('_')[2]\n    return date\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/postprocessing/#src.thaw_slump_segmentation.postprocessing.get_processing_status","title":"<code>get_processing_status(raw_data_dir, processing_dir, inference_dir, model, reduce_to_raw=False)</code>","text":"<p>Get the processing status of raw data, intermediate data, and inference results.</p> <p>Parameters:</p> Name Type Description Default <code>raw_data_dir</code> <code>Path</code> <p>Path to the directory containing raw input data (tiles or scenes).</p> required <code>processing_dir</code> <code>Path</code> <p>Path to the directory containing processed intermediate data.</p> required <code>inference_dir</code> <code>Path</code> <p>Path to the directory containing inference results.</p> required <code>model</code> <code>str</code> <p>Name of the model used for inference.</p> required <code>reduce_to_raw</code> <code>bool</code> <p>If True, return only the raw data that hasn't been processed yet. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <p>pandas.DataFrame: A DataFrame containing the processing status of each dataset, with columns: - 'name': Name of the dataset. - 'path': Path to the dataset. - 'inference_finished': Boolean indicating if inference has been completed for the dataset.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provided raw_data_dir is not 'tiles' or 'scenes'.</p> Notes <ul> <li>The function assumes that the processed intermediate data is located in <code>processing_dir/tiles</code>.</li> <li>The function checks if at least 5 files are available for each processed dataset.</li> <li>The function assumes that the inference results are located in <code>inference_dir/model/*</code>.</li> <li>There are two TODO comments in the code that need to be addressed.</li> </ul> Source code in <code>src/thaw_slump_segmentation/postprocessing.py</code> <pre><code>def get_processing_status(raw_data_dir, processing_dir, inference_dir, model, reduce_to_raw=False):\n    \"\"\"\n    Get the processing status of raw data, intermediate data, and inference results.\n\n    Args:\n        raw_data_dir (Path): Path to the directory containing raw input data (tiles or scenes).\n        processing_dir (Path): Path to the directory containing processed intermediate data.\n        inference_dir (Path): Path to the directory containing inference results.\n        model (str): Name of the model used for inference.\n        reduce_to_raw (bool, optional): If True, return only the raw data that hasn't been processed yet.\n            Default is False.\n\n    Returns:\n        pandas.DataFrame: A DataFrame containing the processing status of each dataset, with columns:\n            - 'name': Name of the dataset.\n            - 'path': Path to the dataset.\n            - 'inference_finished': Boolean indicating if inference has been completed for the dataset.\n\n    Raises:\n        ValueError: If the provided raw_data_dir is not 'tiles' or 'scenes'.\n\n    Notes:\n        - The function assumes that the processed intermediate data is located in `processing_dir/tiles`.\n        - The function checks if at least 5 files are available for each processed dataset.\n        - The function assumes that the inference results are located in `inference_dir/model/*`.\n        - There are two TODO comments in the code that need to be addressed.\n    \"\"\"\n    # get processing status for raw input data\n    if raw_data_dir.name == 'tiles':\n        df_raw = get_datasets(raw_data_dir, depth=1)\n    elif raw_data_dir.name == 'scenes':\n        df_raw = get_datasets(raw_data_dir, depth=0)\n    else:\n        raise ValueError('Please point to tiles or scenes path!')\n    # get processed\n\n    # get processing status for intermediate data\n    df_processed = get_datasets(processing_dir / 'tiles', depth=0, preprocessed=True)\n\n    # check if all files are available\n    df_processed = df_processed[df_processed.apply(lambda x: len(list(x['path'].glob('*'))) &gt;= 5, axis=1)]\n\n    # get all non preprocessed raw images\n    diff = df_raw[~df_raw['name'].isin(df_processed['name'])]\n    # TODO: issue here\n    if reduce_to_raw == True:\n        # TODO: check\n        df_merged = diff\n    else:\n        df_merged = pd.concat([df_processed, diff]).reset_index()\n\n    # make a dataframe with checks for processing status\n    products_list = [prod.name for prod in list((inference_dir / model).glob('*'))]\n    df_merged['inference_finished'] = df_merged.apply(lambda x: x['name'] in (products_list), axis=1)\n\n    return df_merged\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/postprocessing/#src.thaw_slump_segmentation.postprocessing.get_processing_status_ensemble","title":"<code>get_processing_status_ensemble(inference_dir, model_input_names=['RTS_v5_notcvis', 'RTS_v5_tcvis'], model_ensemble_name='RTS_v5_ensemble')</code>","text":"<p>Get processing status for a model ensemble and its individual models based on available data.</p> <p>This function examines the contents of specified directories within the 'inference_dir' to determine the processing status of a model ensemble and its constituent models. It constructs DataFrames indicating whether data is available for each model, and whether the processing has been completed for both the ensemble and individual models.</p> <p>Parameters:</p> Name Type Description Default <code>inference_dir</code> <code>Path - like</code> <p>Path to the directory containing inference data.</p> required <code>model_input_names</code> <code>list</code> <p>List of model input directory names. Default values are ['RTS_v5_notcvis', 'RTS_v5_tcvis'].</p> <code>['RTS_v5_notcvis', 'RTS_v5_tcvis']</code> <code>model_ensemble_name</code> <code>str</code> <p>Name of the model ensemble directory. Default value is 'RTS_v5_ensemble'.</p> <code>'RTS_v5_ensemble'</code> <p>Returns:</p> Type Description <p>pandas.DataFrame: A DataFrame containing the processing status for each model</p> <p>and the ensemble. Columns include 'name', 'data_available', and 'process'.</p> Example <p>inference_dir = Path('/path/to/inference_data') status_df = get_processing_status_ensemble(inference_dir)</p> Source code in <code>src/thaw_slump_segmentation/postprocessing.py</code> <pre><code>def get_processing_status_ensemble(\n    inference_dir, model_input_names=['RTS_v5_notcvis', 'RTS_v5_tcvis'], model_ensemble_name='RTS_v5_ensemble'\n):\n    \"\"\"\n    Get processing status for a model ensemble and its individual models based on available data.\n\n    This function examines the contents of specified directories within the 'inference_dir'\n    to determine the processing status of a model ensemble and its constituent models.\n    It constructs DataFrames indicating whether data is available for each model, and whether\n    the processing has been completed for both the ensemble and individual models.\n\n    Args:\n        inference_dir (Path-like): Path to the directory containing inference data.\n        model_input_names (list, optional): List of model input directory names.\n            Default values are ['RTS_v5_notcvis', 'RTS_v5_tcvis'].\n        model_ensemble_name (str, optional): Name of the model ensemble directory.\n            Default value is 'RTS_v5_ensemble'.\n\n    Returns:\n        pandas.DataFrame: A DataFrame containing the processing status for each model\n        and the ensemble. Columns include 'name', 'data_available', and 'process'.\n\n    Example:\n        &gt;&gt;&gt; inference_dir = Path('/path/to/inference_data')\n        &gt;&gt;&gt; status_df = get_processing_status_ensemble(inference_dir)\n\n    \"\"\"\n    dfs = []\n    for model in model_input_names[:]:\n        ds_names = [prod.name for prod in list((inference_dir / model).glob('*')) if prod.is_dir()]\n        has_proba = [(inference_dir / model / f / 'pred_probability.tif').exists() for f in ds_names]\n        df = pd.DataFrame(data=ds_names, columns=['name']).set_index('name')\n        df['model_name'] = model\n        df['has_proba'] = has_proba\n        dfs.append(df)\n\n    ds_names_ensemble = [prod.name for prod in list((inference_dir / model_ensemble_name).glob('*')) if prod.is_dir()]\n    df_ensemble = pd.DataFrame(data=ds_names_ensemble, columns=['name']).set_index('name')\n    df_ensemble['ensemble_name'] = model_ensemble_name\n    dfs.append(df_ensemble)\n\n    df_process = pd.concat(dfs, axis=1)\n\n    df_process['data_available'] = ~df_process['model_name'].isna().any(axis=1)\n    df_process['proba_available'] = df_process['has_proba'].all(axis=1)\n    df_process['process'] = df_process['data_available'] &amp; (\n        df_process['ensemble_name'].isna() &amp; df_process['proba_available']\n    )\n\n    return (\n        df_process[['process', 'data_available', 'proba_available']]\n        .reset_index(drop=False)\n        .rename(columns={'index': 'name'})\n    )\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/postprocessing/#src.thaw_slump_segmentation.postprocessing.listdirs","title":"<code>listdirs(rootdir)</code>","text":"Source code in <code>src/thaw_slump_segmentation/postprocessing.py</code> <pre><code>def listdirs(rootdir):\n    dirs = []\n    for path in Path(rootdir).iterdir():\n        if path.is_dir():\n            # print(path)\n            dirs.append(path)\n    return dirs\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/postprocessing/#src.thaw_slump_segmentation.postprocessing.listdirs2","title":"<code>listdirs2(rootdir, depth=0)</code>","text":"Source code in <code>src/thaw_slump_segmentation/postprocessing.py</code> <pre><code>def listdirs2(rootdir, depth=0):\n    dirs = []\n    for path in Path(rootdir).iterdir():\n        if path.is_dir():\n            if depth == 1:\n                for path2 in Path(path).iterdir():\n                    if path2.is_dir():\n                        dirs.append(path2)\n            else:\n                dirs.append(path)\n    return dirs\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/postprocessing/#src.thaw_slump_segmentation.postprocessing.load_and_parse_vector","title":"<code>load_and_parse_vector(file_path, filter_water=False)</code>","text":"<p>Load a GeoDataFrame from a given file path, reproject it to EPSG:4326, and parse image metadata from the file path to add as attributes.</p> <p>This function reads a GeoDataFrame from the specified file path, converts the GeoDataFrame's coordinate reference system to EPSG:4326, and parses the image ID from the parent directory name of the file path. It then extracts components from the image ID (take ID, tile ID, date, and satellite) and adds them as new columns in the GeoDataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str or Path</code> <p>Path to the vector file.</p> required <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>geopandas.GeoDataFrame: A GeoDataFrame with added attributes representing</p> <code>GeoDataFrame</code> <p>parsed image metadata.</p> Example <p>file_path = '/path/to/your/vector_file.geojson' parsed_gdf = load_and_parse_vector(file_path)</p> Source code in <code>src/thaw_slump_segmentation/postprocessing.py</code> <pre><code>def load_and_parse_vector(file_path: Union[str, Path], filter_water: bool = False) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Load a GeoDataFrame from a given file path, reproject it to EPSG:4326,\n    and parse image metadata from the file path to add as attributes.\n\n    This function reads a GeoDataFrame from the specified file path, converts\n    the GeoDataFrame's coordinate reference system to EPSG:4326, and parses\n    the image ID from the parent directory name of the file path. It then\n    extracts components from the image ID (take ID, tile ID, date, and satellite)\n    and adds them as new columns in the GeoDataFrame.\n\n    Args:\n        file_path (str or pathlib.Path): Path to the vector file.\n\n    Returns:\n        geopandas.GeoDataFrame: A GeoDataFrame with added attributes representing\n        parsed image metadata.\n\n    Example:\n        &gt;&gt;&gt; file_path = '/path/to/your/vector_file.geojson'\n        &gt;&gt;&gt; parsed_gdf = load_and_parse_vector(file_path)\n    \"\"\"\n    try:\n        gdf = gpd.read_file(file_path).to_crs('EPSG:4326')\n        if filter_water:\n            gdf = filter_remove_water(gdf)\n    except:\n        print(f'Error on File: {file_path}')\n        return None\n\n    image_id = file_path.parent.name\n    # parse and put into right format\n    PSProductType = get_PS_products_type(image_id)\n    if PSProductType == 'PSOrthoTile':\n        take_id, tile_id, date, satellite = image_id.split('_')\n    elif PSProductType == 'PSScene':\n        if len(image_id.split('_')) == 4:\n            date, take_id, _, satellite = image_id.split('_')\n        else:\n            date, take_id, satellite = image_id.split('_')\n\n        date = f'{date[:4]}-{date[4:6]}-{date[6:]}'\n        tile_id = None\n\n    gdf['image_id'] = image_id\n    gdf['take_id'] = take_id\n    gdf['tile_id'] = tile_id\n    gdf['date'] = date\n    gdf['year'] = pd.to_datetime(gdf['date']).dt.year\n    gdf['satellite'] = satellite\n\n    return gdf\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/postprocessing/#src.thaw_slump_segmentation.postprocessing.print_processing_stats","title":"<code>print_processing_stats(df_final)</code>","text":"<p>Print the processing statistics for a given DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df_final</code> <code>DataFrame</code> <p>A DataFrame containing the processing status of each dataset, with columns 'preprocessed' and 'inference_finished'.</p> required <p>Returns:</p> Type Description <p>None</p> Prints <ul> <li>Number of total images</li> <li>Number of preprocessed images</li> <li>Number of images for preprocessing</li> <li>Number of images for inference</li> <li>Number of finished images</li> </ul> Source code in <code>src/thaw_slump_segmentation/postprocessing.py</code> <pre><code>def print_processing_stats(df_final):\n    \"\"\"\n    Print the processing statistics for a given DataFrame.\n\n    Args:\n        df_final (pandas.DataFrame): A DataFrame containing the processing status of each dataset,\n            with columns 'preprocessed' and 'inference_finished'.\n\n    Returns:\n        None\n\n    Prints:\n        - Number of total images\n        - Number of preprocessed images\n        - Number of images for preprocessing\n        - Number of images for inference\n        - Number of finished images\n    \"\"\"\n    total_images = int(len(df_final))\n    preprocessed_images = int(df_final.preprocessed.sum())\n    preprocessing_images = int(total_images - preprocessed_images)\n    finished_images = int(df_final.inference_finished.sum())\n\n    print(f'Number of images: {total_images}')\n    print(f'Number of preprocessed images: {preprocessed_images}')\n    print(f'Number of images for preprocessing: {preprocessing_images}')\n    print(f'Number of images for inference: {preprocessed_images - finished_images}')\n    print(f'Number of finished images: {finished_images}')\n    return total_images, preprocessed_images, preprocessing_images, finished_images\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/postprocessing/#src.thaw_slump_segmentation.postprocessing.run_inference","title":"<code>run_inference(df, model, processing_dir, inference_dir, model_dir=Path('/isipd/projects/p_aicore_pf/initze/models/thaw_slumps'), gpu=0, patch_size=1024, margin_size=256, gdal_bin='/usr/bin', gdal_path='/usr/bin', **kwargs)</code>","text":"Source code in <code>src/thaw_slump_segmentation/postprocessing.py</code> <pre><code>def run_inference(\n    df,\n    model,\n    processing_dir,\n    inference_dir,\n    model_dir=Path('/isipd/projects/p_aicore_pf/initze/models/thaw_slumps'),\n    gpu=0,\n    patch_size=1024,\n    margin_size=256,\n    gdal_bin = \"/usr/bin\",\n    gdal_path = \"/usr/bin\",\n    **kwargs # consume legacy args (run)\n):\n    if len(df) == 0:\n        print('Empty dataframe')\n    else:\n        # CUDA_VISIBLE_DEVICES (see https://developer.nvidia.com/blog/cuda-pro-tip-control-gpu-visibility-cuda_visible_devices/)\n        # comma-separated list of device IDs \n        try:\n            gpu_csv = \",\".join(gpu)\n        except TypeError:\n            gpu_csv = gpu\n        os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(gpu_csv)\n        print(f\" run_inference with GPU {gpu_csv}\")\n        inference(\n            name=model,\n            data_dir=processing_dir, inference_dir=inference_dir, \n            patch_size=patch_size, margin_size=margin_size, \n            model_path=model_dir/model, \n            tile_to_predict=df.name.values,\n            gdal_bin=gdal_bin, gdal_path=gdal_path\n            )\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/postprocessing/#src.thaw_slump_segmentation.postprocessing.update_DEM","title":"<code>update_DEM(vrt_target_dir)</code>","text":"<p>Function to update elevation vrts</p> Source code in <code>src/thaw_slump_segmentation/postprocessing.py</code> <pre><code>def update_DEM(vrt_target_dir):\n    \"\"\"\n    Function to update elevation vrts\n    \"\"\"\n    os.system('./create_ArcticDEM.sh')\n    shutil.copy('elevation.vrt', vrt_target_dir)\n    shutil.copy('slope.vrt', vrt_target_dir)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/postprocessing/#src.thaw_slump_segmentation.postprocessing.update_DEM2","title":"<code>update_DEM2(dem_data_dir, vrt_target_dir)</code>","text":"<p>Update elevation and slope virtual raster tiles (VRTs) from a directory of GeoTIFF tiles.</p> <p>Parameters:</p> Name Type Description Default <code>dem_data_dir</code> <code>Path</code> <p>Path to the directory containing the elevation and slope GeoTIFF tiles.</p> required <code>vrt_target_dir</code> <code>Path</code> <p>Path to the directory where the updated VRTs will be saved.</p> required <p>This function creates two VRTs, one for elevation and one for slope, from the GeoTIFF tiles in the <code>dem_data_dir</code> directory. The VRTs are saved in the <code>vrt_target_dir</code> directory with the filenames 'elevation.vrt' and 'slope.vrt', respectively.</p> <p>The function uses the <code>gdalbuildvrt</code> command-line utility to create the VRTs, and it sets the nodata value for both the input tiles and the output VRTs to 0.</p> Source code in <code>src/thaw_slump_segmentation/postprocessing.py</code> <pre><code>def update_DEM2(dem_data_dir, vrt_target_dir):\n    \"\"\"\n    Update elevation and slope virtual raster tiles (VRTs) from a directory of GeoTIFF tiles.\n\n    Args:\n        dem_data_dir (Path): Path to the directory containing the elevation and slope GeoTIFF tiles.\n        vrt_target_dir (Path): Path to the directory where the updated VRTs will be saved.\n\n    This function creates two VRTs, one for elevation and one for slope, from the GeoTIFF tiles\n    in the `dem_data_dir` directory. The VRTs are saved in the `vrt_target_dir` directory with\n    the filenames 'elevation.vrt' and 'slope.vrt', respectively.\n\n    The function uses the `gdalbuildvrt` command-line utility to create the VRTs, and it sets\n    the nodata value for both the input tiles and the output VRTs to 0.\n    \"\"\"\n    # elevation\n    flist = list((dem_data_dir / 'tiles_rel_el').glob('*.tif'))\n    flist = [f'{f.absolute().as_posix()}\\n' for f in flist]\n    file_list_text = Path(vrt_target_dir) / 'flist_rel_el.txt'\n    with open(file_list_text, 'w') as src:\n        src.writelines(flist)\n    vrt_target = Path(vrt_target_dir) / 'elevation.vrt'\n    s = f'gdalbuildvrt -input_file_list {file_list_text} -srcnodata \"0\" -vrtnodata \"0\" {vrt_target}'\n    os.system(s)\n    os.remove(file_list_text)\n\n    # slope\n    flist = list((dem_data_dir / 'tiles_slope').glob('*.tif'))\n    flist = [f'{f.absolute().as_posix()}\\n' for f in flist]\n    file_list_text = Path(vrt_target_dir) / 'flist_slope.txt'\n    with open(file_list_text, 'w') as src:\n        src.writelines(flist)\n    vrt_target = Path(vrt_target_dir) / 'slope.vrt'\n    s = f'gdalbuildvrt -input_file_list {file_list_text} -srcnodata \"0\" -vrtnodata \"0\" {vrt_target}'\n    os.system(s)\n    os.remove(file_list_text)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/data_pre_processing/dem/","title":"Dem","text":""},{"location":"reference/src/thaw_slump_segmentation/data_pre_processing/dem/#src.thaw_slump_segmentation.data_pre_processing.dem.buildDemVrtMain","title":"<code>buildDemVrtMain = typer.Typer()</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/data_pre_processing/dem/#src.thaw_slump_segmentation.data_pre_processing.dem.l","title":"<code>l = logging.getLogger('thaw_slump_segmentation.preprocess.dem')</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/data_pre_processing/dem/#src.thaw_slump_segmentation.data_pre_processing.dem.subdirs","title":"<code>subdirs = {'elevation': 'tiles_rel_el', 'slope': 'tiles_slope'}</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/data_pre_processing/dem/#src.thaw_slump_segmentation.data_pre_processing.dem.buildDemVrt","title":"<code>buildDemVrt(dem_data_dir, vrt_target_dir)</code>","text":"<p>parses the subfolders 'tiles_rel_el' and 'tiles_slope' of <code>dem_data_dir</code> to create VRT (virtual raster tile) files (https://gdal.org/drivers/raster/vrt.html).</p> <p>A working installation of the python gdal bindings is required!</p> <p>Parameters:</p> Name Type Description Default <code>dem_data_dir</code> <code>Path</code> <p>The folder containing the source folders</p> required <code>vrt_target_dir</code> <code>Path</code> <p>The folder where to write the VRT files</p> required <p>Raises:</p> Type Description <code>EnvironmentError</code> <p>if gdal python bindings could not be imported</p> <code>IOError</code> <p>If target files are not writable</p> Source code in <code>src/thaw_slump_segmentation/data_pre_processing/dem.py</code> <pre><code>@buildDemVrtMain.command()\ndef buildDemVrt(\n    dem_data_dir:Annotated[ Path, typer.Option(\"--dem_data_dir\") ], \n    vrt_target_dir:Annotated[ Path, typer.Option(\"--vrt_target_dir\") ] \n    ):\n    \"\"\"parses the subfolders 'tiles_rel_el' and 'tiles_slope' of `dem_data_dir` to\n    create VRT (virtual raster tile) files (https://gdal.org/drivers/raster/vrt.html).\n\n    A working installation of the python gdal bindings is required!\n\n    Args:\n        dem_data_dir (Path): The folder containing the source folders\n        vrt_target_dir (Path): The folder where to write the VRT files\n\n    Raises:\n        EnvironmentError: if gdal python bindings could not be imported\n        IOError: If target files are not writable\n    \"\"\"\n\n    try:\n        from osgeo import gdal\n    except ModuleNotFoundError:\n        raise EnvironmentError(\"The python GDAL bindings where not found. Please install those which are appropriate for your platform.\")\n\n    l.info(f\"GDAL found: {gdal.__version__}\")\n\n    # decide on the exception behavior of GDAL to supress a warning if we dont\n    # don't know if this is necessary in all GDAL versions\n    try:\n        gdal.UseExceptions()\n    except AttributeError():\n        pass\n\n    # check first if BOTH files are writable\n    non_writable_files = []\n    for name in subdirs.keys():\n        output_file_path = vrt_target_dir / f\"{name}.vrt\"\n        if not (\n            (os.access(output_file_path, os.F_OK) and os.access(output_file_path, os.W_OK)) # f exists + writable\n            or os.access(output_file_path.parent, os.W_OK) # ...OR folder writable\n        ):\n            non_writable_files.append(output_file_path)\n    if len(non_writable_files) &gt; 0:\n        raise IOError(f\"cannot write to {', '.join([ f.name for f in non_writable_files ])}\")\n\n    for name,subdir in subdirs.items():\n        output_file_path = vrt_target_dir / f\"{name}.vrt\"\n        # check the file first if we can write to it\n\n        ds_path = dem_data_dir / subdir\n        filelist = [ str(f.absolute().resolve()) for f in ds_path.glob(\"*.tif\") ]\n        l.info(f\"{len(filelist)} files for {name}\\n --&gt; '{output_file_path}'\")\n        src_nodata = \"nan\" if name==\"slope\" else 0\n        gdal.BuildVRT( str(output_file_path.absolute()), filelist, options=gdal.BuildVRTOptions(srcNodata=src_nodata, VRTNodata=0) )\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/data_pre_processing/earthengine/","title":"Earthengine","text":""},{"location":"reference/src/thaw_slump_segmentation/data_pre_processing/earthengine/#src.thaw_slump_segmentation.data_pre_processing.earthengine.ee_geom_from_image_bounds","title":"<code>ee_geom_from_image_bounds(image_path, buffer=1000)</code>","text":"Source code in <code>src/thaw_slump_segmentation/data_pre_processing/earthengine.py</code> <pre><code>def ee_geom_from_image_bounds(image_path, buffer=1000):\n    with rio.open(image_path) as src:\n        epsg = 'EPSG:{}'.format(src.crs.to_epsg())\n        xmin, xmax, ymin, ymax = [src.bounds.left, src.bounds.right, src.bounds.bottom, src.bounds.top]\n    region_rect = [[xmin - buffer, ymin - buffer], [xmax + buffer, ymax + buffer]]\n\n    # make polygon\n    transformer = Transformer.from_crs(epsg, \"epsg:4326\")\n    region_transformed = [transformer.transform(*vertex)[::-1] for vertex in region_rect]\n    return ee.Geometry.Rectangle(coords=region_transformed), [xmin, xmax, ymin, ymax], epsg\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/data_pre_processing/earthengine/#src.thaw_slump_segmentation.data_pre_processing.earthengine.get_ArcticDEM_rel_el","title":"<code>get_ArcticDEM_rel_el(kernel_size=300, offset=30, factor=300)</code>","text":"Source code in <code>src/thaw_slump_segmentation/data_pre_processing/earthengine.py</code> <pre><code>def get_ArcticDEM_rel_el(kernel_size=300, offset=30, factor=300):\n    dem = ee.Image(\"UMN/PGC/ArcticDEM/V3/2m_mosaic\")\n    conv = dem.convolve(ee.Kernel.circle(kernel_size, 'meters'))\n    diff = (dem.subtract(conv).add(ee.Image.constant(offset)).multiply(ee.Image.constant(factor)).toInt16())\n    return diff\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/data_pre_processing/earthengine/#src.thaw_slump_segmentation.data_pre_processing.earthengine.get_ArcticDEM_slope","title":"<code>get_ArcticDEM_slope()</code>","text":"Source code in <code>src/thaw_slump_segmentation/data_pre_processing/earthengine.py</code> <pre><code>def get_ArcticDEM_slope():\n    dem = ee.Image(\"UMN/PGC/ArcticDEM/V3/2m_mosaic\")\n    slope = ee.Terrain.slope(dem)\n    return slope\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/data_pre_processing/earthengine/#src.thaw_slump_segmentation.data_pre_processing.earthengine.get_tcvis_from_gee","title":"<code>get_tcvis_from_gee(image_directory, ee_image, out_filename, buffer=200, resolution=3, remove_files=True)</code>","text":"Source code in <code>src/thaw_slump_segmentation/data_pre_processing/earthengine.py</code> <pre><code>def get_tcvis_from_gee(image_directory, ee_image, out_filename, buffer=200, resolution=3, remove_files=True):\n    image_directory = os.path.abspath(image_directory)\n    assert os.path.isdir(image_directory)\n    outfile_path = os.path.join(image_directory, out_filename)\n    if os.path.exists(outfile_path):\n        _logger.info(f'{out_filename} already exists. Skipping download!')\n        return 2\n    else:\n        _logger.info(\"Starting download Dataset from Google Earthengine\")\n    image_list = glob.glob(os.path.join(image_directory, r'*_SR.tif'))\n    impath = image_list[0]\n    basepath = os.path.basename(image_directory)\n    basename_tmpimage = basepath + '_ee_tmp'\n\n    geom_4326, coords, epsg = ee_geom_from_image_bounds(impath, buffer=buffer)\n\n    export_props = {'scale': 30,\n                    'name': '{basename}'.format(basename=basename_tmpimage),\n                    'region': geom_4326,\n                    'filePerBand': False,\n                    'crs': epsg}\n\n    url = ee_image.getDownloadURL(export_props)\n\n    zippath = os.path.join(image_directory, 'out.zip')\n    myfile = requests.get(url, allow_redirects=True)\n    # SUper slow large download\n    open(zippath, 'wb').write(myfile.content)\n\n    with zipfile.ZipFile(zippath, 'r') as zip_ref:\n        zip_ref.extractall(image_directory)\n\n    infile = os.path.join(image_directory, basename_tmpimage + '.tif')\n\n    xmin, xmax, ymin, ymax = coords\n    xres, yres = utils.resolution_from_image(impath)\n    s_warp = f'{gdal.warp} -t_srs {epsg} -tr {xres} {yres} \\\n               -srcnodata None -te {xmin} {ymin} {xmax} {ymax} {infile} {outfile_path}'\n    log_run(s_warp, _logger)\n\n    if remove_files:\n        os.remove(zippath)\n        os.remove(infile)\n\n    return 1\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/data_pre_processing/gdal/","title":"Gdal","text":""},{"location":"reference/src/thaw_slump_segmentation/data_pre_processing/gdal/#src.thaw_slump_segmentation.data_pre_processing.gdal.initialize","title":"<code>initialize(args=None, *, bin=None, path=None)</code>","text":"Source code in <code>src/thaw_slump_segmentation/data_pre_processing/gdal.py</code> <pre><code>def initialize(args=None, *, bin=None, path=None):\n    # If command line arguments are given, use those:\n    system_yml = Path('system.yml')\n\n    if args is not None:\n        # print('Manually set path')\n        _module.gdal_path = args.gdal_path\n        _module.gdal_bin = args.gdal_bin\n    elif bin is not None and path is not None:\n        # print('Manually set path')\n        _module.gdal_path = path\n        _module.gdal_bin = bin\n    # Otherwise, fall back to the ones from system.yml\n    elif system_yml.exists():\n        # print('yml file')\n        system_config = yaml.load(system_yml.open(), Loader=yaml.SafeLoader)\n        if 'gdal_path' in system_config:\n            _module.gdal_path = system_config['gdal_path']\n        if 'gdal_bin' in system_config:\n            _module.gdal_bin = system_config['gdal_bin']\n\n    else:\n        # print('Empty path')\n        _module.gdal_path = ''\n        _module.gdal_bin = ''\n\n    # print(_module.gdal_bin)\n    # print(_module.gdal_path)\n    _module.rasterize = os.path.join(_module.gdal_bin, 'gdal_rasterize')\n    _module.translate = os.path.join(_module.gdal_bin, 'gdal_translate')\n    _module.warp = os.path.join(_module.gdal_bin, 'gdalwarp')\n\n    _module.merge = os.path.join(_module.gdal_path, 'gdal_merge.py')\n    _module.retile = os.path.join(_module.gdal_path, 'gdal_retile.py')\n    _module.polygonize = os.path.join(_module.gdal_path, 'gdal_polygonize.py')\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/data_pre_processing/udm/","title":"Udm","text":""},{"location":"reference/src/thaw_slump_segmentation/data_pre_processing/udm/#src.thaw_slump_segmentation.data_pre_processing.udm.burn_mask","title":"<code>burn_mask(file_src, file_dst, file_udm, file_udm2=None, mask_value=0)</code>","text":"<p>Apply a data mask to a raster file and save the result to a new file.</p> <p>Parameters:</p> Name Type Description Default <code>file_src</code> <code>str</code> <p>Path to the input raster file.</p> required <code>file_dst</code> <code>str</code> <p>Path to the output raster file.</p> required <code>file_udm</code> <code>str</code> <p>Path to the UDM file (not used in this function).</p> required <code>file_udm2</code> <code>str</code> <p>Path to the UDM2 V2 file. If provided, the data mask will be derived from this file using the <code>get_mask_from_udm2_v2</code> function.</p> <code>None</code> <code>mask_value</code> <code>int</code> <p>Value to use for masked (invalid) pixels in the output file. Default is 0.</p> <code>0</code> <p>Returns:</p> Name Type Description <code>int</code> <p>Always returns 1 (for successful execution).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>file_udm2</code> is not provided.</p> Notes <ul> <li>The function reads the input raster file using rasterio and applies the data mask by multiplying the raster data with the mask.</li> <li>The masked raster data is then written to the output file with the same metadata as the input file.</li> <li>If <code>file_udm2</code> is not provided, a <code>ValueError</code> is raised.</li> <li>The <code>file_udm</code> parameter is not used in this function.</li> <li>The function uses the rasterio library for reading and writing raster files.</li> </ul> Source code in <code>src/thaw_slump_segmentation/data_pre_processing/udm.py</code> <pre><code>def burn_mask(file_src, file_dst, file_udm, file_udm2=None, mask_value=0):\n    \"\"\"\n    Apply a data mask to a raster file and save the result to a new file.\n\n    Args:\n        file_src (str): Path to the input raster file.\n        file_dst (str): Path to the output raster file.\n        file_udm (str): Path to the UDM file (not used in this function).\n        file_udm2 (str, optional): Path to the UDM2 V2 file. If provided, the data mask\n            will be derived from this file using the `get_mask_from_udm2_v2` function.\n        mask_value (int, optional): Value to use for masked (invalid) pixels in the output file.\n            Default is 0.\n\n    Returns:\n        int: Always returns 1 (for successful execution).\n\n    Raises:\n        ValueError: If `file_udm2` is not provided.\n\n    Notes:\n        - The function reads the input raster file using rasterio and applies the data mask\n        by multiplying the raster data with the mask.\n        - The masked raster data is then written to the output file with the same metadata\n        as the input file.\n        - If `file_udm2` is not provided, a `ValueError` is raised.\n        - The `file_udm` parameter is not used in this function.\n        - The function uses the rasterio library for reading and writing raster files.\n    \"\"\"\n    with rio.Env(): \n        if file_udm2:\n            mask_udm2 = get_mask_from_udm2_v2(file_udm2)\n        else:\n            raise ValueError\n\n        with rio.open(file_src) as ds_src:\n            # apply data mask (multiply)\n            data = ds_src.read() * mask_udm2\n            profile = ds_src.profile\n            with rio.open(file_dst, 'w', **profile) as ds_dst:\n                ds_dst.write(data)\n    return 1\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/data_pre_processing/udm/#src.thaw_slump_segmentation.data_pre_processing.udm.get_mask_from_udm","title":"<code>get_mask_from_udm(infile, nodata=1)</code>","text":"Source code in <code>src/thaw_slump_segmentation/data_pre_processing/udm.py</code> <pre><code>def get_mask_from_udm(infile, nodata=1):\n    with rio.open(infile) as src:\n        mask = src.read()[0] &gt;= nodata\n    return np.array(mask, dtype=np.uint8)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/data_pre_processing/udm/#src.thaw_slump_segmentation.data_pre_processing.udm.get_mask_from_udm2","title":"<code>get_mask_from_udm2(infile, bands=[1, 2, 4, 5], nodata=1)</code>","text":"<p>return masked array, where True = NoData</p> Source code in <code>src/thaw_slump_segmentation/data_pre_processing/udm.py</code> <pre><code>def get_mask_from_udm2(infile, bands=[1,2,4,5], nodata=1):\n    \"\"\"\n    return masked array, where True = NoData\n    \"\"\"\n    with rio.open(infile) as src:\n        a = src.read()\n        mask = a[bands].max(axis=0) == nodata\n    return mask\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/data_pre_processing/udm/#src.thaw_slump_segmentation.data_pre_processing.udm.get_mask_from_udm2_v2","title":"<code>get_mask_from_udm2_v2(infile)</code>","text":"<p>Create a data mask from a UDM2 V2 file.</p> <p>The data mask is a boolean array where True values represent good data, and False values represent no data or unusable data.</p> <p>Parameters:</p> Name Type Description Default <code>infile</code> <code>str</code> <p>Path to the input UDM2 V2 file.</p> required <p>Returns:</p> Type Description <p>numpy.ndarray: A boolean array representing the data mask.</p> Notes <ul> <li> <p>The function assumes that the input file is a UDM2 V2 file with   specific band meanings:</p> <ul> <li>Band 0: Clear data</li> <li>Bands 1, 2, 4, 5: Unusable data (if any of these bands has a value of 1)</li> <li>Band 7: No data</li> </ul> </li> <li> <p>The data mask is created by combining the following conditions:</p> <ul> <li>Clear data (band 0 == 1)</li> <li>Not no data (band 7 != 1)</li> <li>Not unusable data (maximum of bands 1, 2, 4, 5 != 1)</li> </ul> </li> <li> <p>The function uses the rasterio library to read the input file.</p> </li> </ul> Source code in <code>src/thaw_slump_segmentation/data_pre_processing/udm.py</code> <pre><code>def get_mask_from_udm2_v2(infile):\n    \"\"\"\n    Create a data mask from a UDM2 V2 file.\n\n    The data mask is a boolean array where True values represent good data,\n    and False values represent no data or unusable data.\n\n    Args:\n        infile (str): Path to the input UDM2 V2 file.\n\n    Returns:\n        numpy.ndarray: A boolean array representing the data mask.\n\n    Notes:\n        - The function assumes that the input file is a UDM2 V2 file with\n          specific band meanings:\n            - Band 0: Clear data\n            - Bands 1, 2, 4, 5: Unusable data (if any of these bands has a value of 1)\n            - Band 7: No data\n\n        - The data mask is created by combining the following conditions:\n            - Clear data (band 0 == 1)\n            - Not no data (band 7 != 1)\n            - Not unusable data (maximum of bands 1, 2, 4, 5 != 1)\n\n        - The function uses the rasterio library to read the input file.\n    \"\"\"\n    with rio.open(infile) as src:\n        a = src.read()\n        unusable_data = a[[1,2,4,5]].max(axis=0) == 1\n        clear_data = a[[0]] == 1\n        no_data = a[[7]] == 1\n        # final data mask: 0 = no or crappy data, 1 = good data\n        data_mask = np.logical_and(clear_data, ~np.logical_or(no_data, unusable_data))\n    return data_mask\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/data_pre_processing/utils/","title":"Utils","text":""},{"location":"reference/src/thaw_slump_segmentation/data_pre_processing/utils/#src.thaw_slump_segmentation.data_pre_processing.utils.aux_data_to_tiles","title":"<code>aux_data_to_tiles(image_directory, aux_data, outfile)</code>","text":"Source code in <code>src/thaw_slump_segmentation/data_pre_processing/utils.py</code> <pre><code>def aux_data_to_tiles(image_directory, aux_data, outfile):\n    # load template and get props\n    images = get_mask_images(image_directory, udm='udm.tif', udm2='udm2.tif', images=['_SR.tif'], fail_on_missing_udm=False)\n    image = images['images'][0]\n\n    # prepare gdalwarp call\n    xmin, xmax, ymin, ymax = geom_from_image_bounds(image)\n    crs = crs_from_image(image)\n    xres, yres = resolution_from_image(image)\n\n    # run gdalwarp call\n    outfile = f'{image_directory}/{outfile}'#os.path.join(image_directory,outfile)\n    s_run = f'{gdal.warp} -te {xmin} {ymin} {xmax} {ymax} -tr {xres} {yres} -r cubic -t_srs {crs} -co COMPRESS=DEFLATE {aux_data} {outfile}'\n    log_run(s_run, _logger)\n    return 1\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/data_pre_processing/utils/#src.thaw_slump_segmentation.data_pre_processing.utils.check_input_data","title":"<code>check_input_data(input_directory)</code>","text":"Source code in <code>src/thaw_slump_segmentation/data_pre_processing/utils.py</code> <pre><code>def check_input_data(input_directory):\n    directory_list = [f for f in input_directory.glob('*') if f.is_dir()]\n    return directory_list\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/data_pre_processing/utils/#src.thaw_slump_segmentation.data_pre_processing.utils.crs_from_image","title":"<code>crs_from_image(image_path)</code>","text":"Source code in <code>src/thaw_slump_segmentation/data_pre_processing/utils.py</code> <pre><code>def crs_from_image(image_path):\n    with rio.open(image_path) as src:\n        return 'EPSG:{}'.format(src.crs.to_epsg())\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/data_pre_processing/utils/#src.thaw_slump_segmentation.data_pre_processing.utils.geom_from_image_bounds","title":"<code>geom_from_image_bounds(image_path)</code>","text":"Source code in <code>src/thaw_slump_segmentation/data_pre_processing/utils.py</code> <pre><code>def geom_from_image_bounds(image_path):\n    with rio.open(image_path) as src:\n        epsg = 'EPSG:{}'.format(src.crs.to_epsg())\n        return [src.bounds.left, src.bounds.right, src.bounds.bottom, src.bounds.top]\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/data_pre_processing/utils/#src.thaw_slump_segmentation.data_pre_processing.utils.get_mask_images","title":"<code>get_mask_images(image_directory, udm='udm.tif', udm2='udm2.tif', images=['_SR.tif', 'tcvis.tif', '_mask.tif', 'relative_elevation.tif', 'slope.tif', 'ndvi.tif'], fail_on_missing_udm=True)</code>","text":"Source code in <code>src/thaw_slump_segmentation/data_pre_processing/utils.py</code> <pre><code>def get_mask_images(image_directory, udm='udm.tif', udm2='udm2.tif', images=['_SR.tif', 'tcvis.tif', '_mask.tif', 'relative_elevation.tif', 'slope.tif', 'ndvi.tif'], fail_on_missing_udm=True):\n    flist = glob.glob(os.path.join(image_directory, '*'))\n    image_files = []\n    for im in images:\n        image_files.extend([f for f in flist if im in f])\n\n    # check which udms are available, if not then set to None\n    try:\n        udm_file = [f for f in flist if udm in f][0]\n    except:\n        udm_file = None\n    try:\n        udm2_file = [f for f in flist if udm2 in f][0]\n    except:\n        udm2_file = None\n\n    # raise error if no udms available\n    if (udm_file == None) &amp; (udm2_file == None):\n        if fail_on_missing_udm:\n            raise ValueError(f'There are no udm or udm2 files for image {image_directory.name}!')\n        else:\n            _logger.warning(\"no UDM files found\")\n\n    remaining_files = [f for f in flist if f not in [udm_file, *image_files]]\n\n    return dict(udm=udm_file, udm2=udm2_file, images=image_files, others=remaining_files)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/data_pre_processing/utils/#src.thaw_slump_segmentation.data_pre_processing.utils.has_projection","title":"<code>has_projection(image_directory)</code>","text":"Source code in <code>src/thaw_slump_segmentation/data_pre_processing/utils.py</code> <pre><code>def has_projection(image_directory):\n    image_directory = os.path.abspath(image_directory)\n    assert os.path.isdir(image_directory)\n    image_list = glob.glob(os.path.join(image_directory, r'*_SR.tif'))\n    impath = image_list[0]\n\n    # TODO: crs not detected\n    with rio.open(impath) as src:\n        try:\n            src.crs.to_epsg()\n            return True\n        except AttributeError:\n            return False\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/data_pre_processing/utils/#src.thaw_slump_segmentation.data_pre_processing.utils.make_ndvi_file","title":"<code>make_ndvi_file(image_directory, nir_band=3, red_band=2)</code>","text":"Source code in <code>src/thaw_slump_segmentation/data_pre_processing/utils.py</code> <pre><code>def make_ndvi_file(image_directory, nir_band=3, red_band=2):\n    images = get_mask_images(image_directory, images=['_SR.tif'], fail_on_missing_udm=False)\n    file_src = images['images'][0]\n    file_dst = os.path.join(os.path.dirname(file_src), 'ndvi.tif')\n    with rio.Env():\n        with rio.open(images['images'][0]) as ds_src:\n            data = ds_src.read().astype(np.float32)\n            #mask = ds_src.read_masks()[0] != 0\n            mask = data[[red_band, nir_band]].sum(axis=0) != 0\n            ndvi = np.zeros_like(data[0])\n            upper = (data[nir_band][mask] - data[red_band][mask])\n            lower = (data[nir_band][mask] + data[red_band][mask])\n             # NDVI but transposed to 0..2 and scaled by 10000 to integer so data range 0..20000 \n            ndvi[mask] = np.around((np.divide(upper, lower) + 1) * 1e4)\n            ndvi = ndvi.astype(np.uint16)\n            profile = ds_src.profile\n            profile['count'] = 1\n\n            with rio.open(file_dst, 'w', **profile) as ds_dst:\n                ds_dst.write(ndvi.astype(rio.uint16), 1)\n    return 1\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/data_pre_processing/utils/#src.thaw_slump_segmentation.data_pre_processing.utils.mask_input_data","title":"<code>mask_input_data(image_directory, output_directory)</code>","text":"Source code in <code>src/thaw_slump_segmentation/data_pre_processing/utils.py</code> <pre><code>def mask_input_data(image_directory, output_directory):\n    try:\n        mask_image_paths = get_mask_images(image_directory)\n    except ValueError as e:\n        # probably no udm files found...\n        _logger.warn(\"found no datamask, skipping masking\")\n        return 1\n\n    for image in mask_image_paths['images']:\n        dir_out = os.path.join(output_directory, os.path.basename(image_directory))\n        image_out = os.path.join(dir_out, os.path.basename(image))\n        os.makedirs(dir_out, exist_ok=True)\n        burn_mask(image, image_out, file_udm=mask_image_paths['udm'], file_udm2=mask_image_paths['udm2'])\n    return 1\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/data_pre_processing/utils/#src.thaw_slump_segmentation.data_pre_processing.utils.move_files","title":"<code>move_files(image_directory, backup_dir)</code>","text":"Source code in <code>src/thaw_slump_segmentation/data_pre_processing/utils.py</code> <pre><code>def move_files(image_directory, backup_dir):\n    try:\n        shutil.move(image_directory, backup_dir)\n        return 1\n    except:\n        return 2\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/data_pre_processing/utils/#src.thaw_slump_segmentation.data_pre_processing.utils.pre_cleanup","title":"<code>pre_cleanup(input_directory)</code>","text":"Source code in <code>src/thaw_slump_segmentation/data_pre_processing/utils.py</code> <pre><code>def pre_cleanup(input_directory):\n    flist_dirty = glob.glob(os.path.join(input_directory, '*.aux.xml'))\n    if len(flist_dirty) &gt; 0:\n        for f in flist_dirty:\n            os.remove(f)\n            _logger.info(f'Removed File {f}')\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/data_pre_processing/utils/#src.thaw_slump_segmentation.data_pre_processing.utils.rename_clip_to_standard","title":"<code>rename_clip_to_standard(image_directory)</code>","text":"Source code in <code>src/thaw_slump_segmentation/data_pre_processing/utils.py</code> <pre><code>def rename_clip_to_standard(image_directory):\n    image_directory = os.path.abspath(image_directory)\n    imlist = glob.glob(os.path.join(image_directory, r'*_clip*'))\n    if len(imlist) &gt; 0:\n        for p in imlist:\n            p_out = os.path.join(image_directory, os.path.basename(p).replace('_clip', ''))\n            if not os.path.exists(p_out):\n                os.rename(p, p_out)\n        return 1\n    else:\n        _logger.info('No \"_clip\" naming found. Assume renaming not necessary')\n        return 2\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/data_pre_processing/utils/#src.thaw_slump_segmentation.data_pre_processing.utils.resolution_from_image","title":"<code>resolution_from_image(image_path)</code>","text":"Source code in <code>src/thaw_slump_segmentation/data_pre_processing/utils.py</code> <pre><code>def resolution_from_image(image_path):\n    with rio.open(image_path) as src:\n        return src.res\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/data_pre_processing/utils/#src.thaw_slump_segmentation.data_pre_processing.utils.vector_to_raster_mask","title":"<code>vector_to_raster_mask(image_directory, delete_intermediate_files=True)</code>","text":"Source code in <code>src/thaw_slump_segmentation/data_pre_processing/utils.py</code> <pre><code>def vector_to_raster_mask(image_directory, delete_intermediate_files=True):\n    vectorfile = glob.glob(os.path.join(image_directory, '*.shp'))[0]\n    layername = os.path.basename(vectorfile)[0:-4] # shapefile layer name is filename without suffix\n    rasterfile = glob.glob(os.path.join(image_directory, r'*_SR.tif'))[0]\n    maskfile = os.path.join(image_directory, 'mask.tif')\n\n    # use the image base name prefix as basename \n    # should be the same as the folder for planet and sentinel inference, but is different for the sentinel training data\n    # because the folders are the id of the planet tile the sentinel data is associated with\n    basename = os.path.basename(rasterfile)[0:-7] # without _SR.tif\n\n    maskfile2 = os.path.join(image_directory, f'{basename}_mask.tif')\n\n    try:\n        #s_merge = f'python {gdal.merge} -createonly -init 0 -o {maskfile} -ot Byte -co COMPRESS=DEFLATE {rasterfile}'\n        s_merge = f'{gdal.merge} -createonly -init 0 -o {maskfile} -ot Byte -co COMPRESS=DEFLATE {rasterfile}'\n        log_run(s_merge, _logger)\n        # Add empty band to mask\n        s_translate = f'{gdal.translate} -of GTiff -ot Byte -co COMPRESS=DEFLATE -b 1 {maskfile} {maskfile2}'\n        log_run(s_translate, _logger)\n        # Burn digitized polygons into mask\n        s_rasterize = f'{gdal.rasterize} -l {layername} -a label {vectorfile} {maskfile2}'\n        log_run(s_rasterize, _logger)\n    except:\n        return 2\n    if delete_intermediate_files:\n        os.remove(maskfile)\n    return 1\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_prc/","title":"Binary instance prc","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_prc/#src.thaw_slump_segmentation.metrics.binary_instance_prc.MatchingMetric","title":"<code>MatchingMetric = Literal['iou', 'boundary']</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_prc/#src.thaw_slump_segmentation.metrics.binary_instance_prc.BinaryInstanceAveragePrecision","title":"<code>BinaryInstanceAveragePrecision</code>","text":"<p>               Bases: <code>BinaryInstancePrecisionRecallCurve</code></p> Source code in <code>src/thaw_slump_segmentation/metrics/binary_instance_prc.py</code> <pre><code>class BinaryInstanceAveragePrecision(BinaryInstancePrecisionRecallCurve):\n    is_differentiable: bool = False\n    higher_is_better: bool = True\n    full_state_update: bool = False\n    plot_lower_bound: float = 0.0\n    plot_upper_bound: float = 1.0\n\n    def compute(self) -&gt; Tensor:  # type: ignore[override]\n        \"\"\"Compute metric.\"\"\"\n        return _binary_average_precision_compute(self.confmat, self.thresholds)\n\n    def plot(  # type: ignore[override]\n        self, val: Optional[Tensor | Sequence[Tensor]] = None, ax: Optional[_AX_TYPE] = None  # type: ignore\n    ) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n\n        return self._plot(val, ax)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_prc/#src.thaw_slump_segmentation.metrics.binary_instance_prc.BinaryInstanceAveragePrecision.full_state_update","title":"<code>full_state_update: bool = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_prc/#src.thaw_slump_segmentation.metrics.binary_instance_prc.BinaryInstanceAveragePrecision.higher_is_better","title":"<code>higher_is_better: bool = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_prc/#src.thaw_slump_segmentation.metrics.binary_instance_prc.BinaryInstanceAveragePrecision.is_differentiable","title":"<code>is_differentiable: bool = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_prc/#src.thaw_slump_segmentation.metrics.binary_instance_prc.BinaryInstanceAveragePrecision.plot_lower_bound","title":"<code>plot_lower_bound: float = 0.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_prc/#src.thaw_slump_segmentation.metrics.binary_instance_prc.BinaryInstanceAveragePrecision.plot_upper_bound","title":"<code>plot_upper_bound: float = 1.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_prc/#src.thaw_slump_segmentation.metrics.binary_instance_prc.BinaryInstanceAveragePrecision.compute","title":"<code>compute()</code>","text":"<p>Compute metric.</p> Source code in <code>src/thaw_slump_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def compute(self) -&gt; Tensor:  # type: ignore[override]\n    \"\"\"Compute metric.\"\"\"\n    return _binary_average_precision_compute(self.confmat, self.thresholds)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_prc/#src.thaw_slump_segmentation.metrics.binary_instance_prc.BinaryInstanceAveragePrecision.plot","title":"<code>plot(val=None, ax=None)</code>","text":"Source code in <code>src/thaw_slump_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def plot(  # type: ignore[override]\n    self, val: Optional[Tensor | Sequence[Tensor]] = None, ax: Optional[_AX_TYPE] = None  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n\n    return self._plot(val, ax)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_prc/#src.thaw_slump_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve","title":"<code>BinaryInstancePrecisionRecallCurve</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Compute the precision-recall curve for binary instance segmentation.</p> <p>This metric works similar to <code>torchmetrics.classification.PrecisionRecallCurve</code>, with two key differences: 1. It calculates the tp, fp, fn values for each instance (blob) in the batch, and then aggregates them.     Instead of calculating the values for each pixel. 2. The \"thresholds\" argument is required.     Calculating the thresholds at the compute stage would cost to much memory for this usecase.</p> Source code in <code>src/thaw_slump_segmentation/metrics/binary_instance_prc.py</code> <pre><code>class BinaryInstancePrecisionRecallCurve(Metric):\n    \"\"\"Compute the precision-recall curve for binary instance segmentation.\n\n    This metric works similar to `torchmetrics.classification.PrecisionRecallCurve`, with two key differences:\n    1. It calculates the tp, fp, fn values for each instance (blob) in the batch, and then aggregates them.\n        Instead of calculating the values for each pixel.\n    2. The \"thresholds\" argument is required.\n        Calculating the thresholds at the compute stage would cost to much memory for this usecase.\n\n    \"\"\"\n\n    is_differentiable: bool = False\n    higher_is_better: Optional[bool] = None\n    full_state_update: bool = False\n\n    preds: List[Tensor]\n    target: List[Tensor]\n    confmat: Tensor\n    thesholds: Tensor\n\n    def __init__(\n        self,\n        thresholds: int | List[float] | Tensor = None,\n        matching_threshold: float = 0.5,\n        matching_metric: MatchingMetric = 'iou',\n        boundary_dilation: float | int = 0.02,\n        ignore_index: Optional[int] = None,\n        validate_args: bool = True,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(**kwargs)\n        if validate_args:\n            _binary_precision_recall_curve_arg_validation(thresholds, ignore_index)\n            _boundary_arg_validation(matching_threshold, matching_metric, boundary_dilation)\n            if thresholds is None:\n                raise ValueError('Argument `thresholds` must be provided for this metric.')\n\n        self.matching_threshold = matching_threshold\n        self.matching_metric = matching_metric\n        self.boundary_dilation = boundary_dilation\n        self.ignore_index = ignore_index\n        self.validate_args = validate_args\n\n        if ignore_index is not None:\n            # TODO: implement ignore_index\n            raise ValueError('Argument `ignore_index` is not supported for this metric yet.')\n\n        thresholds = _adjust_threshold_arg(thresholds)\n        self.register_buffer('thresholds', thresholds, persistent=False)\n        self.add_state('confmat', default=torch.zeros(len(thresholds), 2, 2, dtype=torch.long), dist_reduce_fx='sum')\n\n    def update(self, preds: Tensor, target: Tensor) -&gt; None:\n        \"\"\"Update metric states.\"\"\"\n        if self.validate_args:\n            _binary_precision_recall_curve_tensor_validation(preds, target, self.ignore_index)\n            if not preds.dim() == 3:\n                raise ValueError(f'Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.')\n\n        # Format\n        if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n            preds = preds.sigmoid()\n\n        instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n\n        len_t = len(self.thresholds)\n        confmat = self.thresholds.new_zeros((len_t, 2, 2), dtype=torch.int64)\n        for i in range(len_t):\n            preds_i = preds &gt;= self.thresholds[i]\n            instance_list_preds_i = mask_to_instances(preds_i.to(torch.uint8), self.validate_args)\n            for target_i, preds_i in zip(instance_list_target, instance_list_preds_i):\n                tp, fp, fn = match_instances(\n                    target_i,\n                    preds_i,\n                    match_threshold=self.matching_threshold,\n                    match_metric=self.matching_metric,\n                    boundary_dilation=self.boundary_dilation,\n                    validate_args=self.validate_args,\n                )\n                confmat[i, 1, 1] += tp\n                confmat[i, 0, 1] += fp\n                confmat[i, 1, 0] += fn\n        self.confmat += confmat\n\n    def compute(self) -&gt; Tuple[Tensor, Tensor, Tensor]:\n        \"\"\"Compute metric.\"\"\"\n        return _binary_precision_recall_curve_compute(self.confmat, self.thresholds)\n\n    def plot(\n        self,\n        curve: Optional[Tuple[Tensor, Tensor, Tensor]] = None,\n        score: Optional[Tensor | bool] = None,\n        ax: Optional[_AX_TYPE] = None,  # type: ignore\n    ) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n        \"\"\"Plot a single curve from the metric.\n\n        Args:\n            curve: the output of either `metric.compute` or `metric.forward`. If no value is provided, will\n                automatically call `metric.compute` and plot that result.\n            score: Provide a area-under-the-curve score to be displayed on the plot. If `True` and no curve is provided,\n                will automatically compute the score. The score is computed by using the trapezoidal rule to compute the\n                area under the curve.\n            ax: An matplotlib axis object. If provided will add plot to that axis\n\n        Returns:\n            Figure and Axes object\n\n        Raises:\n            ModuleNotFoundError:\n                If `matplotlib` is not installed\n\n        .. plot::\n            :scale: 75\n\n            &gt;&gt;&gt; from torch import rand, randint\n            &gt;&gt;&gt; from torchmetrics.classification import BinaryPrecisionRecallCurve\n            &gt;&gt;&gt; preds = rand(20)\n            &gt;&gt;&gt; target = randint(2, (20,))\n            &gt;&gt;&gt; metric = BinaryPrecisionRecallCurve()\n            &gt;&gt;&gt; metric.update(preds, target)\n            &gt;&gt;&gt; fig_, ax_ = metric.plot(score=True)\n\n        \"\"\"\n        curve_computed = curve or self.compute()\n        # switch order as the standard way is recall along x-axis and precision along y-axis\n        curve_computed = (curve_computed[1], curve_computed[0], curve_computed[2])\n\n        score = (\n            _auc_compute_without_check(curve_computed[0], curve_computed[1], direction=-1.0)\n            if not curve and score is True\n            else None\n        )\n        return plot_curve(\n            curve_computed, score=score, ax=ax, label_names=('Recall', 'Precision'), name=self.__class__.__name__\n        )\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_prc/#src.thaw_slump_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve.boundary_dilation","title":"<code>boundary_dilation = boundary_dilation</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_prc/#src.thaw_slump_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve.confmat","title":"<code>confmat: Tensor</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_prc/#src.thaw_slump_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve.full_state_update","title":"<code>full_state_update: bool = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_prc/#src.thaw_slump_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve.higher_is_better","title":"<code>higher_is_better: Optional[bool] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_prc/#src.thaw_slump_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve.ignore_index","title":"<code>ignore_index = ignore_index</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_prc/#src.thaw_slump_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve.is_differentiable","title":"<code>is_differentiable: bool = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_prc/#src.thaw_slump_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve.matching_metric","title":"<code>matching_metric = matching_metric</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_prc/#src.thaw_slump_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve.matching_threshold","title":"<code>matching_threshold = matching_threshold</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_prc/#src.thaw_slump_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve.preds","title":"<code>preds: List[Tensor]</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_prc/#src.thaw_slump_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve.target","title":"<code>target: List[Tensor]</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_prc/#src.thaw_slump_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve.thesholds","title":"<code>thesholds: Tensor</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_prc/#src.thaw_slump_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve.validate_args","title":"<code>validate_args = validate_args</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_prc/#src.thaw_slump_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve.__init__","title":"<code>__init__(thresholds=None, matching_threshold=0.5, matching_metric='iou', boundary_dilation=0.02, ignore_index=None, validate_args=True, **kwargs)</code>","text":"Source code in <code>src/thaw_slump_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def __init__(\n    self,\n    thresholds: int | List[float] | Tensor = None,\n    matching_threshold: float = 0.5,\n    matching_metric: MatchingMetric = 'iou',\n    boundary_dilation: float | int = 0.02,\n    ignore_index: Optional[int] = None,\n    validate_args: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    super().__init__(**kwargs)\n    if validate_args:\n        _binary_precision_recall_curve_arg_validation(thresholds, ignore_index)\n        _boundary_arg_validation(matching_threshold, matching_metric, boundary_dilation)\n        if thresholds is None:\n            raise ValueError('Argument `thresholds` must be provided for this metric.')\n\n    self.matching_threshold = matching_threshold\n    self.matching_metric = matching_metric\n    self.boundary_dilation = boundary_dilation\n    self.ignore_index = ignore_index\n    self.validate_args = validate_args\n\n    if ignore_index is not None:\n        # TODO: implement ignore_index\n        raise ValueError('Argument `ignore_index` is not supported for this metric yet.')\n\n    thresholds = _adjust_threshold_arg(thresholds)\n    self.register_buffer('thresholds', thresholds, persistent=False)\n    self.add_state('confmat', default=torch.zeros(len(thresholds), 2, 2, dtype=torch.long), dist_reduce_fx='sum')\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_prc/#src.thaw_slump_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve.compute","title":"<code>compute()</code>","text":"<p>Compute metric.</p> Source code in <code>src/thaw_slump_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def compute(self) -&gt; Tuple[Tensor, Tensor, Tensor]:\n    \"\"\"Compute metric.\"\"\"\n    return _binary_precision_recall_curve_compute(self.confmat, self.thresholds)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_prc/#src.thaw_slump_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve.plot","title":"<code>plot(curve=None, score=None, ax=None)</code>","text":"<p>Plot a single curve from the metric.</p> <p>Parameters:</p> Name Type Description Default <code>curve</code> <code>Optional[Tuple[Tensor, Tensor, Tensor]]</code> <p>the output of either <code>metric.compute</code> or <code>metric.forward</code>. If no value is provided, will automatically call <code>metric.compute</code> and plot that result.</p> <code>None</code> <code>score</code> <code>Optional[Tensor | bool]</code> <p>Provide a area-under-the-curve score to be displayed on the plot. If <code>True</code> and no curve is provided, will automatically compute the score. The score is computed by using the trapezoidal rule to compute the area under the curve.</p> <code>None</code> <code>ax</code> <code>Optional[_AX_TYPE]</code> <p>An matplotlib axis object. If provided will add plot to that axis</p> <code>None</code> <p>Returns:</p> Type Description <code>_PLOT_OUT_TYPE</code> <p>Figure and Axes object</p> <p>Raises:</p> Type Description <code>ModuleNotFoundError</code> <p>If <code>matplotlib</code> is not installed</p> <p>.. plot::     :scale: 75</p> <pre><code>&gt;&gt;&gt; from torch import rand, randint\n&gt;&gt;&gt; from torchmetrics.classification import BinaryPrecisionRecallCurve\n&gt;&gt;&gt; preds = rand(20)\n&gt;&gt;&gt; target = randint(2, (20,))\n&gt;&gt;&gt; metric = BinaryPrecisionRecallCurve()\n&gt;&gt;&gt; metric.update(preds, target)\n&gt;&gt;&gt; fig_, ax_ = metric.plot(score=True)\n</code></pre> Source code in <code>src/thaw_slump_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def plot(\n    self,\n    curve: Optional[Tuple[Tensor, Tensor, Tensor]] = None,\n    score: Optional[Tensor | bool] = None,\n    ax: Optional[_AX_TYPE] = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    \"\"\"Plot a single curve from the metric.\n\n    Args:\n        curve: the output of either `metric.compute` or `metric.forward`. If no value is provided, will\n            automatically call `metric.compute` and plot that result.\n        score: Provide a area-under-the-curve score to be displayed on the plot. If `True` and no curve is provided,\n            will automatically compute the score. The score is computed by using the trapezoidal rule to compute the\n            area under the curve.\n        ax: An matplotlib axis object. If provided will add plot to that axis\n\n    Returns:\n        Figure and Axes object\n\n    Raises:\n        ModuleNotFoundError:\n            If `matplotlib` is not installed\n\n    .. plot::\n        :scale: 75\n\n        &gt;&gt;&gt; from torch import rand, randint\n        &gt;&gt;&gt; from torchmetrics.classification import BinaryPrecisionRecallCurve\n        &gt;&gt;&gt; preds = rand(20)\n        &gt;&gt;&gt; target = randint(2, (20,))\n        &gt;&gt;&gt; metric = BinaryPrecisionRecallCurve()\n        &gt;&gt;&gt; metric.update(preds, target)\n        &gt;&gt;&gt; fig_, ax_ = metric.plot(score=True)\n\n    \"\"\"\n    curve_computed = curve or self.compute()\n    # switch order as the standard way is recall along x-axis and precision along y-axis\n    curve_computed = (curve_computed[1], curve_computed[0], curve_computed[2])\n\n    score = (\n        _auc_compute_without_check(curve_computed[0], curve_computed[1], direction=-1.0)\n        if not curve and score is True\n        else None\n    )\n    return plot_curve(\n        curve_computed, score=score, ax=ax, label_names=('Recall', 'Precision'), name=self.__class__.__name__\n    )\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_prc/#src.thaw_slump_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve.update","title":"<code>update(preds, target)</code>","text":"<p>Update metric states.</p> Source code in <code>src/thaw_slump_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update metric states.\"\"\"\n    if self.validate_args:\n        _binary_precision_recall_curve_tensor_validation(preds, target, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f'Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.')\n\n    # Format\n    if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n        preds = preds.sigmoid()\n\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n\n    len_t = len(self.thresholds)\n    confmat = self.thresholds.new_zeros((len_t, 2, 2), dtype=torch.int64)\n    for i in range(len_t):\n        preds_i = preds &gt;= self.thresholds[i]\n        instance_list_preds_i = mask_to_instances(preds_i.to(torch.uint8), self.validate_args)\n        for target_i, preds_i in zip(instance_list_target, instance_list_preds_i):\n            tp, fp, fn = match_instances(\n                target_i,\n                preds_i,\n                match_threshold=self.matching_threshold,\n                match_metric=self.matching_metric,\n                boundary_dilation=self.boundary_dilation,\n                validate_args=self.validate_args,\n            )\n            confmat[i, 1, 1] += tp\n            confmat[i, 0, 1] += fp\n            confmat[i, 1, 0] += fn\n    self.confmat += confmat\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_stat_scores/","title":"Binary instance stat scores","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_stat_scores/#src.thaw_slump_segmentation.metrics.binary_instance_stat_scores.MatchingMetric","title":"<code>MatchingMetric = Literal['iou', 'boundary']</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_stat_scores/#src.thaw_slump_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceAccuracy","title":"<code>BinaryInstanceAccuracy</code>","text":"<p>               Bases: <code>BinaryInstanceStatScores</code></p> Source code in <code>src/thaw_slump_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>class BinaryInstanceAccuracy(BinaryInstanceStatScores):\n    is_differentiable: bool = False\n    higher_is_better: Optional[bool] = True\n    full_state_update: bool = False\n    plot_lower_bound: float = 0.0\n    plot_upper_bound: float = 1.0\n\n    def compute(self) -&gt; Tensor:\n        \"\"\"Compute metric.\"\"\"\n        tp, fp, tn, fn = self._final_state()\n        return _accuracy_reduce(\n            tp,\n            fp,\n            tn,\n            fn,\n            average='binary',\n            multidim_average=self.multidim_average,\n        )\n\n    def plot(\n        self,\n        val: Optional[Tensor | Sequence[Tensor]] = None,\n        ax: Optional[_AX_TYPE] = None,  # type: ignore\n    ) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n        return self._plot(val, ax)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_stat_scores/#src.thaw_slump_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceAccuracy.full_state_update","title":"<code>full_state_update: bool = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_stat_scores/#src.thaw_slump_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceAccuracy.higher_is_better","title":"<code>higher_is_better: Optional[bool] = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_stat_scores/#src.thaw_slump_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceAccuracy.is_differentiable","title":"<code>is_differentiable: bool = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_stat_scores/#src.thaw_slump_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceAccuracy.plot_lower_bound","title":"<code>plot_lower_bound: float = 0.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_stat_scores/#src.thaw_slump_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceAccuracy.plot_upper_bound","title":"<code>plot_upper_bound: float = 1.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_stat_scores/#src.thaw_slump_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceAccuracy.compute","title":"<code>compute()</code>","text":"<p>Compute metric.</p> Source code in <code>src/thaw_slump_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def compute(self) -&gt; Tensor:\n    \"\"\"Compute metric.\"\"\"\n    tp, fp, tn, fn = self._final_state()\n    return _accuracy_reduce(\n        tp,\n        fp,\n        tn,\n        fn,\n        average='binary',\n        multidim_average=self.multidim_average,\n    )\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_stat_scores/#src.thaw_slump_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceAccuracy.plot","title":"<code>plot(val=None, ax=None)</code>","text":"Source code in <code>src/thaw_slump_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def plot(\n    self,\n    val: Optional[Tensor | Sequence[Tensor]] = None,\n    ax: Optional[_AX_TYPE] = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    return self._plot(val, ax)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_stat_scores/#src.thaw_slump_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceConfusionMatrix","title":"<code>BinaryInstanceConfusionMatrix</code>","text":"<p>               Bases: <code>BinaryInstanceStatScores</code></p> Source code in <code>src/thaw_slump_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>class BinaryInstanceConfusionMatrix(BinaryInstanceStatScores):\n    is_differentiable: bool = False\n    higher_is_better: Optional[bool] = None\n    full_state_update: bool = False\n\n    def __init__(\n        self,\n        normalize: Optional[bool] = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(**kwargs)\n        if normalize is not None and not isinstance(normalize, bool):\n            raise ValueError(f\"Argument `normalize` needs to be of bool type but got {type(normalize)}\")\n        self.normalize = normalize\n\n    def compute(self) -&gt; Tensor:\n        \"\"\"Compute the final statistics.\"\"\"\n        tp, fp, tn, fn = self._final_state()\n        # tn is always 0\n        if self.normalize:\n            all = tp + fp + fn\n            return torch.tensor([[0, fp / all], [fn / all, tp / all]], device=tp.device)\n        else:\n            return torch.tensor([[tn, fp], [fn, tp]], device=tp.device)\n\n    def plot(\n        self,\n        val: Optional[Tensor] = None,\n        ax: Optional[_AX_TYPE] = None,  # type: ignore\n        add_text: bool = True,\n        labels: Optional[List[str]] = None,  # type: ignore\n        cmap: Optional[_CMAP_TYPE] = None,  # type: ignore\n    ) -&gt; _PLOT_OUT_TYPE: # type: ignore\n        val = val or self.compute()\n        if not isinstance(val, Tensor):\n            raise TypeError(f\"Expected val to be a single tensor but got {val}\")\n        fig, ax = plot_confusion_matrix(val, ax=ax, add_text=add_text, labels=labels, cmap=cmap)\n        return fig, ax\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_stat_scores/#src.thaw_slump_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceConfusionMatrix.full_state_update","title":"<code>full_state_update: bool = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_stat_scores/#src.thaw_slump_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceConfusionMatrix.higher_is_better","title":"<code>higher_is_better: Optional[bool] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_stat_scores/#src.thaw_slump_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceConfusionMatrix.is_differentiable","title":"<code>is_differentiable: bool = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_stat_scores/#src.thaw_slump_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceConfusionMatrix.normalize","title":"<code>normalize = normalize</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_stat_scores/#src.thaw_slump_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceConfusionMatrix.__init__","title":"<code>__init__(normalize=None, **kwargs)</code>","text":"Source code in <code>src/thaw_slump_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def __init__(\n    self,\n    normalize: Optional[bool] = None,\n    **kwargs: Any,\n) -&gt; None:\n    super().__init__(**kwargs)\n    if normalize is not None and not isinstance(normalize, bool):\n        raise ValueError(f\"Argument `normalize` needs to be of bool type but got {type(normalize)}\")\n    self.normalize = normalize\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_stat_scores/#src.thaw_slump_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceConfusionMatrix.compute","title":"<code>compute()</code>","text":"<p>Compute the final statistics.</p> Source code in <code>src/thaw_slump_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def compute(self) -&gt; Tensor:\n    \"\"\"Compute the final statistics.\"\"\"\n    tp, fp, tn, fn = self._final_state()\n    # tn is always 0\n    if self.normalize:\n        all = tp + fp + fn\n        return torch.tensor([[0, fp / all], [fn / all, tp / all]], device=tp.device)\n    else:\n        return torch.tensor([[tn, fp], [fn, tp]], device=tp.device)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_stat_scores/#src.thaw_slump_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceConfusionMatrix.plot","title":"<code>plot(val=None, ax=None, add_text=True, labels=None, cmap=None)</code>","text":"Source code in <code>src/thaw_slump_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def plot(\n    self,\n    val: Optional[Tensor] = None,\n    ax: Optional[_AX_TYPE] = None,  # type: ignore\n    add_text: bool = True,\n    labels: Optional[List[str]] = None,  # type: ignore\n    cmap: Optional[_CMAP_TYPE] = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE: # type: ignore\n    val = val or self.compute()\n    if not isinstance(val, Tensor):\n        raise TypeError(f\"Expected val to be a single tensor but got {val}\")\n    fig, ax = plot_confusion_matrix(val, ax=ax, add_text=add_text, labels=labels, cmap=cmap)\n    return fig, ax\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_stat_scores/#src.thaw_slump_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceF1Score","title":"<code>BinaryInstanceF1Score</code>","text":"<p>               Bases: <code>BinaryInstanceFBetaScore</code></p> Source code in <code>src/thaw_slump_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>class BinaryInstanceF1Score(BinaryInstanceFBetaScore):\n    def __init__(\n        self,\n        threshold: float = 0.5,\n        multidim_average: Literal['global', 'samplewise'] = 'global',\n        ignore_index: Optional[int] = None,\n        validate_args: bool = True,\n        zero_division: float = 0,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(\n            beta=1.0,\n            threshold=threshold,\n            multidim_average=multidim_average,\n            ignore_index=ignore_index,\n            validate_args=validate_args,\n            zero_division=zero_division,\n            **kwargs,\n        )\n\n    def plot(\n        self,\n        val: Optional[Tensor | Sequence[Tensor]] = None,\n        ax: Optional[_AX_TYPE] = None,  # type: ignore\n    ) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n        return self._plot(val, ax)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_stat_scores/#src.thaw_slump_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceF1Score.__init__","title":"<code>__init__(threshold=0.5, multidim_average='global', ignore_index=None, validate_args=True, zero_division=0, **kwargs)</code>","text":"Source code in <code>src/thaw_slump_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def __init__(\n    self,\n    threshold: float = 0.5,\n    multidim_average: Literal['global', 'samplewise'] = 'global',\n    ignore_index: Optional[int] = None,\n    validate_args: bool = True,\n    zero_division: float = 0,\n    **kwargs: Any,\n) -&gt; None:\n    super().__init__(\n        beta=1.0,\n        threshold=threshold,\n        multidim_average=multidim_average,\n        ignore_index=ignore_index,\n        validate_args=validate_args,\n        zero_division=zero_division,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_stat_scores/#src.thaw_slump_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceF1Score.plot","title":"<code>plot(val=None, ax=None)</code>","text":"Source code in <code>src/thaw_slump_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def plot(\n    self,\n    val: Optional[Tensor | Sequence[Tensor]] = None,\n    ax: Optional[_AX_TYPE] = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    return self._plot(val, ax)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_stat_scores/#src.thaw_slump_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore","title":"<code>BinaryInstanceFBetaScore</code>","text":"<p>               Bases: <code>BinaryInstanceStatScores</code></p> Source code in <code>src/thaw_slump_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>class BinaryInstanceFBetaScore(BinaryInstanceStatScores):\n    is_differentiable: bool = False\n    higher_is_better: Optional[bool] = True\n    full_state_update: bool = False\n    plot_lower_bound: float = 0.0\n    plot_upper_bound: float = 1.0\n\n    def __init__(\n        self,\n        beta: float,\n        threshold: float = 0.5,\n        multidim_average: Literal['global', 'samplewise'] = 'global',\n        ignore_index: Optional[int] = None,\n        validate_args: bool = True,\n        zero_division: float = 0,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(\n            threshold=threshold,\n            multidim_average=multidim_average,\n            ignore_index=ignore_index,\n            validate_args=False,\n            **kwargs,\n        )\n        if validate_args:\n            _binary_fbeta_score_arg_validation(beta, threshold, multidim_average, ignore_index, zero_division)\n        self.validate_args = validate_args\n        self.zero_division = zero_division\n        self.beta = beta\n\n    def compute(self) -&gt; Tensor:\n        \"\"\"Compute metric.\"\"\"\n        tp, fp, tn, fn = self._final_state()\n        return _fbeta_reduce(\n            tp,\n            fp,\n            tn,\n            fn,\n            self.beta,\n            average='binary',\n            multidim_average=self.multidim_average,\n            zero_division=self.zero_division,\n        )\n\n    def plot(\n        self,\n        val: Optional[Tensor | Sequence[Tensor]] = None,\n        ax: Optional[_AX_TYPE] = None,  # type: ignore\n    ) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n        return self._plot(val, ax)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_stat_scores/#src.thaw_slump_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore.beta","title":"<code>beta = beta</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_stat_scores/#src.thaw_slump_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore.full_state_update","title":"<code>full_state_update: bool = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_stat_scores/#src.thaw_slump_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore.higher_is_better","title":"<code>higher_is_better: Optional[bool] = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_stat_scores/#src.thaw_slump_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore.is_differentiable","title":"<code>is_differentiable: bool = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_stat_scores/#src.thaw_slump_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore.plot_lower_bound","title":"<code>plot_lower_bound: float = 0.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_stat_scores/#src.thaw_slump_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore.plot_upper_bound","title":"<code>plot_upper_bound: float = 1.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_stat_scores/#src.thaw_slump_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore.validate_args","title":"<code>validate_args = validate_args</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_stat_scores/#src.thaw_slump_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore.zero_division","title":"<code>zero_division = zero_division</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_stat_scores/#src.thaw_slump_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore.__init__","title":"<code>__init__(beta, threshold=0.5, multidim_average='global', ignore_index=None, validate_args=True, zero_division=0, **kwargs)</code>","text":"Source code in <code>src/thaw_slump_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def __init__(\n    self,\n    beta: float,\n    threshold: float = 0.5,\n    multidim_average: Literal['global', 'samplewise'] = 'global',\n    ignore_index: Optional[int] = None,\n    validate_args: bool = True,\n    zero_division: float = 0,\n    **kwargs: Any,\n) -&gt; None:\n    super().__init__(\n        threshold=threshold,\n        multidim_average=multidim_average,\n        ignore_index=ignore_index,\n        validate_args=False,\n        **kwargs,\n    )\n    if validate_args:\n        _binary_fbeta_score_arg_validation(beta, threshold, multidim_average, ignore_index, zero_division)\n    self.validate_args = validate_args\n    self.zero_division = zero_division\n    self.beta = beta\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_stat_scores/#src.thaw_slump_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore.compute","title":"<code>compute()</code>","text":"<p>Compute metric.</p> Source code in <code>src/thaw_slump_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def compute(self) -&gt; Tensor:\n    \"\"\"Compute metric.\"\"\"\n    tp, fp, tn, fn = self._final_state()\n    return _fbeta_reduce(\n        tp,\n        fp,\n        tn,\n        fn,\n        self.beta,\n        average='binary',\n        multidim_average=self.multidim_average,\n        zero_division=self.zero_division,\n    )\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_stat_scores/#src.thaw_slump_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore.plot","title":"<code>plot(val=None, ax=None)</code>","text":"Source code in <code>src/thaw_slump_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def plot(\n    self,\n    val: Optional[Tensor | Sequence[Tensor]] = None,\n    ax: Optional[_AX_TYPE] = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    return self._plot(val, ax)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_stat_scores/#src.thaw_slump_segmentation.metrics.binary_instance_stat_scores.BinaryInstancePrecision","title":"<code>BinaryInstancePrecision</code>","text":"<p>               Bases: <code>BinaryInstanceStatScores</code></p> Source code in <code>src/thaw_slump_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>class BinaryInstancePrecision(BinaryInstanceStatScores):\n    is_differentiable: bool = False\n    higher_is_better: Optional[bool] = True\n    full_state_update: bool = False\n    plot_lower_bound: float = 0.0\n    plot_upper_bound: float = 1.0\n\n    def compute(self) -&gt; Tensor:\n        \"\"\"Compute metric.\"\"\"\n        tp, fp, tn, fn = self._final_state()\n        return _precision_recall_reduce(\n            'precision',\n            tp,\n            fp,\n            tn,\n            fn,\n            average='binary',\n            multidim_average=self.multidim_average,\n            zero_division=self.zero_division,\n        )\n\n    def plot(\n        self,\n        val: Optional[Tensor | Sequence[Tensor]] = None,\n        ax: Optional[_AX_TYPE] = None,  # type: ignore\n    ) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n        return self._plot(val, ax)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_stat_scores/#src.thaw_slump_segmentation.metrics.binary_instance_stat_scores.BinaryInstancePrecision.full_state_update","title":"<code>full_state_update: bool = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_stat_scores/#src.thaw_slump_segmentation.metrics.binary_instance_stat_scores.BinaryInstancePrecision.higher_is_better","title":"<code>higher_is_better: Optional[bool] = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_stat_scores/#src.thaw_slump_segmentation.metrics.binary_instance_stat_scores.BinaryInstancePrecision.is_differentiable","title":"<code>is_differentiable: bool = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_stat_scores/#src.thaw_slump_segmentation.metrics.binary_instance_stat_scores.BinaryInstancePrecision.plot_lower_bound","title":"<code>plot_lower_bound: float = 0.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_stat_scores/#src.thaw_slump_segmentation.metrics.binary_instance_stat_scores.BinaryInstancePrecision.plot_upper_bound","title":"<code>plot_upper_bound: float = 1.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_stat_scores/#src.thaw_slump_segmentation.metrics.binary_instance_stat_scores.BinaryInstancePrecision.compute","title":"<code>compute()</code>","text":"<p>Compute metric.</p> Source code in <code>src/thaw_slump_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def compute(self) -&gt; Tensor:\n    \"\"\"Compute metric.\"\"\"\n    tp, fp, tn, fn = self._final_state()\n    return _precision_recall_reduce(\n        'precision',\n        tp,\n        fp,\n        tn,\n        fn,\n        average='binary',\n        multidim_average=self.multidim_average,\n        zero_division=self.zero_division,\n    )\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_stat_scores/#src.thaw_slump_segmentation.metrics.binary_instance_stat_scores.BinaryInstancePrecision.plot","title":"<code>plot(val=None, ax=None)</code>","text":"Source code in <code>src/thaw_slump_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def plot(\n    self,\n    val: Optional[Tensor | Sequence[Tensor]] = None,\n    ax: Optional[_AX_TYPE] = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    return self._plot(val, ax)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_stat_scores/#src.thaw_slump_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceRecall","title":"<code>BinaryInstanceRecall</code>","text":"<p>               Bases: <code>BinaryInstanceStatScores</code></p> Source code in <code>src/thaw_slump_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>class BinaryInstanceRecall(BinaryInstanceStatScores):\n    is_differentiable: bool = False\n    higher_is_better: Optional[bool] = True\n    full_state_update: bool = False\n    plot_lower_bound: float = 0.0\n    plot_upper_bound: float = 1.0\n\n    def compute(self) -&gt; Tensor:\n        \"\"\"Compute metric.\"\"\"\n        tp, fp, tn, fn = self._final_state()\n        return _precision_recall_reduce(\n            'recall',\n            tp,\n            fp,\n            tn,\n            fn,\n            average='binary',\n            multidim_average=self.multidim_average,\n            zero_division=self.zero_division,\n        )\n\n    def plot(\n        self,\n        val: Optional[Tensor | Sequence[Tensor]] = None,\n        ax: Optional[_AX_TYPE] = None,  # type: ignore\n    ) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n        return self._plot(val, ax)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_stat_scores/#src.thaw_slump_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceRecall.full_state_update","title":"<code>full_state_update: bool = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_stat_scores/#src.thaw_slump_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceRecall.higher_is_better","title":"<code>higher_is_better: Optional[bool] = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_stat_scores/#src.thaw_slump_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceRecall.is_differentiable","title":"<code>is_differentiable: bool = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_stat_scores/#src.thaw_slump_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceRecall.plot_lower_bound","title":"<code>plot_lower_bound: float = 0.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_stat_scores/#src.thaw_slump_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceRecall.plot_upper_bound","title":"<code>plot_upper_bound: float = 1.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_stat_scores/#src.thaw_slump_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceRecall.compute","title":"<code>compute()</code>","text":"<p>Compute metric.</p> Source code in <code>src/thaw_slump_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def compute(self) -&gt; Tensor:\n    \"\"\"Compute metric.\"\"\"\n    tp, fp, tn, fn = self._final_state()\n    return _precision_recall_reduce(\n        'recall',\n        tp,\n        fp,\n        tn,\n        fn,\n        average='binary',\n        multidim_average=self.multidim_average,\n        zero_division=self.zero_division,\n    )\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_stat_scores/#src.thaw_slump_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceRecall.plot","title":"<code>plot(val=None, ax=None)</code>","text":"Source code in <code>src/thaw_slump_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def plot(\n    self,\n    val: Optional[Tensor | Sequence[Tensor]] = None,\n    ax: Optional[_AX_TYPE] = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    return self._plot(val, ax)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_stat_scores/#src.thaw_slump_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores","title":"<code>BinaryInstanceStatScores</code>","text":"<p>               Bases: <code>_AbstractStatScores</code></p> Source code in <code>src/thaw_slump_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>class BinaryInstanceStatScores(_AbstractStatScores):\n    is_differentiable: bool = False\n    higher_is_better: Optional[bool] = None\n    full_state_update: bool = False\n\n    def __init__(\n        self,\n        threshold: float = 0.5,\n        matching_threshold: float = 0.5,\n        matching_metric: MatchingMetric = 'iou',\n        boundary_dilation: float | int = 0.02,\n        multidim_average: Literal['global', 'samplewise'] = 'global',\n        ignore_index: Optional[int] = None,\n        validate_args: bool = True,\n        **kwargs: Any,\n    ) -&gt; None:\n        zero_division = kwargs.pop('zero_division', 0)\n        super(_AbstractStatScores, self).__init__(**kwargs)\n        if validate_args:\n            _binary_stat_scores_arg_validation(threshold, multidim_average, ignore_index, zero_division)\n            _boundary_arg_validation(matching_threshold, matching_metric, boundary_dilation)\n\n        self.threshold = threshold\n        self.matching_threshold = matching_threshold\n        self.matching_metric = matching_metric\n        self.boundary_dilation = boundary_dilation\n        self.multidim_average = multidim_average\n        self.ignore_index = ignore_index\n        self.validate_args = validate_args\n        self.zero_division = zero_division\n\n        if ignore_index is not None:\n            # TODO: implement ignore_index\n            raise ValueError('Argument `ignore_index` is not supported for this metric yet.')\n\n        self._create_state(size=1, multidim_average=multidim_average)\n\n    def update(self, preds: Tensor, target: Tensor) -&gt; None:\n        \"\"\"Update state with predictions and targets.\"\"\"\n        if self.validate_args:\n            _binary_stat_scores_tensor_validation(preds, target, self.multidim_average, self.ignore_index)\n            if not preds.dim() == 3:\n                raise ValueError(f'Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.')\n\n        # Format\n        if preds.is_floating_point():\n            if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n                # preds is logits, convert with sigmoid\n                preds = preds.sigmoid()\n            preds = preds &gt; self.threshold\n\n        if self.ignore_index is not None:\n            idx = target == self.ignore_index\n            target = target.clone()\n            target[idx] = -1\n\n        # Update state\n        instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n        instance_list_preds = mask_to_instances(preds.to(torch.uint8), self.validate_args)\n\n        for target_i, preds_i in zip(instance_list_target, instance_list_preds):\n            tp, fp, fn = match_instances(\n                target_i,\n                preds_i,\n                match_threshold=self.matching_threshold,\n                match_metric=self.matching_metric,\n                boundary_dilation=self.boundary_dilation,\n                validate_args=self.validate_args,\n            )\n            self._update_state(tp, fp, 0, fn)\n\n    def compute(self) -&gt; Tensor:\n        \"\"\"Compute the final statistics.\"\"\"\n        tp, fp, tn, fn = self._final_state()\n        return _binary_stat_scores_compute(tp, fp, tn, fn, self.multidim_average)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_stat_scores/#src.thaw_slump_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores.boundary_dilation","title":"<code>boundary_dilation = boundary_dilation</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_stat_scores/#src.thaw_slump_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores.full_state_update","title":"<code>full_state_update: bool = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_stat_scores/#src.thaw_slump_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores.higher_is_better","title":"<code>higher_is_better: Optional[bool] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_stat_scores/#src.thaw_slump_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores.ignore_index","title":"<code>ignore_index = ignore_index</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_stat_scores/#src.thaw_slump_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores.is_differentiable","title":"<code>is_differentiable: bool = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_stat_scores/#src.thaw_slump_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores.matching_metric","title":"<code>matching_metric = matching_metric</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_stat_scores/#src.thaw_slump_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores.matching_threshold","title":"<code>matching_threshold = matching_threshold</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_stat_scores/#src.thaw_slump_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores.multidim_average","title":"<code>multidim_average = multidim_average</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_stat_scores/#src.thaw_slump_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores.threshold","title":"<code>threshold = threshold</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_stat_scores/#src.thaw_slump_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores.validate_args","title":"<code>validate_args = validate_args</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_stat_scores/#src.thaw_slump_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores.zero_division","title":"<code>zero_division = zero_division</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_stat_scores/#src.thaw_slump_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores.__init__","title":"<code>__init__(threshold=0.5, matching_threshold=0.5, matching_metric='iou', boundary_dilation=0.02, multidim_average='global', ignore_index=None, validate_args=True, **kwargs)</code>","text":"Source code in <code>src/thaw_slump_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def __init__(\n    self,\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    matching_metric: MatchingMetric = 'iou',\n    boundary_dilation: float | int = 0.02,\n    multidim_average: Literal['global', 'samplewise'] = 'global',\n    ignore_index: Optional[int] = None,\n    validate_args: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    zero_division = kwargs.pop('zero_division', 0)\n    super(_AbstractStatScores, self).__init__(**kwargs)\n    if validate_args:\n        _binary_stat_scores_arg_validation(threshold, multidim_average, ignore_index, zero_division)\n        _boundary_arg_validation(matching_threshold, matching_metric, boundary_dilation)\n\n    self.threshold = threshold\n    self.matching_threshold = matching_threshold\n    self.matching_metric = matching_metric\n    self.boundary_dilation = boundary_dilation\n    self.multidim_average = multidim_average\n    self.ignore_index = ignore_index\n    self.validate_args = validate_args\n    self.zero_division = zero_division\n\n    if ignore_index is not None:\n        # TODO: implement ignore_index\n        raise ValueError('Argument `ignore_index` is not supported for this metric yet.')\n\n    self._create_state(size=1, multidim_average=multidim_average)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_stat_scores/#src.thaw_slump_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores.compute","title":"<code>compute()</code>","text":"<p>Compute the final statistics.</p> Source code in <code>src/thaw_slump_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def compute(self) -&gt; Tensor:\n    \"\"\"Compute the final statistics.\"\"\"\n    tp, fp, tn, fn = self._final_state()\n    return _binary_stat_scores_compute(tp, fp, tn, fn, self.multidim_average)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/metrics/binary_instance_stat_scores/#src.thaw_slump_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores.update","title":"<code>update(preds, target)</code>","text":"<p>Update state with predictions and targets.</p> Source code in <code>src/thaw_slump_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update state with predictions and targets.\"\"\"\n    if self.validate_args:\n        _binary_stat_scores_tensor_validation(preds, target, self.multidim_average, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f'Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.')\n\n    # Format\n    if preds.is_floating_point():\n        if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n            # preds is logits, convert with sigmoid\n            preds = preds.sigmoid()\n        preds = preds &gt; self.threshold\n\n    if self.ignore_index is not None:\n        idx = target == self.ignore_index\n        target = target.clone()\n        target[idx] = -1\n\n    # Update state\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n    instance_list_preds = mask_to_instances(preds.to(torch.uint8), self.validate_args)\n\n    for target_i, preds_i in zip(instance_list_target, instance_list_preds):\n        tp, fp, fn = match_instances(\n            target_i,\n            preds_i,\n            match_threshold=self.matching_threshold,\n            match_metric=self.matching_metric,\n            boundary_dilation=self.boundary_dilation,\n            validate_args=self.validate_args,\n        )\n        self._update_state(tp, fp, 0, fn)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/metrics/boundary_helpers/","title":"Boundary helpers","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/boundary_helpers/#src.thaw_slump_segmentation.metrics.boundary_helpers.MatchingMetric","title":"<code>MatchingMetric = Literal['iou', 'boundary']</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/boundary_helpers/#src.thaw_slump_segmentation.metrics.boundary_helpers.erode_pytorch","title":"<code>erode_pytorch(mask, iterations=1, validate_args=False)</code>","text":"<p>Erodes a binary mask using a square kernel in PyTorch.</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>Tensor</code> <p>The binary mask. Shape: (batch_size, height, width), dtype: torch.uint8</p> required <code>iterations</code> <code>int</code> <p>The size of the erosion. Defaults to 1.</p> <code>1</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The eroded mask. Shape: (batch_size, height, width), dtype: torch.uint8</p> Source code in <code>src/thaw_slump_segmentation/metrics/boundary_helpers.py</code> <pre><code>@torch.no_grad()\ndef erode_pytorch(mask: torch.Tensor, iterations: int = 1, validate_args: bool = False) -&gt; torch.Tensor:\n    \"\"\"Erodes a binary mask using a square kernel in PyTorch.\n\n    Args:\n        mask (torch.Tensor): The binary mask. Shape: (batch_size, height, width), dtype: torch.uint8\n        iterations (int, optional): The size of the erosion. Defaults to 1.\n\n    Returns:\n        torch.Tensor: The eroded mask. Shape: (batch_size, height, width), dtype: torch.uint8\n    \"\"\"\n    if validate_args:\n        assert mask.dim() == 3, f'Expected 3 dimensions, got {mask.dim()}'\n        assert mask.dtype == torch.uint8, f'Expected torch.uint8, got {mask.dtype}'\n        assert mask.min() &gt;= 0 and mask.max() &lt;= 1, f'Expected binary mask, got {mask.min()} and {mask.max()}'\n\n    kernel = torch.ones(1, 1, 3, 3, device=mask.device)\n    erode = torch.nn.functional.conv2d(mask.float().unsqueeze(1), kernel, padding=1, stride=1)\n\n    for _ in range(iterations - 1):\n        erode = torch.nn.functional.conv2d(erode, kernel, padding=1, stride=1)\n\n    return (erode == erode.max()).to(torch.uint8).squeeze(1)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/metrics/boundary_helpers/#src.thaw_slump_segmentation.metrics.boundary_helpers.get_boundaries","title":"<code>get_boundaries(instances_target_onehot, instances_preds_onehot, dilation=0.02, validate_args=False)</code>","text":"Source code in <code>src/thaw_slump_segmentation/metrics/boundary_helpers.py</code> <pre><code>@torch.no_grad()\ndef get_boundaries(\n    instances_target_onehot: torch.Tensor,\n    instances_preds_onehot: torch.Tensor,\n    dilation: float | int = 0.02,\n    validate_args: bool = False,\n):\n    if validate_args:\n        assert instances_target_onehot.dim() == 3, f'Expected 3 dimensions, got {instances_target_onehot.dim()}'\n        assert instances_preds_onehot.dim() == 3, f'Expected 3 dimensions, got {instances_preds_onehot.dim()}'\n        assert (\n            instances_target_onehot.dtype == torch.uint8\n        ), f'Expected torch.uint8, got {instances_target_onehot.dtype}'\n        assert instances_preds_onehot.dtype == torch.uint8, f'Expected torch.uint8, got {instances_preds_onehot.dtype}'\n        assert (\n            instances_target_onehot.shape[1:] == instances_preds_onehot.shape[1:]\n        ), f'Shapes (..., H, W) do not match: {instances_target_onehot.shape} and {instances_preds_onehot.shape}'\n        assert (\n            instances_target_onehot.min() &gt;= 0 and instances_target_onehot.max() &lt;= 1\n        ), f'Expected binary mask, got {instances_target_onehot.min()} and {instances_target_onehot.max()}'\n        assert (\n            instances_preds_onehot.min() &gt;= 0 and instances_preds_onehot.max() &lt;= 1\n        ), f'Expected binary mask, got {instances_preds_onehot.min()} and {instances_preds_onehot.max()}'\n\n    n, h, w = instances_target_onehot.shape\n    if isinstance(dilation, float):\n        img_diag = sqrt(h**2 + w**2)\n        dilation = int(round(dilation * img_diag))\n        if dilation &lt; 1:\n            dilation = 1\n\n    # Pad the instances to avoid boundary issues\n    pad = torchvision.transforms.Pad(1)\n    instances_target_onehot_padded = pad(instances_target_onehot)\n    instances_preds_onehot_padded = pad(instances_preds_onehot)\n\n    # Erode the instances to get the boundaries\n    eroded_target = erode_pytorch(instances_target_onehot_padded, iterations=dilation, validate_args=validate_args)\n    eroded_preds = erode_pytorch(instances_preds_onehot_padded, iterations=dilation, validate_args=validate_args)\n\n    # Remove the padding\n    eroded_target = eroded_target[:, 1:-1, 1:-1]\n    eroded_preds = eroded_preds[:, 1:-1, 1:-1]\n\n    # Calculate the boundary of the instances\n    boundaries_target = instances_target_onehot - eroded_target\n    boundaries_preds = instances_preds_onehot - eroded_preds\n    return boundaries_target, boundaries_preds\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/metrics/boundary_helpers/#src.thaw_slump_segmentation.metrics.boundary_helpers.instance_boundary_iou","title":"<code>instance_boundary_iou(instances_target_onehot, instances_preds_onehot, dilation=0.02, validate_args=False)</code>","text":"<p>Calculates the IoU of the boundaries of instances. Expects non-batched, one-hot encoded input from skimage.measure.label</p> <p>Parameters:</p> Name Type Description Default <code>instances_target</code> <code>Tensor</code> <p>The instance mask of the target. Shape: (num_instances, height, width), dtype: torch.uint8</p> required <code>instances_preds</code> <code>Tensor</code> <p>The instance mask of the prediction. Shape: (num_instances, height, width), dtype: torch.uint8</p> required <code>dilation</code> <code>float | int</code> <p>The dilation factor for the boundary. Dilation in pixels if int, else ratio to calculate <code>dilation = dilation_ratio * image_diagonal</code>. Default: 0.02</p> <code>0.02</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The IoU of the boundaries. Shape: (num_instances,)</p> Source code in <code>src/thaw_slump_segmentation/metrics/boundary_helpers.py</code> <pre><code>@torch.no_grad()\ndef instance_boundary_iou(\n    instances_target_onehot: torch.Tensor,\n    instances_preds_onehot: torch.Tensor,\n    dilation: float | int = 0.02,\n    validate_args: bool = False,\n) -&gt; torch.Tensor:\n    \"\"\"Calculates the IoU of the boundaries of instances.\n    Expects non-batched, one-hot encoded input from skimage.measure.label\n\n    Args:\n        instances_target (torch.Tensor): The instance mask of the target.\n            Shape: (num_instances, height, width), dtype: torch.uint8\n        instances_preds (torch.Tensor): The instance mask of the prediction.\n            Shape: (num_instances, height, width), dtype: torch.uint8\n        dilation (float | int): The dilation factor for the boundary.\n            Dilation in pixels if int, else ratio to calculate `dilation = dilation_ratio * image_diagonal`.\n            Default: 0.02\n\n    Returns:\n        torch.Tensor: The IoU of the boundaries. Shape: (num_instances,)\n    \"\"\"\n\n    # Calculate the boundary of the instances\n    boundaries_target, boundaries_preds = get_boundaries(\n        instances_target_onehot, instances_preds_onehot, dilation, validate_args\n    )\n\n    # Calculate the IoU of the boundaries (broadcast because of the different number of instances)\n    intersection = (boundaries_target.unsqueeze(1) &amp; boundaries_preds.unsqueeze(0)).sum(\n        dim=(2, 3)\n    )  # Shape: (num_instances_target, num_instances_preds)\n    union = (boundaries_target.unsqueeze(1) | boundaries_preds.unsqueeze(0)).sum(\n        dim=(2, 3)\n    )  # Shape: (num_instances_target, num_instances_preds)\n    iou = intersection / union  # Shape: (num_instances_target, num_instances_preds)\n\n    return iou\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/metrics/boundary_iou/","title":"Boundary iou","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/boundary_iou/#src.thaw_slump_segmentation.metrics.boundary_iou.MatchingMetric","title":"<code>MatchingMetric = Literal['iou', 'boundary']</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/boundary_iou/#src.thaw_slump_segmentation.metrics.boundary_iou.BinaryBoundaryIoU","title":"<code>BinaryBoundaryIoU</code>","text":"<p>               Bases: <code>Metric</code></p> Source code in <code>src/thaw_slump_segmentation/metrics/boundary_iou.py</code> <pre><code>class BinaryBoundaryIoU(Metric):\n    intersection: Tensor | list[Tensor]\n    union: Tensor | list[Tensor]\n\n    is_differentiable: bool = False\n    higher_is_better: Optional[bool] = True\n    full_state_update: bool = False\n    plot_lower_bound: float = 0.0\n    plot_upper_bound: float = 1.0\n\n    def __init__(\n        self,\n        dilation: float | int = 0.02,\n        threshold: float = 0.5,\n        multidim_average: Literal['global', 'samplewise'] = 'global',\n        ignore_index: Optional[int] = None,\n        validate_args: bool = True,\n        **kwargs,\n    ):\n        zero_division = kwargs.pop('zero_division', 0)\n        super().__init__(**kwargs)\n\n        if validate_args:\n            _binary_stat_scores_arg_validation(threshold, multidim_average, ignore_index, zero_division)\n            if not isinstance(dilation, (float, int)):\n                raise ValueError(f'Expected argument `dilation` to be a float or int, but got {dilation}.')\n\n        self.dilation = dilation\n        self.threshold = threshold\n        self.multidim_average = multidim_average\n        self.ignore_index = ignore_index\n        self.validate_args = validate_args\n        self.zero_division = zero_division\n\n        if ignore_index is not None:\n            # TODO: implement ignore_index\n            raise ValueError('Argument `ignore_index` is not supported for this metric yet.')\n\n        if multidim_average == 'samplewise':\n            self.add_state('intersection', default=[], dist_reduce_fx='cat')\n            self.add_state('union', default=[], dist_reduce_fx='cat')\n        else:\n            self.add_state('intersection', default=torch.tensor(0), dist_reduce_fx='sum')\n            self.add_state('union', default=torch.tensor(0), dist_reduce_fx='sum')\n\n    def update(self, preds: Tensor, target: Tensor) -&gt; None:\n        if self.validate_args:\n            _binary_stat_scores_tensor_validation(preds, target, self.multidim_average, self.ignore_index)\n            if not preds.shape == target.shape:\n                raise ValueError(\n                    f'Expected `preds` and `target` to have the same shape, but got {preds.shape} and {target.shape}.'\n                )\n            if not preds.dim() == 3:\n                raise ValueError(f'Expected `preds` and `target` to have 3 dimensions, but got {preds.dim()}.')\n\n        # Format\n        if preds.is_floating_point():\n            if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n                # preds is logits, convert with sigmoid\n                preds = preds.sigmoid()\n            preds = preds &gt; self.threshold\n\n        if self.ignore_index is not None:\n            idx = target == self.ignore_index\n            target = target.clone()\n            target[idx] = -1\n\n        target = target.to(torch.uint8)\n        preds = preds.to(torch.uint8)\n\n        target, preds = get_boundaries(target, preds, self.dilation, self.validate_args)\n\n        intersection = (target &amp; preds).sum().item()\n        union = (target | preds).sum().item()\n\n        if self.multidim_average == 'global':\n            self.intersection += intersection\n            self.union += union\n        else:\n            self.intersection.append(intersection)\n            self.union.append(union)\n\n    def compute(self) -&gt; Tensor:\n        if self.multidim_average == 'global':\n            return self.intersection / self.union\n        else:\n            self.intersection = torch.tensor(self.intersection)\n            self.union = torch.tensor(self.union)\n            return self.intersection / self.union\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/metrics/boundary_iou/#src.thaw_slump_segmentation.metrics.boundary_iou.BinaryBoundaryIoU.dilation","title":"<code>dilation = dilation</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/boundary_iou/#src.thaw_slump_segmentation.metrics.boundary_iou.BinaryBoundaryIoU.full_state_update","title":"<code>full_state_update: bool = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/boundary_iou/#src.thaw_slump_segmentation.metrics.boundary_iou.BinaryBoundaryIoU.higher_is_better","title":"<code>higher_is_better: Optional[bool] = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/boundary_iou/#src.thaw_slump_segmentation.metrics.boundary_iou.BinaryBoundaryIoU.ignore_index","title":"<code>ignore_index = ignore_index</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/boundary_iou/#src.thaw_slump_segmentation.metrics.boundary_iou.BinaryBoundaryIoU.intersection","title":"<code>intersection: Tensor | list[Tensor]</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/boundary_iou/#src.thaw_slump_segmentation.metrics.boundary_iou.BinaryBoundaryIoU.is_differentiable","title":"<code>is_differentiable: bool = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/boundary_iou/#src.thaw_slump_segmentation.metrics.boundary_iou.BinaryBoundaryIoU.multidim_average","title":"<code>multidim_average = multidim_average</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/boundary_iou/#src.thaw_slump_segmentation.metrics.boundary_iou.BinaryBoundaryIoU.plot_lower_bound","title":"<code>plot_lower_bound: float = 0.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/boundary_iou/#src.thaw_slump_segmentation.metrics.boundary_iou.BinaryBoundaryIoU.plot_upper_bound","title":"<code>plot_upper_bound: float = 1.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/boundary_iou/#src.thaw_slump_segmentation.metrics.boundary_iou.BinaryBoundaryIoU.threshold","title":"<code>threshold = threshold</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/boundary_iou/#src.thaw_slump_segmentation.metrics.boundary_iou.BinaryBoundaryIoU.union","title":"<code>union: Tensor | list[Tensor]</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/boundary_iou/#src.thaw_slump_segmentation.metrics.boundary_iou.BinaryBoundaryIoU.validate_args","title":"<code>validate_args = validate_args</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/boundary_iou/#src.thaw_slump_segmentation.metrics.boundary_iou.BinaryBoundaryIoU.zero_division","title":"<code>zero_division = zero_division</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/boundary_iou/#src.thaw_slump_segmentation.metrics.boundary_iou.BinaryBoundaryIoU.__init__","title":"<code>__init__(dilation=0.02, threshold=0.5, multidim_average='global', ignore_index=None, validate_args=True, **kwargs)</code>","text":"Source code in <code>src/thaw_slump_segmentation/metrics/boundary_iou.py</code> <pre><code>def __init__(\n    self,\n    dilation: float | int = 0.02,\n    threshold: float = 0.5,\n    multidim_average: Literal['global', 'samplewise'] = 'global',\n    ignore_index: Optional[int] = None,\n    validate_args: bool = True,\n    **kwargs,\n):\n    zero_division = kwargs.pop('zero_division', 0)\n    super().__init__(**kwargs)\n\n    if validate_args:\n        _binary_stat_scores_arg_validation(threshold, multidim_average, ignore_index, zero_division)\n        if not isinstance(dilation, (float, int)):\n            raise ValueError(f'Expected argument `dilation` to be a float or int, but got {dilation}.')\n\n    self.dilation = dilation\n    self.threshold = threshold\n    self.multidim_average = multidim_average\n    self.ignore_index = ignore_index\n    self.validate_args = validate_args\n    self.zero_division = zero_division\n\n    if ignore_index is not None:\n        # TODO: implement ignore_index\n        raise ValueError('Argument `ignore_index` is not supported for this metric yet.')\n\n    if multidim_average == 'samplewise':\n        self.add_state('intersection', default=[], dist_reduce_fx='cat')\n        self.add_state('union', default=[], dist_reduce_fx='cat')\n    else:\n        self.add_state('intersection', default=torch.tensor(0), dist_reduce_fx='sum')\n        self.add_state('union', default=torch.tensor(0), dist_reduce_fx='sum')\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/metrics/boundary_iou/#src.thaw_slump_segmentation.metrics.boundary_iou.BinaryBoundaryIoU.compute","title":"<code>compute()</code>","text":"Source code in <code>src/thaw_slump_segmentation/metrics/boundary_iou.py</code> <pre><code>def compute(self) -&gt; Tensor:\n    if self.multidim_average == 'global':\n        return self.intersection / self.union\n    else:\n        self.intersection = torch.tensor(self.intersection)\n        self.union = torch.tensor(self.union)\n        return self.intersection / self.union\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/metrics/boundary_iou/#src.thaw_slump_segmentation.metrics.boundary_iou.BinaryBoundaryIoU.update","title":"<code>update(preds, target)</code>","text":"Source code in <code>src/thaw_slump_segmentation/metrics/boundary_iou.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    if self.validate_args:\n        _binary_stat_scores_tensor_validation(preds, target, self.multidim_average, self.ignore_index)\n        if not preds.shape == target.shape:\n            raise ValueError(\n                f'Expected `preds` and `target` to have the same shape, but got {preds.shape} and {target.shape}.'\n            )\n        if not preds.dim() == 3:\n            raise ValueError(f'Expected `preds` and `target` to have 3 dimensions, but got {preds.dim()}.')\n\n    # Format\n    if preds.is_floating_point():\n        if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n            # preds is logits, convert with sigmoid\n            preds = preds.sigmoid()\n        preds = preds &gt; self.threshold\n\n    if self.ignore_index is not None:\n        idx = target == self.ignore_index\n        target = target.clone()\n        target[idx] = -1\n\n    target = target.to(torch.uint8)\n    preds = preds.to(torch.uint8)\n\n    target, preds = get_boundaries(target, preds, self.dilation, self.validate_args)\n\n    intersection = (target &amp; preds).sum().item()\n    union = (target | preds).sum().item()\n\n    if self.multidim_average == 'global':\n        self.intersection += intersection\n        self.union += union\n    else:\n        self.intersection.append(intersection)\n        self.union.append(union)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/metrics/instance_helpers/","title":"Instance helpers","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/instance_helpers/#src.thaw_slump_segmentation.metrics.instance_helpers.CUCIM_AVAILABLE","title":"<code>CUCIM_AVAILABLE = True</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/instance_helpers/#src.thaw_slump_segmentation.metrics.instance_helpers.MatchingMetric","title":"<code>MatchingMetric = Literal['iou', 'boundary']</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/metrics/instance_helpers/#src.thaw_slump_segmentation.metrics.instance_helpers.mask_to_instances","title":"<code>mask_to_instances(x, validate_args=False)</code>","text":"<p>Converts a binary segmentation mask into multiple instance masks. Expects a batched version of the input.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The binary segmentation mask. Shape: (batch_size, height, width), dtype: torch.uint8</p> required <p>Returns:</p> Type Description <code>list[Tensor]</code> <p>list[torch.Tensor]: The instance masks. Length of list: batch_size. Shape of a tensor: (height, width), dtype: torch.uint8</p> Source code in <code>src/thaw_slump_segmentation/metrics/instance_helpers.py</code> <pre><code>@torch.no_grad()\ndef mask_to_instances(x: torch.Tensor, validate_args: bool = False) -&gt; list[torch.Tensor]:\n    \"\"\"Converts a binary segmentation mask into multiple instance masks. Expects a batched version of the input.\n\n    Args:\n        x (torch.Tensor): The binary segmentation mask. Shape: (batch_size, height, width), dtype: torch.uint8\n\n    Returns:\n        list[torch.Tensor]: The instance masks. Length of list: batch_size.\n            Shape of a tensor: (height, width), dtype: torch.uint8\n    \"\"\"\n    if validate_args:\n        assert x.dim() == 3, f'Expected 3 dimensions, got {x.dim()}'\n        assert x.dtype == torch.uint8, f'Expected torch.uint8, got {x.dtype}'\n        assert x.min() &gt;= 0 and x.max() &lt;= 1, f'Expected binary mask, got {x.min()} and {x.max()}'\n\n    if CUCIM_AVAILABLE:\n        # Check if device is cuda\n        assert x.device.type == 'cuda', f'Expected device to be cuda, got {x.device.type}'\n        x = cp.asarray(x).astype(cp.uint8)\n\n        instances = []\n        for x_i in x:\n            instances_i = label_cucim(x_i)\n            instances_i = torch.tensor(instances_i, dtype=torch.uint8)\n            instances.append(instances_i)\n        return instances\n\n    else:\n        instances = []\n        for x_i in x:\n            x_i = x_i.cpu().numpy()\n            instances_i = label_skimage(x_i)\n            instances_i = torch.tensor(instances_i, dtype=torch.uint8)\n            instances.append(instances_i)\n        return instances\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/metrics/instance_helpers/#src.thaw_slump_segmentation.metrics.instance_helpers.match_instances","title":"<code>match_instances(instances_target, instances_preds, match_metric='iou', match_threshold=0.5, boundary_dilation=0.02, validate_args=False)</code>","text":"<p>Matches instances between target and prediction masks. Expects non-batched input from skimage.measure.label.</p> <p>Parameters:</p> Name Type Description Default <code>instances_target</code> <code>Tensor</code> <p>The instance mask of the target. Shape: (height, width), dtype: torch.uint8</p> required <code>instances_preds</code> <code>Tensor</code> <p>The instance mask of the prediction. Shape: (height, width), dtype: torch.uint8</p> required <p>Returns:</p> Type Description <code>tuple[int, int, int]</code> <p>tuple[int, int, int]: True positives, false positives, false negatives</p> Source code in <code>src/thaw_slump_segmentation/metrics/instance_helpers.py</code> <pre><code>@torch.no_grad()\ndef match_instances(\n    instances_target: torch.Tensor,\n    instances_preds: torch.Tensor,\n    match_metric: MatchingMetric = 'iou',\n    match_threshold: float = 0.5,\n    boundary_dilation: float | int = 0.02,\n    validate_args: bool = False,\n) -&gt; tuple[int, int, int]:\n    \"\"\"Matches instances between target and prediction masks. Expects non-batched input from skimage.measure.label.\n\n    Args:\n        instances_target (torch.Tensor): The instance mask of the target. Shape: (height, width), dtype: torch.uint8\n        instances_preds (torch.Tensor): The instance mask of the prediction. Shape: (height, width), dtype: torch.uint8\n\n    Returns:\n        tuple[int, int, int]: True positives, false positives, false negatives\n    \"\"\"\n    if validate_args:\n        assert instances_target.dim() == 2, f'Expected 2 dimensions, got {instances_target.dim()}'\n        assert instances_preds.dim() == 2, f'Expected 2 dimensions, got {instances_preds.dim()}'\n        assert instances_target.dtype == torch.uint8, f'Expected torch.uint8, got {instances_target.dtype}'\n        assert instances_preds.dtype == torch.uint8, f'Expected torch.uint8, got {instances_preds.dtype}'\n        assert (\n            instances_target.shape == instances_preds.shape\n        ), f'Shapes do not match: {instances_target.shape} and {instances_preds.shape}'\n\n    # Create one-hot encoding of instances, so that each instance is a channel\n    target_labels = list(range(1, instances_target.max() + 1))\n    pred_labels = list(range(1, instances_preds.max() + 1))\n\n    if len(target_labels) == 0:\n        return 0, len(pred_labels), 0\n\n    if len(pred_labels) == 0:\n        return 0, 0, len(target_labels)\n\n    instances_target_onehot = torch.stack([instances_target == i for i in target_labels], dim=0).to(torch.uint8)\n    instances_preds_onehot = torch.stack([instances_preds == i for i in pred_labels], dim=0).to(torch.uint8)\n\n    # Now the instances are channels, hence tensors of shape (num_instances, height, width)\n\n    # Calculate IoU (we need to do a n-m intersection and union, therefore we need to broadcast)\n    intersection = (instances_target_onehot.unsqueeze(1) &amp; instances_preds_onehot.unsqueeze(0)).sum(\n        dim=(2, 3)\n    )  # Shape: (num_instances_target, num_instances_preds)\n    union = (instances_target_onehot.unsqueeze(1) | instances_preds_onehot.unsqueeze(0)).sum(\n        dim=(2, 3)\n    )  # Shape: (num_instances_target, num_instances_preds)\n    iou = intersection / union  # Shape: (num_instances_target, num_instances_preds)\n\n    if match_metric == 'boundary':\n        boundary_iou = instance_boundary_iou(\n            instances_target_onehot, instances_preds_onehot, boundary_dilation, validate_args\n        )\n        # Use the minimum IoU of the instance and its boundary, like described in the BoundaryIoU paper\n        iou = torch.min(iou, boundary_iou)\n\n    # Match instances based on IoU\n    tp = (iou &gt;= match_threshold).sum().item()\n    fp = len(pred_labels) - tp\n    fn = len(target_labels) - tp\n\n    return tp, fp, fn\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/__version__/","title":"version","text":""},{"location":"reference/src/thaw_slump_segmentation/models/__version__/#src.thaw_slump_segmentation.models.__version__.VERSION","title":"<code>VERSION = (0, 1, 3)</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/__version__/#src.thaw_slump_segmentation.models.__version__.__version__","title":"<code>__version__ = '.'.join(map(str, VERSION))</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/base/heads/","title":"Heads","text":""},{"location":"reference/src/thaw_slump_segmentation/models/base/heads/#src.thaw_slump_segmentation.models.base.heads.ClassificationHead","title":"<code>ClassificationHead</code>","text":"<p>               Bases: <code>Sequential</code></p> Source code in <code>src/thaw_slump_segmentation/models/base/heads.py</code> <pre><code>class ClassificationHead(nn.Sequential):\n\n    def __init__(self, in_channels, classes, pooling=\"avg\", dropout=0.2, activation=None):\n        if pooling not in (\"max\", \"avg\"):\n            raise ValueError(\"Pooling should be one of ('max', 'avg'), got {}.\".format(pooling))\n        pool = nn.AdaptiveAvgPool2d(1) if pooling == 'avg' else nn.AdaptiveMaxPool2d(1)\n        flatten = Flatten()\n        dropout = nn.Dropout(p=dropout, inplace=True) if dropout else nn.Identity()\n        linear = nn.Linear(in_channels, classes, bias=True)\n        activation = Activation(activation)\n        super().__init__(pool, flatten, dropout, linear, activation)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/base/heads/#src.thaw_slump_segmentation.models.base.heads.ClassificationHead.__init__","title":"<code>__init__(in_channels, classes, pooling='avg', dropout=0.2, activation=None)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/base/heads.py</code> <pre><code>def __init__(self, in_channels, classes, pooling=\"avg\", dropout=0.2, activation=None):\n    if pooling not in (\"max\", \"avg\"):\n        raise ValueError(\"Pooling should be one of ('max', 'avg'), got {}.\".format(pooling))\n    pool = nn.AdaptiveAvgPool2d(1) if pooling == 'avg' else nn.AdaptiveMaxPool2d(1)\n    flatten = Flatten()\n    dropout = nn.Dropout(p=dropout, inplace=True) if dropout else nn.Identity()\n    linear = nn.Linear(in_channels, classes, bias=True)\n    activation = Activation(activation)\n    super().__init__(pool, flatten, dropout, linear, activation)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/base/heads/#src.thaw_slump_segmentation.models.base.heads.SegmentationHead","title":"<code>SegmentationHead</code>","text":"<p>               Bases: <code>Sequential</code></p> Source code in <code>src/thaw_slump_segmentation/models/base/heads.py</code> <pre><code>class SegmentationHead(nn.Sequential):\n\n    def __init__(self, in_channels, out_channels, kernel_size=3, activation=None, upsampling=1):\n        conv2d = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size // 2)\n        upsampling = nn.UpsamplingBilinear2d(scale_factor=upsampling) if upsampling &gt; 1 else nn.Identity()\n        activation = Activation(activation)\n        super().__init__(conv2d, upsampling, activation)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/base/heads/#src.thaw_slump_segmentation.models.base.heads.SegmentationHead.__init__","title":"<code>__init__(in_channels, out_channels, kernel_size=3, activation=None, upsampling=1)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/base/heads.py</code> <pre><code>def __init__(self, in_channels, out_channels, kernel_size=3, activation=None, upsampling=1):\n    conv2d = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size // 2)\n    upsampling = nn.UpsamplingBilinear2d(scale_factor=upsampling) if upsampling &gt; 1 else nn.Identity()\n    activation = Activation(activation)\n    super().__init__(conv2d, upsampling, activation)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/base/initialization/","title":"Initialization","text":""},{"location":"reference/src/thaw_slump_segmentation/models/base/initialization/#src.thaw_slump_segmentation.models.base.initialization.initialize_decoder","title":"<code>initialize_decoder(module)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/base/initialization.py</code> <pre><code>def initialize_decoder(module):\n    for m in module.modules():\n\n        if isinstance(m, nn.Conv2d):\n            nn.init.kaiming_uniform_(m.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n\n        elif isinstance(m, nn.BatchNorm2d):\n            nn.init.constant_(m.weight, 1)\n            nn.init.constant_(m.bias, 0)\n\n        elif isinstance(m, nn.Linear):\n            nn.init.xavier_uniform_(m.weight)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/base/initialization/#src.thaw_slump_segmentation.models.base.initialization.initialize_head","title":"<code>initialize_head(module)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/base/initialization.py</code> <pre><code>def initialize_head(module):\n    for m in module.modules():\n        if isinstance(m, (nn.Linear, nn.Conv2d)):\n            nn.init.xavier_uniform_(m.weight)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/base/model/","title":"Model","text":""},{"location":"reference/src/thaw_slump_segmentation/models/base/model/#src.thaw_slump_segmentation.models.base.model.SegmentationModel","title":"<code>SegmentationModel</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/thaw_slump_segmentation/models/base/model.py</code> <pre><code>class SegmentationModel(torch.nn.Module):\n\n    def initialize(self):\n        init.initialize_decoder(self.decoder)\n        init.initialize_head(self.segmentation_head)\n        if self.classification_head is not None:\n            init.initialize_head(self.classification_head)\n\n    def forward(self, x):\n        \"\"\"Sequentially pass `x` trough model`s encoder, decoder and heads\"\"\"\n        features = self.encoder(x)\n        decoder_output = self.decoder(*features)\n\n        masks = self.segmentation_head(decoder_output)\n\n        if self.classification_head is not None:\n            labels = self.classification_head(features[-1])\n            return masks, labels\n\n        return masks\n\n    def predict(self, x):\n        \"\"\"Inference method. Switch model to `eval` mode, call `.forward(x)` with `torch.no_grad()`\n\n        Args:\n            x: 4D torch tensor with shape (batch_size, channels, height, width)\n\n        Return:\n            prediction: 4D torch tensor with shape (batch_size, classes, height, width)\n\n        \"\"\"\n        if self.training:\n            self.eval()\n\n        with torch.no_grad():\n            x = self.forward(x)\n\n        return x\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/base/model/#src.thaw_slump_segmentation.models.base.model.SegmentationModel.forward","title":"<code>forward(x)</code>","text":"<p>Sequentially pass <code>x</code> trough model`s encoder, decoder and heads</p> Source code in <code>src/thaw_slump_segmentation/models/base/model.py</code> <pre><code>def forward(self, x):\n    \"\"\"Sequentially pass `x` trough model`s encoder, decoder and heads\"\"\"\n    features = self.encoder(x)\n    decoder_output = self.decoder(*features)\n\n    masks = self.segmentation_head(decoder_output)\n\n    if self.classification_head is not None:\n        labels = self.classification_head(features[-1])\n        return masks, labels\n\n    return masks\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/base/model/#src.thaw_slump_segmentation.models.base.model.SegmentationModel.initialize","title":"<code>initialize()</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/base/model.py</code> <pre><code>def initialize(self):\n    init.initialize_decoder(self.decoder)\n    init.initialize_head(self.segmentation_head)\n    if self.classification_head is not None:\n        init.initialize_head(self.classification_head)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/base/model/#src.thaw_slump_segmentation.models.base.model.SegmentationModel.predict","title":"<code>predict(x)</code>","text":"<p>Inference method. Switch model to <code>eval</code> mode, call <code>.forward(x)</code> with <code>torch.no_grad()</code></p> <p>Parameters:</p> Name Type Description Default <code>x</code> <p>4D torch tensor with shape (batch_size, channels, height, width)</p> required Return <p>prediction: 4D torch tensor with shape (batch_size, classes, height, width)</p> Source code in <code>src/thaw_slump_segmentation/models/base/model.py</code> <pre><code>def predict(self, x):\n    \"\"\"Inference method. Switch model to `eval` mode, call `.forward(x)` with `torch.no_grad()`\n\n    Args:\n        x: 4D torch tensor with shape (batch_size, channels, height, width)\n\n    Return:\n        prediction: 4D torch tensor with shape (batch_size, classes, height, width)\n\n    \"\"\"\n    if self.training:\n        self.eval()\n\n    with torch.no_grad():\n        x = self.forward(x)\n\n    return x\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/base/modules/","title":"Modules","text":""},{"location":"reference/src/thaw_slump_segmentation/models/base/modules/#src.thaw_slump_segmentation.models.base.modules.Activation","title":"<code>Activation</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/thaw_slump_segmentation/models/base/modules.py</code> <pre><code>class Activation(nn.Module):\n\n    def __init__(self, name, **params):\n\n        super().__init__()\n\n        if name is None or name == 'identity':\n            self.activation = nn.Identity(**params)\n        elif name == 'sigmoid':\n            self.activation = nn.Sigmoid()\n        elif name == 'softmax2d':\n            self.activation = nn.Softmax(dim=1, **params)\n        elif name == 'softmax':\n            self.activation = nn.Softmax(**params)\n        elif name == 'logsoftmax':\n            self.activation = nn.LogSoftmax(**params)\n        elif name == 'tanh':\n            self.activation = nn.Tanh()\n        elif name == 'argmax':\n            self.activation = ArgMax(**params)\n        elif name == 'argmax2d':\n            self.activation = ArgMax(dim=1, **params)\n        elif callable(name):\n            self.activation = name(**params)\n        else:\n            raise ValueError('Activation should be callable/sigmoid/softmax/logsoftmax/tanh/None; got {}'.format(name))\n\n    def forward(self, x):\n        return self.activation(x)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/base/modules/#src.thaw_slump_segmentation.models.base.modules.Activation.activation","title":"<code>activation = nn.Identity(**params)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/base/modules/#src.thaw_slump_segmentation.models.base.modules.Activation.__init__","title":"<code>__init__(name, **params)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/base/modules.py</code> <pre><code>def __init__(self, name, **params):\n\n    super().__init__()\n\n    if name is None or name == 'identity':\n        self.activation = nn.Identity(**params)\n    elif name == 'sigmoid':\n        self.activation = nn.Sigmoid()\n    elif name == 'softmax2d':\n        self.activation = nn.Softmax(dim=1, **params)\n    elif name == 'softmax':\n        self.activation = nn.Softmax(**params)\n    elif name == 'logsoftmax':\n        self.activation = nn.LogSoftmax(**params)\n    elif name == 'tanh':\n        self.activation = nn.Tanh()\n    elif name == 'argmax':\n        self.activation = ArgMax(**params)\n    elif name == 'argmax2d':\n        self.activation = ArgMax(dim=1, **params)\n    elif callable(name):\n        self.activation = name(**params)\n    else:\n        raise ValueError('Activation should be callable/sigmoid/softmax/logsoftmax/tanh/None; got {}'.format(name))\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/base/modules/#src.thaw_slump_segmentation.models.base.modules.Activation.forward","title":"<code>forward(x)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/base/modules.py</code> <pre><code>def forward(self, x):\n    return self.activation(x)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/base/modules/#src.thaw_slump_segmentation.models.base.modules.ArgMax","title":"<code>ArgMax</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/thaw_slump_segmentation/models/base/modules.py</code> <pre><code>class ArgMax(nn.Module):\n\n    def __init__(self, dim=None):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        return torch.argmax(x, dim=self.dim)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/base/modules/#src.thaw_slump_segmentation.models.base.modules.ArgMax.dim","title":"<code>dim = dim</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/base/modules/#src.thaw_slump_segmentation.models.base.modules.ArgMax.__init__","title":"<code>__init__(dim=None)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/base/modules.py</code> <pre><code>def __init__(self, dim=None):\n    super().__init__()\n    self.dim = dim\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/base/modules/#src.thaw_slump_segmentation.models.base.modules.ArgMax.forward","title":"<code>forward(x)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/base/modules.py</code> <pre><code>def forward(self, x):\n    return torch.argmax(x, dim=self.dim)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/base/modules/#src.thaw_slump_segmentation.models.base.modules.Attention","title":"<code>Attention</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/thaw_slump_segmentation/models/base/modules.py</code> <pre><code>class Attention(nn.Module):\n\n    def __init__(self, name, **params):\n        super().__init__()\n\n        if name is None:\n            self.attention = nn.Identity(**params)\n        elif name == 'scse':\n            self.attention = SCSEModule(**params)\n        else:\n            raise ValueError(\"Attention {} is not implemented\".format(name))\n\n    def forward(self, x):\n        return self.attention(x)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/base/modules/#src.thaw_slump_segmentation.models.base.modules.Attention.attention","title":"<code>attention = nn.Identity(**params)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/base/modules/#src.thaw_slump_segmentation.models.base.modules.Attention.__init__","title":"<code>__init__(name, **params)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/base/modules.py</code> <pre><code>def __init__(self, name, **params):\n    super().__init__()\n\n    if name is None:\n        self.attention = nn.Identity(**params)\n    elif name == 'scse':\n        self.attention = SCSEModule(**params)\n    else:\n        raise ValueError(\"Attention {} is not implemented\".format(name))\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/base/modules/#src.thaw_slump_segmentation.models.base.modules.Attention.forward","title":"<code>forward(x)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/base/modules.py</code> <pre><code>def forward(self, x):\n    return self.attention(x)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/base/modules/#src.thaw_slump_segmentation.models.base.modules.Conv2dReLU","title":"<code>Conv2dReLU</code>","text":"<p>               Bases: <code>Sequential</code></p> Source code in <code>src/thaw_slump_segmentation/models/base/modules.py</code> <pre><code>class Conv2dReLU(nn.Sequential):\n    def __init__(\n            self,\n            in_channels,\n            out_channels,\n            kernel_size,\n            padding=0,\n            stride=1,\n            use_batchnorm=True,\n    ):\n\n        if use_batchnorm == \"inplace\" and InPlaceABN is None:\n            raise RuntimeError(\n                \"In order to use `use_batchnorm='inplace'` inplace_abn package must be installed. \"\n                + \"To install see: https://github.com/mapillary/inplace_abn\"\n            )\n\n        conv = nn.Conv2d(\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride=stride,\n            padding=padding,\n            bias=not (use_batchnorm),\n        )\n        relu = nn.ReLU(inplace=True)\n\n        if use_batchnorm == \"inplace\":\n            bn = InPlaceABN(out_channels, activation=\"leaky_relu\", activation_param=0.0)\n            relu = nn.Identity()\n\n        elif use_batchnorm and use_batchnorm != \"inplace\":\n            bn = nn.BatchNorm2d(out_channels)\n\n        else:\n            bn = nn.Identity()\n\n        super(Conv2dReLU, self).__init__(conv, bn, relu)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/base/modules/#src.thaw_slump_segmentation.models.base.modules.Conv2dReLU.__init__","title":"<code>__init__(in_channels, out_channels, kernel_size, padding=0, stride=1, use_batchnorm=True)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/base/modules.py</code> <pre><code>def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        padding=0,\n        stride=1,\n        use_batchnorm=True,\n):\n\n    if use_batchnorm == \"inplace\" and InPlaceABN is None:\n        raise RuntimeError(\n            \"In order to use `use_batchnorm='inplace'` inplace_abn package must be installed. \"\n            + \"To install see: https://github.com/mapillary/inplace_abn\"\n        )\n\n    conv = nn.Conv2d(\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride=stride,\n        padding=padding,\n        bias=not (use_batchnorm),\n    )\n    relu = nn.ReLU(inplace=True)\n\n    if use_batchnorm == \"inplace\":\n        bn = InPlaceABN(out_channels, activation=\"leaky_relu\", activation_param=0.0)\n        relu = nn.Identity()\n\n    elif use_batchnorm and use_batchnorm != \"inplace\":\n        bn = nn.BatchNorm2d(out_channels)\n\n    else:\n        bn = nn.Identity()\n\n    super(Conv2dReLU, self).__init__(conv, bn, relu)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/base/modules/#src.thaw_slump_segmentation.models.base.modules.Flatten","title":"<code>Flatten</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/thaw_slump_segmentation/models/base/modules.py</code> <pre><code>class Flatten(nn.Module):\n    def forward(self, x):\n        return x.view(x.shape[0], -1)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/base/modules/#src.thaw_slump_segmentation.models.base.modules.Flatten.forward","title":"<code>forward(x)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/base/modules.py</code> <pre><code>def forward(self, x):\n    return x.view(x.shape[0], -1)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/base/modules/#src.thaw_slump_segmentation.models.base.modules.SCSEModule","title":"<code>SCSEModule</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/thaw_slump_segmentation/models/base/modules.py</code> <pre><code>class SCSEModule(nn.Module):\n    def __init__(self, in_channels, reduction=16):\n        super().__init__()\n        self.cSE = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(in_channels, in_channels // reduction, 1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels // reduction, in_channels, 1),\n            nn.Sigmoid(),\n        )\n        self.sSE = nn.Sequential(nn.Conv2d(in_channels, 1, 1), nn.Sigmoid())\n\n    def forward(self, x):\n        return x * self.cSE(x) + x * self.sSE(x)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/base/modules/#src.thaw_slump_segmentation.models.base.modules.SCSEModule.cSE","title":"<code>cSE = nn.Sequential(nn.AdaptiveAvgPool2d(1), nn.Conv2d(in_channels, in_channels // reduction, 1), nn.ReLU(inplace=True), nn.Conv2d(in_channels // reduction, in_channels, 1), nn.Sigmoid())</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/base/modules/#src.thaw_slump_segmentation.models.base.modules.SCSEModule.sSE","title":"<code>sSE = nn.Sequential(nn.Conv2d(in_channels, 1, 1), nn.Sigmoid())</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/base/modules/#src.thaw_slump_segmentation.models.base.modules.SCSEModule.__init__","title":"<code>__init__(in_channels, reduction=16)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/base/modules.py</code> <pre><code>def __init__(self, in_channels, reduction=16):\n    super().__init__()\n    self.cSE = nn.Sequential(\n        nn.AdaptiveAvgPool2d(1),\n        nn.Conv2d(in_channels, in_channels // reduction, 1),\n        nn.ReLU(inplace=True),\n        nn.Conv2d(in_channels // reduction, in_channels, 1),\n        nn.Sigmoid(),\n    )\n    self.sSE = nn.Sequential(nn.Conv2d(in_channels, 1, 1), nn.Sigmoid())\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/base/modules/#src.thaw_slump_segmentation.models.base.modules.SCSEModule.forward","title":"<code>forward(x)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/base/modules.py</code> <pre><code>def forward(self, x):\n    return x * self.cSE(x) + x * self.sSE(x)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/deeplabv3/decoder/","title":"Decoder","text":"<p>BSD 3-Clause License</p> <p>Copyright (c) Soumith Chintala 2016, All rights reserved.</p> <p>Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:</p> <ul> <li> <p>Redistributions of source code must retain the above copyright notice, this   list of conditions and the following disclaimer.</p> </li> <li> <p>Redistributions in binary form must reproduce the above copyright notice,   this list of conditions and the following disclaimer in the documentation   and/or other materials provided with the distribution.</p> </li> <li> <p>Neither the name of the copyright holder nor the names of its   contributors may be used to endorse or promote products derived from   this software without specific prior written permission.</p> </li> </ul> <p>THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</p>"},{"location":"reference/src/thaw_slump_segmentation/models/deeplabv3/decoder/#src.thaw_slump_segmentation.models.deeplabv3.decoder.__all__","title":"<code>__all__ = ['DeepLabV3Decoder']</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/deeplabv3/decoder/#src.thaw_slump_segmentation.models.deeplabv3.decoder.ASPP","title":"<code>ASPP</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/thaw_slump_segmentation/models/deeplabv3/decoder.py</code> <pre><code>class ASPP(nn.Module):\n    def __init__(self, in_channels, out_channels, atrous_rates, separable=False):\n        super(ASPP, self).__init__()\n        modules = []\n        modules.append(\n            nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, 1, bias=False),\n                nn.BatchNorm2d(out_channels),\n                nn.ReLU(),\n            )\n        )\n\n        rate1, rate2, rate3 = tuple(atrous_rates)\n        ASPPConvModule = ASPPConv if not separable else ASPPSeparableConv\n\n        modules.append(ASPPConvModule(in_channels, out_channels, rate1))\n        modules.append(ASPPConvModule(in_channels, out_channels, rate2))\n        modules.append(ASPPConvModule(in_channels, out_channels, rate3))\n        modules.append(ASPPPooling(in_channels, out_channels))\n\n        self.convs = nn.ModuleList(modules)\n\n        self.project = nn.Sequential(\n            nn.Conv2d(5 * out_channels, out_channels, kernel_size=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n        )\n\n    def forward(self, x):\n        res = []\n        for conv in self.convs:\n            res.append(conv(x))\n        res = torch.cat(res, dim=1)\n        return self.project(res)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/deeplabv3/decoder/#src.thaw_slump_segmentation.models.deeplabv3.decoder.ASPP.convs","title":"<code>convs = nn.ModuleList(modules)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/deeplabv3/decoder/#src.thaw_slump_segmentation.models.deeplabv3.decoder.ASPP.project","title":"<code>project = nn.Sequential(nn.Conv2d(5 * out_channels, out_channels, kernel_size=1, bias=False), nn.BatchNorm2d(out_channels), nn.ReLU(), nn.Dropout(0.5))</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/deeplabv3/decoder/#src.thaw_slump_segmentation.models.deeplabv3.decoder.ASPP.__init__","title":"<code>__init__(in_channels, out_channels, atrous_rates, separable=False)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/deeplabv3/decoder.py</code> <pre><code>def __init__(self, in_channels, out_channels, atrous_rates, separable=False):\n    super(ASPP, self).__init__()\n    modules = []\n    modules.append(\n        nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n        )\n    )\n\n    rate1, rate2, rate3 = tuple(atrous_rates)\n    ASPPConvModule = ASPPConv if not separable else ASPPSeparableConv\n\n    modules.append(ASPPConvModule(in_channels, out_channels, rate1))\n    modules.append(ASPPConvModule(in_channels, out_channels, rate2))\n    modules.append(ASPPConvModule(in_channels, out_channels, rate3))\n    modules.append(ASPPPooling(in_channels, out_channels))\n\n    self.convs = nn.ModuleList(modules)\n\n    self.project = nn.Sequential(\n        nn.Conv2d(5 * out_channels, out_channels, kernel_size=1, bias=False),\n        nn.BatchNorm2d(out_channels),\n        nn.ReLU(),\n        nn.Dropout(0.5),\n    )\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/deeplabv3/decoder/#src.thaw_slump_segmentation.models.deeplabv3.decoder.ASPP.forward","title":"<code>forward(x)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/deeplabv3/decoder.py</code> <pre><code>def forward(self, x):\n    res = []\n    for conv in self.convs:\n        res.append(conv(x))\n    res = torch.cat(res, dim=1)\n    return self.project(res)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/deeplabv3/decoder/#src.thaw_slump_segmentation.models.deeplabv3.decoder.ASPPConv","title":"<code>ASPPConv</code>","text":"<p>               Bases: <code>Sequential</code></p> Source code in <code>src/thaw_slump_segmentation/models/deeplabv3/decoder.py</code> <pre><code>class ASPPConv(nn.Sequential):\n    def __init__(self, in_channels, out_channels, dilation):\n        super().__init__(\n            nn.Conv2d(\n                in_channels,\n                out_channels,\n                kernel_size=3,\n                padding=dilation,\n                dilation=dilation,\n                bias=False,\n            ),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n        )\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/deeplabv3/decoder/#src.thaw_slump_segmentation.models.deeplabv3.decoder.ASPPConv.__init__","title":"<code>__init__(in_channels, out_channels, dilation)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/deeplabv3/decoder.py</code> <pre><code>def __init__(self, in_channels, out_channels, dilation):\n    super().__init__(\n        nn.Conv2d(\n            in_channels,\n            out_channels,\n            kernel_size=3,\n            padding=dilation,\n            dilation=dilation,\n            bias=False,\n        ),\n        nn.BatchNorm2d(out_channels),\n        nn.ReLU(),\n    )\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/deeplabv3/decoder/#src.thaw_slump_segmentation.models.deeplabv3.decoder.ASPPPooling","title":"<code>ASPPPooling</code>","text":"<p>               Bases: <code>Sequential</code></p> Source code in <code>src/thaw_slump_segmentation/models/deeplabv3/decoder.py</code> <pre><code>class ASPPPooling(nn.Sequential):\n    def __init__(self, in_channels, out_channels):\n        super().__init__(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n        )\n\n    def forward(self, x):\n        size = x.shape[-2:]\n        for mod in self:\n            x = mod(x)\n        return F.interpolate(x, size=size, mode='bilinear', align_corners=False)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/deeplabv3/decoder/#src.thaw_slump_segmentation.models.deeplabv3.decoder.ASPPPooling.__init__","title":"<code>__init__(in_channels, out_channels)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/deeplabv3/decoder.py</code> <pre><code>def __init__(self, in_channels, out_channels):\n    super().__init__(\n        nn.AdaptiveAvgPool2d(1),\n        nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n        nn.BatchNorm2d(out_channels),\n        nn.ReLU(),\n    )\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/deeplabv3/decoder/#src.thaw_slump_segmentation.models.deeplabv3.decoder.ASPPPooling.forward","title":"<code>forward(x)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/deeplabv3/decoder.py</code> <pre><code>def forward(self, x):\n    size = x.shape[-2:]\n    for mod in self:\n        x = mod(x)\n    return F.interpolate(x, size=size, mode='bilinear', align_corners=False)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/deeplabv3/decoder/#src.thaw_slump_segmentation.models.deeplabv3.decoder.ASPPSeparableConv","title":"<code>ASPPSeparableConv</code>","text":"<p>               Bases: <code>Sequential</code></p> Source code in <code>src/thaw_slump_segmentation/models/deeplabv3/decoder.py</code> <pre><code>class ASPPSeparableConv(nn.Sequential):\n    def __init__(self, in_channels, out_channels, dilation):\n        super().__init__(\n            SeparableConv2d(\n                in_channels,\n                out_channels,\n                kernel_size=3,\n                padding=dilation,\n                dilation=dilation,\n                bias=False,\n            ),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n        )\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/deeplabv3/decoder/#src.thaw_slump_segmentation.models.deeplabv3.decoder.ASPPSeparableConv.__init__","title":"<code>__init__(in_channels, out_channels, dilation)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/deeplabv3/decoder.py</code> <pre><code>def __init__(self, in_channels, out_channels, dilation):\n    super().__init__(\n        SeparableConv2d(\n            in_channels,\n            out_channels,\n            kernel_size=3,\n            padding=dilation,\n            dilation=dilation,\n            bias=False,\n        ),\n        nn.BatchNorm2d(out_channels),\n        nn.ReLU(),\n    )\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/deeplabv3/decoder/#src.thaw_slump_segmentation.models.deeplabv3.decoder.DeepLabV3Decoder","title":"<code>DeepLabV3Decoder</code>","text":"<p>               Bases: <code>Sequential</code></p> Source code in <code>src/thaw_slump_segmentation/models/deeplabv3/decoder.py</code> <pre><code>class DeepLabV3Decoder(nn.Sequential):\n    def __init__(self, in_channels, out_channels=256, atrous_rates=(12, 24, 36)):\n        super().__init__(\n            ASPP(in_channels, out_channels, atrous_rates),\n            nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n        )\n        self.out_channels = out_channels\n\n    def forward(self, *features):\n        return super().forward(features[-1])\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/deeplabv3/decoder/#src.thaw_slump_segmentation.models.deeplabv3.decoder.DeepLabV3Decoder.out_channels","title":"<code>out_channels = out_channels</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/deeplabv3/decoder/#src.thaw_slump_segmentation.models.deeplabv3.decoder.DeepLabV3Decoder.__init__","title":"<code>__init__(in_channels, out_channels=256, atrous_rates=(12, 24, 36))</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/deeplabv3/decoder.py</code> <pre><code>def __init__(self, in_channels, out_channels=256, atrous_rates=(12, 24, 36)):\n    super().__init__(\n        ASPP(in_channels, out_channels, atrous_rates),\n        nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False),\n        nn.BatchNorm2d(out_channels),\n        nn.ReLU(),\n    )\n    self.out_channels = out_channels\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/deeplabv3/decoder/#src.thaw_slump_segmentation.models.deeplabv3.decoder.DeepLabV3Decoder.forward","title":"<code>forward(*features)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/deeplabv3/decoder.py</code> <pre><code>def forward(self, *features):\n    return super().forward(features[-1])\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/deeplabv3/decoder/#src.thaw_slump_segmentation.models.deeplabv3.decoder.DeepLabV3PlusDecoder","title":"<code>DeepLabV3PlusDecoder</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/thaw_slump_segmentation/models/deeplabv3/decoder.py</code> <pre><code>class DeepLabV3PlusDecoder(nn.Module):\n    def __init__(\n        self,\n        encoder_channels,\n        out_channels=256,\n        atrous_rates=(12, 24, 36),\n        output_stride=16,\n    ):\n        super().__init__()\n        if output_stride not in {8, 16}:\n            raise ValueError(\"Output stride should be 8 or 16, got {}.\".format(output_stride))\n\n        self.out_channels = out_channels\n        self.output_stride = output_stride\n\n        self.aspp = nn.Sequential(\n            ASPP(encoder_channels[-1], out_channels, atrous_rates, separable=True),\n            SeparableConv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n        )\n\n        scale_factor = 2 if output_stride == 8 else 4\n        self.up = nn.UpsamplingBilinear2d(scale_factor=scale_factor)\n\n        highres_in_channels = encoder_channels[-4]\n        highres_out_channels = 48   # proposed by authors of paper\n        self.block1 = nn.Sequential(\n            nn.Conv2d(highres_in_channels, highres_out_channels, kernel_size=1, bias=False),\n            nn.BatchNorm2d(highres_out_channels),\n            nn.ReLU(),\n        )\n        self.block2 = nn.Sequential(\n            SeparableConv2d(\n                highres_out_channels + out_channels,\n                out_channels,\n                kernel_size=3,\n                padding=1,\n                bias=False,\n            ),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n        )\n\n    def forward(self, *features):\n        aspp_features = self.aspp(features[-1])\n        aspp_features = self.up(aspp_features)\n        high_res_features = self.block1(features[-4])\n        concat_features = torch.cat([aspp_features, high_res_features], dim=1)\n        fused_features = self.block2(concat_features)\n        return fused_features\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/deeplabv3/decoder/#src.thaw_slump_segmentation.models.deeplabv3.decoder.DeepLabV3PlusDecoder.aspp","title":"<code>aspp = nn.Sequential(ASPP(encoder_channels[-1], out_channels, atrous_rates, separable=True), SeparableConv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False), nn.BatchNorm2d(out_channels), nn.ReLU())</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/deeplabv3/decoder/#src.thaw_slump_segmentation.models.deeplabv3.decoder.DeepLabV3PlusDecoder.block1","title":"<code>block1 = nn.Sequential(nn.Conv2d(highres_in_channels, highres_out_channels, kernel_size=1, bias=False), nn.BatchNorm2d(highres_out_channels), nn.ReLU())</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/deeplabv3/decoder/#src.thaw_slump_segmentation.models.deeplabv3.decoder.DeepLabV3PlusDecoder.block2","title":"<code>block2 = nn.Sequential(SeparableConv2d(highres_out_channels + out_channels, out_channels, kernel_size=3, padding=1, bias=False), nn.BatchNorm2d(out_channels), nn.ReLU())</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/deeplabv3/decoder/#src.thaw_slump_segmentation.models.deeplabv3.decoder.DeepLabV3PlusDecoder.out_channels","title":"<code>out_channels = out_channels</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/deeplabv3/decoder/#src.thaw_slump_segmentation.models.deeplabv3.decoder.DeepLabV3PlusDecoder.output_stride","title":"<code>output_stride = output_stride</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/deeplabv3/decoder/#src.thaw_slump_segmentation.models.deeplabv3.decoder.DeepLabV3PlusDecoder.up","title":"<code>up = nn.UpsamplingBilinear2d(scale_factor=scale_factor)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/deeplabv3/decoder/#src.thaw_slump_segmentation.models.deeplabv3.decoder.DeepLabV3PlusDecoder.__init__","title":"<code>__init__(encoder_channels, out_channels=256, atrous_rates=(12, 24, 36), output_stride=16)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/deeplabv3/decoder.py</code> <pre><code>def __init__(\n    self,\n    encoder_channels,\n    out_channels=256,\n    atrous_rates=(12, 24, 36),\n    output_stride=16,\n):\n    super().__init__()\n    if output_stride not in {8, 16}:\n        raise ValueError(\"Output stride should be 8 or 16, got {}.\".format(output_stride))\n\n    self.out_channels = out_channels\n    self.output_stride = output_stride\n\n    self.aspp = nn.Sequential(\n        ASPP(encoder_channels[-1], out_channels, atrous_rates, separable=True),\n        SeparableConv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n        nn.BatchNorm2d(out_channels),\n        nn.ReLU(),\n    )\n\n    scale_factor = 2 if output_stride == 8 else 4\n    self.up = nn.UpsamplingBilinear2d(scale_factor=scale_factor)\n\n    highres_in_channels = encoder_channels[-4]\n    highres_out_channels = 48   # proposed by authors of paper\n    self.block1 = nn.Sequential(\n        nn.Conv2d(highres_in_channels, highres_out_channels, kernel_size=1, bias=False),\n        nn.BatchNorm2d(highres_out_channels),\n        nn.ReLU(),\n    )\n    self.block2 = nn.Sequential(\n        SeparableConv2d(\n            highres_out_channels + out_channels,\n            out_channels,\n            kernel_size=3,\n            padding=1,\n            bias=False,\n        ),\n        nn.BatchNorm2d(out_channels),\n        nn.ReLU(),\n    )\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/deeplabv3/decoder/#src.thaw_slump_segmentation.models.deeplabv3.decoder.DeepLabV3PlusDecoder.forward","title":"<code>forward(*features)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/deeplabv3/decoder.py</code> <pre><code>def forward(self, *features):\n    aspp_features = self.aspp(features[-1])\n    aspp_features = self.up(aspp_features)\n    high_res_features = self.block1(features[-4])\n    concat_features = torch.cat([aspp_features, high_res_features], dim=1)\n    fused_features = self.block2(concat_features)\n    return fused_features\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/deeplabv3/decoder/#src.thaw_slump_segmentation.models.deeplabv3.decoder.SeparableConv2d","title":"<code>SeparableConv2d</code>","text":"<p>               Bases: <code>Sequential</code></p> Source code in <code>src/thaw_slump_segmentation/models/deeplabv3/decoder.py</code> <pre><code>class SeparableConv2d(nn.Sequential):\n\n    def __init__(\n            self,\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride=1,\n            padding=0,\n            dilation=1,\n            bias=True,\n    ):\n        dephtwise_conv = nn.Conv2d(\n            in_channels,\n            in_channels,\n            kernel_size,\n            stride=stride,\n            padding=padding,\n            dilation=dilation,\n            groups=in_channels,\n            bias=False,\n        )\n        pointwise_conv = nn.Conv2d(\n            in_channels,\n            out_channels,\n            kernel_size=1,\n            bias=bias,\n        )\n        super().__init__(dephtwise_conv, pointwise_conv)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/deeplabv3/decoder/#src.thaw_slump_segmentation.models.deeplabv3.decoder.SeparableConv2d.__init__","title":"<code>__init__(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, bias=True)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/deeplabv3/decoder.py</code> <pre><code>def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride=1,\n        padding=0,\n        dilation=1,\n        bias=True,\n):\n    dephtwise_conv = nn.Conv2d(\n        in_channels,\n        in_channels,\n        kernel_size,\n        stride=stride,\n        padding=padding,\n        dilation=dilation,\n        groups=in_channels,\n        bias=False,\n    )\n    pointwise_conv = nn.Conv2d(\n        in_channels,\n        out_channels,\n        kernel_size=1,\n        bias=bias,\n    )\n    super().__init__(dephtwise_conv, pointwise_conv)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/deeplabv3/model/","title":"Model","text":""},{"location":"reference/src/thaw_slump_segmentation/models/deeplabv3/model/#src.thaw_slump_segmentation.models.deeplabv3.model.DeepLabV3","title":"<code>DeepLabV3</code>","text":"<p>               Bases: <code>SegmentationModel</code></p> <p>DeepLabV3_ implementation from \"Rethinking Atrous Convolution for Semantic Image Segmentation\"</p> <p>Parameters:</p> Name Type Description Default <code>encoder_name</code> <code>str</code> <p>Name of the classification model that will be used as an encoder (a.k.a backbone) to extract features of different spatial resolution</p> <code>'resnet34'</code> <code>encoder_depth</code> <code>int</code> <p>A number of stages used in encoder in range [3, 5]. Each stage generate features  two times smaller in spatial dimensions than previous one (e.g. for depth 0 we will have features with shapes [(N, C, H, W),], for depth 1 - [(N, C, H, W), (N, C, H // 2, W // 2)] and so on). Default is 5</p> <code>5</code> <code>encoder_weights</code> <code>Optional[str]</code> <p>One of None (random initialization), \"imagenet\" (pre-training on ImageNet) and  other pretrained weights (see table with available weights for each encoder_name)</p> <code>'imagenet'</code> <code>decoder_channels</code> <code>int</code> <p>A number of convolution filters in ASPP module. Default is 256</p> <code>256</code> <code>in_channels</code> <code>int</code> <p>A number of input channels for the model, default is 3 (RGB images)</p> <code>3</code> <code>classes</code> <code>int</code> <p>A number of classes for output mask (or you can think as a number of channels of output mask)</p> <code>1</code> <code>activation</code> <code>Optional[str]</code> <p>An activation function to apply after the final convolution layer. Available options are \"sigmoid\", \"softmax\", \"logsoftmax\", \"tanh\", \"identity\", callable and None. Default is None</p> <code>None</code> <code>upsampling</code> <code>int</code> <p>Final upsampling factor. Default is 8 to preserve input-output spatial shape identity</p> <code>8</code> <code>aux_params</code> <code>Optional[dict]</code> <p>Dictionary with parameters of the auxiliary output (classification head). Auxiliary output is build  on top of encoder if aux_params is not None (default). Supported params:     - classes (int): A number of classes     - pooling (str): One of \"max\", \"avg\". Default is \"avg\"     - dropout (float): Dropout factor in [0, 1)     - activation (str): An activation function to apply \"sigmoid\"/\"softmax\" (could be None to return logits)</p> <code>None</code> <p>Returns:     <code>torch.nn.Module</code>: DeepLabV3</p> <p>.. _DeeplabV3:     https://arxiv.org/abs/1706.05587</p> Source code in <code>src/thaw_slump_segmentation/models/deeplabv3/model.py</code> <pre><code>class DeepLabV3(SegmentationModel):\n    \"\"\"DeepLabV3_ implementation from \"Rethinking Atrous Convolution for Semantic Image Segmentation\"\n\n    Args:\n        encoder_name: Name of the classification model that will be used as an encoder (a.k.a backbone)\n            to extract features of different spatial resolution\n        encoder_depth: A number of stages used in encoder in range [3, 5]. Each stage generate features \n            two times smaller in spatial dimensions than previous one (e.g. for depth 0 we will have features\n            with shapes [(N, C, H, W),], for depth 1 - [(N, C, H, W), (N, C, H // 2, W // 2)] and so on).\n            Default is 5\n        encoder_weights: One of **None** (random initialization), **\"imagenet\"** (pre-training on ImageNet) and \n            other pretrained weights (see table with available weights for each encoder_name)\n        decoder_channels: A number of convolution filters in ASPP module. Default is 256\n        in_channels: A number of input channels for the model, default is 3 (RGB images)\n        classes: A number of classes for output mask (or you can think as a number of channels of output mask)\n        activation: An activation function to apply after the final convolution layer.\n            Available options are **\"sigmoid\"**, **\"softmax\"**, **\"logsoftmax\"**, **\"tanh\"**, **\"identity\"**, **callable** and **None**.\n            Default is **None**\n        upsampling: Final upsampling factor. Default is 8 to preserve input-output spatial shape identity\n        aux_params: Dictionary with parameters of the auxiliary output (classification head). Auxiliary output is build \n            on top of encoder if **aux_params** is not **None** (default). Supported params:\n                - classes (int): A number of classes\n                - pooling (str): One of \"max\", \"avg\". Default is \"avg\"\n                - dropout (float): Dropout factor in [0, 1)\n                - activation (str): An activation function to apply \"sigmoid\"/\"softmax\" (could be **None** to return logits)\n    Returns:\n        ``torch.nn.Module``: **DeepLabV3**\n\n    .. _DeeplabV3:\n        https://arxiv.org/abs/1706.05587\n\n    \"\"\"\n\n    def __init__(\n            self,\n            encoder_name: str = \"resnet34\",\n            encoder_depth: int = 5,\n            encoder_weights: Optional[str] = \"imagenet\",\n            decoder_channels: int = 256,\n            in_channels: int = 3,\n            classes: int = 1,\n            activation: Optional[str] = None,\n            upsampling: int = 8,\n            aux_params: Optional[dict] = None,\n    ):\n        super().__init__()\n\n        self.encoder = get_encoder(\n            encoder_name,\n            in_channels=in_channels,\n            depth=encoder_depth,\n            weights=encoder_weights,\n        )\n        self.encoder.make_dilated(\n            stage_list=[4, 5],\n            dilation_list=[2, 4]\n        )\n\n        self.decoder = DeepLabV3Decoder(\n            in_channels=self.encoder.out_channels[-1],\n            out_channels=decoder_channels,\n        )\n\n        self.segmentation_head = SegmentationHead(\n            in_channels=self.decoder.out_channels,\n            out_channels=classes,\n            activation=activation,\n            kernel_size=1,\n            upsampling=upsampling,\n        )\n\n        if aux_params is not None:\n            self.classification_head = ClassificationHead(\n                in_channels=self.encoder.out_channels[-1], **aux_params\n            )\n        else:\n            self.classification_head = None\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/deeplabv3/model/#src.thaw_slump_segmentation.models.deeplabv3.model.DeepLabV3.classification_head","title":"<code>classification_head = ClassificationHead(in_channels=self.encoder.out_channels[-1], **aux_params)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/deeplabv3/model/#src.thaw_slump_segmentation.models.deeplabv3.model.DeepLabV3.decoder","title":"<code>decoder = DeepLabV3Decoder(in_channels=self.encoder.out_channels[-1], out_channels=decoder_channels)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/deeplabv3/model/#src.thaw_slump_segmentation.models.deeplabv3.model.DeepLabV3.encoder","title":"<code>encoder = get_encoder(encoder_name, in_channels=in_channels, depth=encoder_depth, weights=encoder_weights)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/deeplabv3/model/#src.thaw_slump_segmentation.models.deeplabv3.model.DeepLabV3.segmentation_head","title":"<code>segmentation_head = SegmentationHead(in_channels=self.decoder.out_channels, out_channels=classes, activation=activation, kernel_size=1, upsampling=upsampling)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/deeplabv3/model/#src.thaw_slump_segmentation.models.deeplabv3.model.DeepLabV3.__init__","title":"<code>__init__(encoder_name='resnet34', encoder_depth=5, encoder_weights='imagenet', decoder_channels=256, in_channels=3, classes=1, activation=None, upsampling=8, aux_params=None)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/deeplabv3/model.py</code> <pre><code>def __init__(\n        self,\n        encoder_name: str = \"resnet34\",\n        encoder_depth: int = 5,\n        encoder_weights: Optional[str] = \"imagenet\",\n        decoder_channels: int = 256,\n        in_channels: int = 3,\n        classes: int = 1,\n        activation: Optional[str] = None,\n        upsampling: int = 8,\n        aux_params: Optional[dict] = None,\n):\n    super().__init__()\n\n    self.encoder = get_encoder(\n        encoder_name,\n        in_channels=in_channels,\n        depth=encoder_depth,\n        weights=encoder_weights,\n    )\n    self.encoder.make_dilated(\n        stage_list=[4, 5],\n        dilation_list=[2, 4]\n    )\n\n    self.decoder = DeepLabV3Decoder(\n        in_channels=self.encoder.out_channels[-1],\n        out_channels=decoder_channels,\n    )\n\n    self.segmentation_head = SegmentationHead(\n        in_channels=self.decoder.out_channels,\n        out_channels=classes,\n        activation=activation,\n        kernel_size=1,\n        upsampling=upsampling,\n    )\n\n    if aux_params is not None:\n        self.classification_head = ClassificationHead(\n            in_channels=self.encoder.out_channels[-1], **aux_params\n        )\n    else:\n        self.classification_head = None\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/deeplabv3/model/#src.thaw_slump_segmentation.models.deeplabv3.model.DeepLabV3Plus","title":"<code>DeepLabV3Plus</code>","text":"<p>               Bases: <code>SegmentationModel</code></p> <p>DeepLabV3+ implementation from \"Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation\"</p> <p>Parameters:</p> Name Type Description Default <code>encoder_name</code> <code>str</code> <p>Name of the classification model that will be used as an encoder (a.k.a backbone) to extract features of different spatial resolution</p> <code>'resnet34'</code> <code>encoder_depth</code> <code>int</code> <p>A number of stages used in encoder in range [3, 5]. Each stage generate features  two times smaller in spatial dimensions than previous one (e.g. for depth 0 we will have features with shapes [(N, C, H, W),], for depth 1 - [(N, C, H, W), (N, C, H // 2, W // 2)] and so on). Default is 5</p> <code>5</code> <code>encoder_weights</code> <code>Optional[str]</code> <p>One of None (random initialization), \"imagenet\" (pre-training on ImageNet) and  other pretrained weights (see table with available weights for each encoder_name)</p> <code>'imagenet'</code> <code>encoder_output_stride</code> <code>int</code> <p>Downsampling factor for last encoder features (see original paper for explanation)</p> <code>16</code> <code>decoder_atrous_rates</code> <code>tuple</code> <p>Dilation rates for ASPP module (should be a tuple of 3 integer values)</p> <code>(12, 24, 36)</code> <code>decoder_channels</code> <code>int</code> <p>A number of convolution filters in ASPP module. Default is 256</p> <code>256</code> <code>in_channels</code> <code>int</code> <p>A number of input channels for the model, default is 3 (RGB images)</p> <code>3</code> <code>classes</code> <code>int</code> <p>A number of classes for output mask (or you can think as a number of channels of output mask)</p> <code>1</code> <code>activation</code> <code>Optional[str]</code> <p>An activation function to apply after the final convolution layer. Available options are \"sigmoid\", \"softmax\", \"logsoftmax\", \"tanh\", \"identity\", callable and None. Default is None</p> <code>None</code> <code>upsampling</code> <code>int</code> <p>Final upsampling factor. Default is 4 to preserve input-output spatial shape identity</p> <code>4</code> <code>aux_params</code> <code>Optional[dict]</code> <p>Dictionary with parameters of the auxiliary output (classification head). Auxiliary output is build  on top of encoder if aux_params is not None (default). Supported params:     - classes (int): A number of classes     - pooling (str): One of \"max\", \"avg\". Default is \"avg\"     - dropout (float): Dropout factor in [0, 1)     - activation (str): An activation function to apply \"sigmoid\"/\"softmax\" (could be None to return logits)</p> <code>None</code> <p>Returns:     <code>torch.nn.Module</code>: DeepLabV3Plus</p> Reference <p>https://arxiv.org/abs/1802.02611v3</p> Source code in <code>src/thaw_slump_segmentation/models/deeplabv3/model.py</code> <pre><code>class DeepLabV3Plus(SegmentationModel):\n    \"\"\"DeepLabV3+ implementation from \"Encoder-Decoder with Atrous Separable\n    Convolution for Semantic Image Segmentation\"\n\n    Args:\n        encoder_name: Name of the classification model that will be used as an encoder (a.k.a backbone)\n            to extract features of different spatial resolution\n        encoder_depth: A number of stages used in encoder in range [3, 5]. Each stage generate features \n            two times smaller in spatial dimensions than previous one (e.g. for depth 0 we will have features\n            with shapes [(N, C, H, W),], for depth 1 - [(N, C, H, W), (N, C, H // 2, W // 2)] and so on).\n            Default is 5\n        encoder_weights: One of **None** (random initialization), **\"imagenet\"** (pre-training on ImageNet) and \n            other pretrained weights (see table with available weights for each encoder_name)\n        encoder_output_stride: Downsampling factor for last encoder features (see original paper for explanation)\n        decoder_atrous_rates: Dilation rates for ASPP module (should be a tuple of 3 integer values)\n        decoder_channels: A number of convolution filters in ASPP module. Default is 256\n        in_channels: A number of input channels for the model, default is 3 (RGB images)\n        classes: A number of classes for output mask (or you can think as a number of channels of output mask)\n        activation: An activation function to apply after the final convolution layer.\n            Available options are **\"sigmoid\"**, **\"softmax\"**, **\"logsoftmax\"**, **\"tanh\"**, **\"identity\"**, **callable** and **None**.\n            Default is **None**\n        upsampling: Final upsampling factor. Default is 4 to preserve input-output spatial shape identity\n        aux_params: Dictionary with parameters of the auxiliary output (classification head). Auxiliary output is build \n            on top of encoder if **aux_params** is not **None** (default). Supported params:\n                - classes (int): A number of classes\n                - pooling (str): One of \"max\", \"avg\". Default is \"avg\"\n                - dropout (float): Dropout factor in [0, 1)\n                - activation (str): An activation function to apply \"sigmoid\"/\"softmax\" (could be **None** to return logits)\n    Returns:\n        ``torch.nn.Module``: **DeepLabV3Plus**\n\n    Reference:\n        https://arxiv.org/abs/1802.02611v3\n\n    \"\"\"\n    def __init__(\n            self,\n            encoder_name: str = \"resnet34\",\n            encoder_depth: int = 5,\n            encoder_weights: Optional[str] = \"imagenet\",\n            encoder_output_stride: int = 16,\n            decoder_channels: int = 256,\n            decoder_atrous_rates: tuple = (12, 24, 36),\n            in_channels: int = 3,\n            classes: int = 1,\n            activation: Optional[str] = None,\n            upsampling: int = 4,\n            aux_params: Optional[dict] = None,\n    ):\n        super().__init__()\n\n        self.encoder = get_encoder(\n            encoder_name,\n            in_channels=in_channels,\n            depth=encoder_depth,\n            weights=encoder_weights,\n        )\n\n        if encoder_output_stride == 8:\n            self.encoder.make_dilated(\n                stage_list=[4, 5],\n                dilation_list=[2, 4]\n            )\n\n        elif encoder_output_stride == 16:\n            self.encoder.make_dilated(\n                stage_list=[5],\n                dilation_list=[2]\n            )\n        else:\n            raise ValueError(\n                \"Encoder output stride should be 8 or 16, got {}\".format(encoder_output_stride)\n            )\n\n        self.decoder = DeepLabV3PlusDecoder(\n            encoder_channels=self.encoder.out_channels,\n            out_channels=decoder_channels,\n            atrous_rates=decoder_atrous_rates,\n            output_stride=encoder_output_stride,\n        )\n\n        self.segmentation_head = SegmentationHead(\n            in_channels=self.decoder.out_channels,\n            out_channels=classes,\n            activation=activation,\n            kernel_size=1,\n            upsampling=upsampling,\n        )\n\n        if aux_params is not None:\n            self.classification_head = ClassificationHead(\n                in_channels=self.encoder.out_channels[-1], **aux_params\n            )\n        else:\n            self.classification_head = None\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/deeplabv3/model/#src.thaw_slump_segmentation.models.deeplabv3.model.DeepLabV3Plus.classification_head","title":"<code>classification_head = ClassificationHead(in_channels=self.encoder.out_channels[-1], **aux_params)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/deeplabv3/model/#src.thaw_slump_segmentation.models.deeplabv3.model.DeepLabV3Plus.decoder","title":"<code>decoder = DeepLabV3PlusDecoder(encoder_channels=self.encoder.out_channels, out_channels=decoder_channels, atrous_rates=decoder_atrous_rates, output_stride=encoder_output_stride)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/deeplabv3/model/#src.thaw_slump_segmentation.models.deeplabv3.model.DeepLabV3Plus.encoder","title":"<code>encoder = get_encoder(encoder_name, in_channels=in_channels, depth=encoder_depth, weights=encoder_weights)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/deeplabv3/model/#src.thaw_slump_segmentation.models.deeplabv3.model.DeepLabV3Plus.segmentation_head","title":"<code>segmentation_head = SegmentationHead(in_channels=self.decoder.out_channels, out_channels=classes, activation=activation, kernel_size=1, upsampling=upsampling)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/deeplabv3/model/#src.thaw_slump_segmentation.models.deeplabv3.model.DeepLabV3Plus.__init__","title":"<code>__init__(encoder_name='resnet34', encoder_depth=5, encoder_weights='imagenet', encoder_output_stride=16, decoder_channels=256, decoder_atrous_rates=(12, 24, 36), in_channels=3, classes=1, activation=None, upsampling=4, aux_params=None)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/deeplabv3/model.py</code> <pre><code>def __init__(\n        self,\n        encoder_name: str = \"resnet34\",\n        encoder_depth: int = 5,\n        encoder_weights: Optional[str] = \"imagenet\",\n        encoder_output_stride: int = 16,\n        decoder_channels: int = 256,\n        decoder_atrous_rates: tuple = (12, 24, 36),\n        in_channels: int = 3,\n        classes: int = 1,\n        activation: Optional[str] = None,\n        upsampling: int = 4,\n        aux_params: Optional[dict] = None,\n):\n    super().__init__()\n\n    self.encoder = get_encoder(\n        encoder_name,\n        in_channels=in_channels,\n        depth=encoder_depth,\n        weights=encoder_weights,\n    )\n\n    if encoder_output_stride == 8:\n        self.encoder.make_dilated(\n            stage_list=[4, 5],\n            dilation_list=[2, 4]\n        )\n\n    elif encoder_output_stride == 16:\n        self.encoder.make_dilated(\n            stage_list=[5],\n            dilation_list=[2]\n        )\n    else:\n        raise ValueError(\n            \"Encoder output stride should be 8 or 16, got {}\".format(encoder_output_stride)\n        )\n\n    self.decoder = DeepLabV3PlusDecoder(\n        encoder_channels=self.encoder.out_channels,\n        out_channels=decoder_channels,\n        atrous_rates=decoder_atrous_rates,\n        output_stride=encoder_output_stride,\n    )\n\n    self.segmentation_head = SegmentationHead(\n        in_channels=self.decoder.out_channels,\n        out_channels=classes,\n        activation=activation,\n        kernel_size=1,\n        upsampling=upsampling,\n    )\n\n    if aux_params is not None:\n        self.classification_head = ClassificationHead(\n            in_channels=self.encoder.out_channels[-1], **aux_params\n        )\n    else:\n        self.classification_head = None\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/_base/","title":"base","text":""},{"location":"reference/src/thaw_slump_segmentation/models/encoders/_base/#src.thaw_slump_segmentation.models.encoders._base.EncoderMixin","title":"<code>EncoderMixin</code>","text":"<p>Add encoder functionality such as: - output channels specification of feature tensors (produced by encoder) - patching first convolution for arbitrary input channels</p> Source code in <code>src/thaw_slump_segmentation/models/encoders/_base.py</code> <pre><code>class EncoderMixin:\n    \"\"\"Add encoder functionality such as:\n        - output channels specification of feature tensors (produced by encoder)\n        - patching first convolution for arbitrary input channels\n    \"\"\"\n\n    @property\n    def out_channels(self):\n        \"\"\"Return channels dimensions for each tensor of forward output of encoder\"\"\"\n        return self._out_channels[: self._depth + 1]\n\n    def set_in_channels(self, in_channels):\n        \"\"\"Change first convolution channels\"\"\"\n        if in_channels == 3:\n            return\n\n        self._in_channels = in_channels\n        if self._out_channels[0] == 3:\n            self._out_channels = tuple([in_channels] + list(self._out_channels)[1:])\n\n        utils.patch_first_conv(model=self, in_channels=in_channels)\n\n    def get_stages(self):\n        \"\"\"Method should be overridden in encoder\"\"\"\n        raise NotImplementedError\n\n    def make_dilated(self, stage_list, dilation_list):\n        stages = self.get_stages()\n        for stage_indx, dilation_rate in zip(stage_list, dilation_list):\n            utils.replace_strides_with_dilation(\n                module=stages[stage_indx],\n                dilation_rate=dilation_rate,\n            )\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/_base/#src.thaw_slump_segmentation.models.encoders._base.EncoderMixin.out_channels","title":"<code>out_channels</code>  <code>property</code>","text":"<p>Return channels dimensions for each tensor of forward output of encoder</p>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/_base/#src.thaw_slump_segmentation.models.encoders._base.EncoderMixin.get_stages","title":"<code>get_stages()</code>","text":"<p>Method should be overridden in encoder</p> Source code in <code>src/thaw_slump_segmentation/models/encoders/_base.py</code> <pre><code>def get_stages(self):\n    \"\"\"Method should be overridden in encoder\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/_base/#src.thaw_slump_segmentation.models.encoders._base.EncoderMixin.make_dilated","title":"<code>make_dilated(stage_list, dilation_list)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/_base.py</code> <pre><code>def make_dilated(self, stage_list, dilation_list):\n    stages = self.get_stages()\n    for stage_indx, dilation_rate in zip(stage_list, dilation_list):\n        utils.replace_strides_with_dilation(\n            module=stages[stage_indx],\n            dilation_rate=dilation_rate,\n        )\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/_base/#src.thaw_slump_segmentation.models.encoders._base.EncoderMixin.set_in_channels","title":"<code>set_in_channels(in_channels)</code>","text":"<p>Change first convolution channels</p> Source code in <code>src/thaw_slump_segmentation/models/encoders/_base.py</code> <pre><code>def set_in_channels(self, in_channels):\n    \"\"\"Change first convolution channels\"\"\"\n    if in_channels == 3:\n        return\n\n    self._in_channels = in_channels\n    if self._out_channels[0] == 3:\n        self._out_channels = tuple([in_channels] + list(self._out_channels)[1:])\n\n    utils.patch_first_conv(model=self, in_channels=in_channels)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/_preprocessing/","title":"preprocessing","text":""},{"location":"reference/src/thaw_slump_segmentation/models/encoders/_preprocessing/#src.thaw_slump_segmentation.models.encoders._preprocessing.preprocess_input","title":"<code>preprocess_input(x, mean=None, std=None, input_space='RGB', input_range=None, **kwargs)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/_preprocessing.py</code> <pre><code>def preprocess_input(\n    x, mean=None, std=None, input_space=\"RGB\", input_range=None, **kwargs\n):\n\n    if input_space == \"BGR\":\n        x = x[..., ::-1].copy()\n\n    if input_range is not None:\n        if x.max() &gt; 1 and input_range[1] == 1:\n            x = x / 255.0\n\n    if mean is not None:\n        mean = np.array(mean)\n        x = x - mean\n\n    if std is not None:\n        std = np.array(std)\n        x = x / std\n\n    return x\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/_utils/","title":"utils","text":""},{"location":"reference/src/thaw_slump_segmentation/models/encoders/_utils/#src.thaw_slump_segmentation.models.encoders._utils.patch_first_conv","title":"<code>patch_first_conv(model, in_channels)</code>","text":"<p>Change first convolution layer input channels. In case:     in_channels == 1 or in_channels == 2 -&gt; reuse original weights     in_channels &gt; 3 -&gt; make random kaiming normal initialization</p> Source code in <code>src/thaw_slump_segmentation/models/encoders/_utils.py</code> <pre><code>def patch_first_conv(model, in_channels):\n    \"\"\"Change first convolution layer input channels.\n    In case:\n        in_channels == 1 or in_channels == 2 -&gt; reuse original weights\n        in_channels &gt; 3 -&gt; make random kaiming normal initialization\n    \"\"\"\n\n    # get first conv\n    for module in model.modules():\n        if isinstance(module, nn.Conv2d):\n            break\n\n    # change input channels for first conv\n    module.in_channels = in_channels\n    weight = module.weight.detach()\n    reset = False\n\n    if in_channels == 1:\n        weight = weight.sum(1, keepdim=True)\n    elif in_channels == 2:\n        weight = weight[:, :2] * (3.0 / 2.0)\n    else:\n        reset = True\n        weight = torch.Tensor(\n            module.out_channels,\n            module.in_channels // module.groups,\n            *module.kernel_size\n        )\n\n    module.weight = nn.parameter.Parameter(weight)\n    if reset:\n        module.reset_parameters()\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/_utils/#src.thaw_slump_segmentation.models.encoders._utils.replace_strides_with_dilation","title":"<code>replace_strides_with_dilation(module, dilation_rate)</code>","text":"<p>Patch Conv2d modules replacing strides with dilation</p> Source code in <code>src/thaw_slump_segmentation/models/encoders/_utils.py</code> <pre><code>def replace_strides_with_dilation(module, dilation_rate):\n    \"\"\"Patch Conv2d modules replacing strides with dilation\"\"\"\n    for mod in module.modules():\n        if isinstance(mod, nn.Conv2d):\n            mod.stride = (1, 1)\n            mod.dilation = (dilation_rate, dilation_rate)\n            kh, kw = mod.kernel_size\n            mod.padding = ((kh // 2) * dilation_rate, (kh // 2) * dilation_rate)\n\n            # Kostyl for EfficientNet\n            if hasattr(mod, \"static_padding\"):\n                mod.static_padding = nn.Identity()\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/densenet/","title":"Densenet","text":"<p>Each encoder should have following attributes and methods and be inherited from <code>_base.EncoderMixin</code></p> <p>Attributes:</p> <pre><code>_out_channels (list of int): specify number of channels for each encoder feature tensor\n_depth (int): specify number of stages in decoder (in other words number of downsampling operations)\n_in_channels (int): default number of input channels in first Conv2d layer for encoder (usually 3)\n</code></pre> <p>Methods:</p> <pre><code>forward(self, x: torch.Tensor)\n    produce list of features of different spatial resolutions, each feature is a 4D torch.tensor of\n    shape NCHW (features should be sorted in descending order according to spatial resolution, starting\n    with resolution same as input `x` tensor).\n\n    Input: `x` with shape (1, 3, 64, 64)\n    Output: [f0, f1, f2, f3, f4, f5] - features with corresponding shapes\n            [(1, 3, 64, 64), (1, 64, 32, 32), (1, 128, 16, 16), (1, 256, 8, 8),\n            (1, 512, 4, 4), (1, 1024, 2, 2)] (C - dim may differ)\n\n    also should support number of features according to specified depth, e.g. if depth = 5,\n    number of feature tensors = 6 (one with same resolution as input and 5 downsampled),\n    depth = 3 -&gt; number of feature tensors = 4 (one with same resolution as input and 3 downsampled).\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/densenet/#src.thaw_slump_segmentation.models.encoders.densenet.densenet_encoders","title":"<code>densenet_encoders = {'densenet121': {'encoder': DenseNetEncoder, 'pretrained_settings': pretrained_settings['densenet121'], 'params': {'out_channels': (3, 64, 256, 512, 1024, 1024), 'num_init_features': 64, 'growth_rate': 32, 'block_config': (6, 12, 24, 16)}}, 'densenet169': {'encoder': DenseNetEncoder, 'pretrained_settings': pretrained_settings['densenet169'], 'params': {'out_channels': (3, 64, 256, 512, 1280, 1664), 'num_init_features': 64, 'growth_rate': 32, 'block_config': (6, 12, 32, 32)}}, 'densenet201': {'encoder': DenseNetEncoder, 'pretrained_settings': pretrained_settings['densenet201'], 'params': {'out_channels': (3, 64, 256, 512, 1792, 1920), 'num_init_features': 64, 'growth_rate': 32, 'block_config': (6, 12, 48, 32)}}, 'densenet161': {'encoder': DenseNetEncoder, 'pretrained_settings': pretrained_settings['densenet161'], 'params': {'out_channels': (3, 96, 384, 768, 2112, 2208), 'num_init_features': 96, 'growth_rate': 48, 'block_config': (6, 12, 36, 24)}}}</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/encoders/densenet/#src.thaw_slump_segmentation.models.encoders.densenet.DenseNetEncoder","title":"<code>DenseNetEncoder</code>","text":"<p>               Bases: <code>DenseNet</code>, <code>EncoderMixin</code></p> Source code in <code>src/thaw_slump_segmentation/models/encoders/densenet.py</code> <pre><code>class DenseNetEncoder(DenseNet, EncoderMixin):\n    def __init__(self, out_channels, depth=5, **kwargs):\n        super().__init__(**kwargs)\n        self._out_channels = out_channels\n        self._depth = depth\n        self._in_channels = 3\n        del self.classifier\n\n    def make_dilated(self, stage_list, dilation_list):\n        raise ValueError(\"DenseNet encoders do not support dilated mode \"\n                         \"due to pooling operation for downsampling!\")\n\n    def get_stages(self):\n        return [\n            nn.Identity(),\n            nn.Sequential(self.features.conv0, self.features.norm0, self.features.relu0),\n            nn.Sequential(self.features.pool0, self.features.denseblock1,\n                          TransitionWithSkip(self.features.transition1)),\n            nn.Sequential(self.features.denseblock2, TransitionWithSkip(self.features.transition2)),\n            nn.Sequential(self.features.denseblock3, TransitionWithSkip(self.features.transition3)),\n            nn.Sequential(self.features.denseblock4, self.features.norm5)\n        ]\n\n    def forward(self, x):\n\n        stages = self.get_stages()\n\n        features = []\n        for i in range(self._depth + 1):\n            x = stages[i](x)\n            if isinstance(x, (list, tuple)):\n                x, skip = x\n                features.append(skip)\n            else:\n                features.append(x)\n\n        return features\n\n    def load_state_dict(self, state_dict):\n        pattern = re.compile(\n            r\"^(.*denselayer\\d+\\.(?:norm|relu|conv))\\.((?:[12])\\.(?:weight|bias|running_mean|running_var))$\"\n        )\n        for key in list(state_dict.keys()):\n            res = pattern.match(key)\n            if res:\n                new_key = res.group(1) + res.group(2)\n                state_dict[new_key] = state_dict[key]\n                del state_dict[key]\n\n        # remove linear\n        state_dict.pop(\"classifier.bias\")\n        state_dict.pop(\"classifier.weight\")\n\n        super().load_state_dict(state_dict)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/densenet/#src.thaw_slump_segmentation.models.encoders.densenet.DenseNetEncoder.__init__","title":"<code>__init__(out_channels, depth=5, **kwargs)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/densenet.py</code> <pre><code>def __init__(self, out_channels, depth=5, **kwargs):\n    super().__init__(**kwargs)\n    self._out_channels = out_channels\n    self._depth = depth\n    self._in_channels = 3\n    del self.classifier\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/densenet/#src.thaw_slump_segmentation.models.encoders.densenet.DenseNetEncoder.forward","title":"<code>forward(x)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/densenet.py</code> <pre><code>def forward(self, x):\n\n    stages = self.get_stages()\n\n    features = []\n    for i in range(self._depth + 1):\n        x = stages[i](x)\n        if isinstance(x, (list, tuple)):\n            x, skip = x\n            features.append(skip)\n        else:\n            features.append(x)\n\n    return features\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/densenet/#src.thaw_slump_segmentation.models.encoders.densenet.DenseNetEncoder.get_stages","title":"<code>get_stages()</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/densenet.py</code> <pre><code>def get_stages(self):\n    return [\n        nn.Identity(),\n        nn.Sequential(self.features.conv0, self.features.norm0, self.features.relu0),\n        nn.Sequential(self.features.pool0, self.features.denseblock1,\n                      TransitionWithSkip(self.features.transition1)),\n        nn.Sequential(self.features.denseblock2, TransitionWithSkip(self.features.transition2)),\n        nn.Sequential(self.features.denseblock3, TransitionWithSkip(self.features.transition3)),\n        nn.Sequential(self.features.denseblock4, self.features.norm5)\n    ]\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/densenet/#src.thaw_slump_segmentation.models.encoders.densenet.DenseNetEncoder.load_state_dict","title":"<code>load_state_dict(state_dict)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/densenet.py</code> <pre><code>def load_state_dict(self, state_dict):\n    pattern = re.compile(\n        r\"^(.*denselayer\\d+\\.(?:norm|relu|conv))\\.((?:[12])\\.(?:weight|bias|running_mean|running_var))$\"\n    )\n    for key in list(state_dict.keys()):\n        res = pattern.match(key)\n        if res:\n            new_key = res.group(1) + res.group(2)\n            state_dict[new_key] = state_dict[key]\n            del state_dict[key]\n\n    # remove linear\n    state_dict.pop(\"classifier.bias\")\n    state_dict.pop(\"classifier.weight\")\n\n    super().load_state_dict(state_dict)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/densenet/#src.thaw_slump_segmentation.models.encoders.densenet.DenseNetEncoder.make_dilated","title":"<code>make_dilated(stage_list, dilation_list)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/densenet.py</code> <pre><code>def make_dilated(self, stage_list, dilation_list):\n    raise ValueError(\"DenseNet encoders do not support dilated mode \"\n                     \"due to pooling operation for downsampling!\")\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/densenet/#src.thaw_slump_segmentation.models.encoders.densenet.TransitionWithSkip","title":"<code>TransitionWithSkip</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/thaw_slump_segmentation/models/encoders/densenet.py</code> <pre><code>class TransitionWithSkip(nn.Module):\n\n    def __init__(self, module):\n        super().__init__()\n        self.module = module\n\n    def forward(self, x):\n        for module in self.module:\n            x = module(x)\n            if isinstance(module, nn.ReLU):\n                skip = x\n        return x, skip\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/densenet/#src.thaw_slump_segmentation.models.encoders.densenet.TransitionWithSkip.module","title":"<code>module = module</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/encoders/densenet/#src.thaw_slump_segmentation.models.encoders.densenet.TransitionWithSkip.__init__","title":"<code>__init__(module)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/densenet.py</code> <pre><code>def __init__(self, module):\n    super().__init__()\n    self.module = module\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/densenet/#src.thaw_slump_segmentation.models.encoders.densenet.TransitionWithSkip.forward","title":"<code>forward(x)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/densenet.py</code> <pre><code>def forward(self, x):\n    for module in self.module:\n        x = module(x)\n        if isinstance(module, nn.ReLU):\n            skip = x\n    return x, skip\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/dpn/","title":"Dpn","text":"<p>Each encoder should have following attributes and methods and be inherited from <code>_base.EncoderMixin</code></p> <p>Attributes:</p> <pre><code>_out_channels (list of int): specify number of channels for each encoder feature tensor\n_depth (int): specify number of stages in decoder (in other words number of downsampling operations)\n_in_channels (int): default number of input channels in first Conv2d layer for encoder (usually 3)\n</code></pre> <p>Methods:</p> <pre><code>forward(self, x: torch.Tensor)\n    produce list of features of different spatial resolutions, each feature is a 4D torch.tensor of\n    shape NCHW (features should be sorted in descending order according to spatial resolution, starting\n    with resolution same as input `x` tensor).\n\n    Input: `x` with shape (1, 3, 64, 64)\n    Output: [f0, f1, f2, f3, f4, f5] - features with corresponding shapes\n            [(1, 3, 64, 64), (1, 64, 32, 32), (1, 128, 16, 16), (1, 256, 8, 8),\n            (1, 512, 4, 4), (1, 1024, 2, 2)] (C - dim may differ)\n\n    also should support number of features according to specified depth, e.g. if depth = 5,\n    number of feature tensors = 6 (one with same resolution as input and 5 downsampled),\n    depth = 3 -&gt; number of feature tensors = 4 (one with same resolution as input and 3 downsampled).\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/dpn/#src.thaw_slump_segmentation.models.encoders.dpn.dpn_encoders","title":"<code>dpn_encoders = {'dpn68': {'encoder': DPNEncoder, 'pretrained_settings': pretrained_settings['dpn68'], 'params': {'stage_idxs': (4, 8, 20, 24), 'out_channels': (3, 10, 144, 320, 704, 832), 'groups': 32, 'inc_sec': (16, 32, 32, 64), 'k_r': 128, 'k_sec': (3, 4, 12, 3), 'num_classes': 1000, 'num_init_features': 10, 'small': True, 'test_time_pool': True}}, 'dpn68b': {'encoder': DPNEncoder, 'pretrained_settings': pretrained_settings['dpn68b'], 'params': {'stage_idxs': (4, 8, 20, 24), 'out_channels': (3, 10, 144, 320, 704, 832), 'b': True, 'groups': 32, 'inc_sec': (16, 32, 32, 64), 'k_r': 128, 'k_sec': (3, 4, 12, 3), 'num_classes': 1000, 'num_init_features': 10, 'small': True, 'test_time_pool': True}}, 'dpn92': {'encoder': DPNEncoder, 'pretrained_settings': pretrained_settings['dpn92'], 'params': {'stage_idxs': (4, 8, 28, 32), 'out_channels': (3, 64, 336, 704, 1552, 2688), 'groups': 32, 'inc_sec': (16, 32, 24, 128), 'k_r': 96, 'k_sec': (3, 4, 20, 3), 'num_classes': 1000, 'num_init_features': 64, 'test_time_pool': True}}, 'dpn98': {'encoder': DPNEncoder, 'pretrained_settings': pretrained_settings['dpn98'], 'params': {'stage_idxs': (4, 10, 30, 34), 'out_channels': (3, 96, 336, 768, 1728, 2688), 'groups': 40, 'inc_sec': (16, 32, 32, 128), 'k_r': 160, 'k_sec': (3, 6, 20, 3), 'num_classes': 1000, 'num_init_features': 96, 'test_time_pool': True}}, 'dpn107': {'encoder': DPNEncoder, 'pretrained_settings': pretrained_settings['dpn107'], 'params': {'stage_idxs': (5, 13, 33, 37), 'out_channels': (3, 128, 376, 1152, 2432, 2688), 'groups': 50, 'inc_sec': (20, 64, 64, 128), 'k_r': 200, 'k_sec': (4, 8, 20, 3), 'num_classes': 1000, 'num_init_features': 128, 'test_time_pool': True}}, 'dpn131': {'encoder': DPNEncoder, 'pretrained_settings': pretrained_settings['dpn131'], 'params': {'stage_idxs': (5, 13, 41, 45), 'out_channels': (3, 128, 352, 832, 1984, 2688), 'groups': 40, 'inc_sec': (16, 32, 32, 128), 'k_r': 160, 'k_sec': (4, 8, 28, 3), 'num_classes': 1000, 'num_init_features': 128, 'test_time_pool': True}}}</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/encoders/dpn/#src.thaw_slump_segmentation.models.encoders.dpn.DPNEncoder","title":"<code>DPNEncoder</code>","text":"<p>               Bases: <code>DPN</code>, <code>EncoderMixin</code></p> Source code in <code>src/thaw_slump_segmentation/models/encoders/dpn.py</code> <pre><code>class DPNEncoder(DPN, EncoderMixin):\n    def __init__(self, stage_idxs, out_channels, depth=5, **kwargs):\n        super().__init__(**kwargs)\n        self._stage_idxs = stage_idxs\n        self._depth = depth\n        self._out_channels = out_channels\n        self._in_channels = 3\n\n        del self.last_linear\n\n    def get_stages(self):\n        return [\n            nn.Identity(),\n            nn.Sequential(self.features[0].conv, self.features[0].bn, self.features[0].act),\n            nn.Sequential(self.features[0].pool, self.features[1 : self._stage_idxs[0]]),\n            self.features[self._stage_idxs[0] : self._stage_idxs[1]],\n            self.features[self._stage_idxs[1] : self._stage_idxs[2]],\n            self.features[self._stage_idxs[2] : self._stage_idxs[3]],\n        ]\n\n    def forward(self, x):\n\n        stages = self.get_stages()\n\n        features = []\n        for i in range(self._depth + 1):\n            x = stages[i](x)\n            if isinstance(x, (list, tuple)):\n                features.append(F.relu(torch.cat(x, dim=1), inplace=True))\n            else:\n                features.append(x)\n\n        return features\n\n    def load_state_dict(self, state_dict, **kwargs):\n        state_dict.pop(\"last_linear.bias\")\n        state_dict.pop(\"last_linear.weight\")\n        super().load_state_dict(state_dict, **kwargs)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/dpn/#src.thaw_slump_segmentation.models.encoders.dpn.DPNEncoder.__init__","title":"<code>__init__(stage_idxs, out_channels, depth=5, **kwargs)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/dpn.py</code> <pre><code>def __init__(self, stage_idxs, out_channels, depth=5, **kwargs):\n    super().__init__(**kwargs)\n    self._stage_idxs = stage_idxs\n    self._depth = depth\n    self._out_channels = out_channels\n    self._in_channels = 3\n\n    del self.last_linear\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/dpn/#src.thaw_slump_segmentation.models.encoders.dpn.DPNEncoder.forward","title":"<code>forward(x)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/dpn.py</code> <pre><code>def forward(self, x):\n\n    stages = self.get_stages()\n\n    features = []\n    for i in range(self._depth + 1):\n        x = stages[i](x)\n        if isinstance(x, (list, tuple)):\n            features.append(F.relu(torch.cat(x, dim=1), inplace=True))\n        else:\n            features.append(x)\n\n    return features\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/dpn/#src.thaw_slump_segmentation.models.encoders.dpn.DPNEncoder.get_stages","title":"<code>get_stages()</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/dpn.py</code> <pre><code>def get_stages(self):\n    return [\n        nn.Identity(),\n        nn.Sequential(self.features[0].conv, self.features[0].bn, self.features[0].act),\n        nn.Sequential(self.features[0].pool, self.features[1 : self._stage_idxs[0]]),\n        self.features[self._stage_idxs[0] : self._stage_idxs[1]],\n        self.features[self._stage_idxs[1] : self._stage_idxs[2]],\n        self.features[self._stage_idxs[2] : self._stage_idxs[3]],\n    ]\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/dpn/#src.thaw_slump_segmentation.models.encoders.dpn.DPNEncoder.load_state_dict","title":"<code>load_state_dict(state_dict, **kwargs)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/dpn.py</code> <pre><code>def load_state_dict(self, state_dict, **kwargs):\n    state_dict.pop(\"last_linear.bias\")\n    state_dict.pop(\"last_linear.weight\")\n    super().load_state_dict(state_dict, **kwargs)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/efficientnet/","title":"Efficientnet","text":"<p>Each encoder should have following attributes and methods and be inherited from <code>_base.EncoderMixin</code></p> <p>Attributes:</p> <pre><code>_out_channels (list of int): specify number of channels for each encoder feature tensor\n_depth (int): specify number of stages in decoder (in other words number of downsampling operations)\n_in_channels (int): default number of input channels in first Conv2d layer for encoder (usually 3)\n</code></pre> <p>Methods:</p> <pre><code>forward(self, x: torch.Tensor)\n    produce list of features of different spatial resolutions, each feature is a 4D torch.tensor of\n    shape NCHW (features should be sorted in descending order according to spatial resolution, starting\n    with resolution same as input `x` tensor).\n\n    Input: `x` with shape (1, 3, 64, 64)\n    Output: [f0, f1, f2, f3, f4, f5] - features with corresponding shapes\n            [(1, 3, 64, 64), (1, 64, 32, 32), (1, 128, 16, 16), (1, 256, 8, 8),\n            (1, 512, 4, 4), (1, 1024, 2, 2)] (C - dim may differ)\n\n    also should support number of features according to specified depth, e.g. if depth = 5,\n    number of feature tensors = 6 (one with same resolution as input and 5 downsampled),\n    depth = 3 -&gt; number of feature tensors = 4 (one with same resolution as input and 3 downsampled).\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/efficientnet/#src.thaw_slump_segmentation.models.encoders.efficientnet.efficient_net_encoders","title":"<code>efficient_net_encoders = {'efficientnet-b0': {'encoder': EfficientNetEncoder, 'pretrained_settings': _get_pretrained_settings('efficientnet-b0'), 'params': {'out_channels': (3, 32, 24, 40, 112, 320), 'stage_idxs': (3, 5, 9, 16), 'model_name': 'efficientnet-b0'}}, 'efficientnet-b1': {'encoder': EfficientNetEncoder, 'pretrained_settings': _get_pretrained_settings('efficientnet-b1'), 'params': {'out_channels': (3, 32, 24, 40, 112, 320), 'stage_idxs': (5, 8, 16, 23), 'model_name': 'efficientnet-b1'}}, 'efficientnet-b2': {'encoder': EfficientNetEncoder, 'pretrained_settings': _get_pretrained_settings('efficientnet-b2'), 'params': {'out_channels': (3, 32, 24, 48, 120, 352), 'stage_idxs': (5, 8, 16, 23), 'model_name': 'efficientnet-b2'}}, 'efficientnet-b3': {'encoder': EfficientNetEncoder, 'pretrained_settings': _get_pretrained_settings('efficientnet-b3'), 'params': {'out_channels': (3, 40, 32, 48, 136, 384), 'stage_idxs': (5, 8, 18, 26), 'model_name': 'efficientnet-b3'}}, 'efficientnet-b4': {'encoder': EfficientNetEncoder, 'pretrained_settings': _get_pretrained_settings('efficientnet-b4'), 'params': {'out_channels': (3, 48, 32, 56, 160, 448), 'stage_idxs': (6, 10, 22, 32), 'model_name': 'efficientnet-b4'}}, 'efficientnet-b5': {'encoder': EfficientNetEncoder, 'pretrained_settings': _get_pretrained_settings('efficientnet-b5'), 'params': {'out_channels': (3, 48, 40, 64, 176, 512), 'stage_idxs': (8, 13, 27, 39), 'model_name': 'efficientnet-b5'}}, 'efficientnet-b6': {'encoder': EfficientNetEncoder, 'pretrained_settings': _get_pretrained_settings('efficientnet-b6'), 'params': {'out_channels': (3, 56, 40, 72, 200, 576), 'stage_idxs': (9, 15, 31, 45), 'model_name': 'efficientnet-b6'}}, 'efficientnet-b7': {'encoder': EfficientNetEncoder, 'pretrained_settings': _get_pretrained_settings('efficientnet-b7'), 'params': {'out_channels': (3, 64, 48, 80, 224, 640), 'stage_idxs': (11, 18, 38, 55), 'model_name': 'efficientnet-b7'}}}</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/encoders/efficientnet/#src.thaw_slump_segmentation.models.encoders.efficientnet.EfficientNetEncoder","title":"<code>EfficientNetEncoder</code>","text":"<p>               Bases: <code>EfficientNet</code>, <code>EncoderMixin</code></p> Source code in <code>src/thaw_slump_segmentation/models/encoders/efficientnet.py</code> <pre><code>class EfficientNetEncoder(EfficientNet, EncoderMixin):\n    def __init__(self, stage_idxs, out_channels, model_name, depth=5):\n\n        blocks_args, global_params = get_model_params(model_name, override_params=None)\n        super().__init__(blocks_args, global_params)\n\n        self._stage_idxs = stage_idxs\n        self._out_channels = out_channels\n        self._depth = depth\n        self._in_channels = 3\n\n        del self._fc\n\n    def get_stages(self):\n        return [\n            nn.Identity(),\n            nn.Sequential(self._conv_stem, self._bn0, self._swish),\n            self._blocks[:self._stage_idxs[0]],\n            self._blocks[self._stage_idxs[0]:self._stage_idxs[1]],\n            self._blocks[self._stage_idxs[1]:self._stage_idxs[2]],\n            self._blocks[self._stage_idxs[2]:],\n        ]\n\n    def forward(self, x):\n        stages = self.get_stages()\n\n        block_number = 0.\n        drop_connect_rate = self._global_params.drop_connect_rate\n\n        features = []\n        for i in range(self._depth + 1):\n\n            # Identity and Sequential stages\n            if i &lt; 2:\n                x = stages[i](x)\n\n            # Block stages need drop_connect rate\n            else:\n                for module in stages[i]:\n                    drop_connect = drop_connect_rate * block_number / len(self._blocks)\n                    block_number += 1.\n                    x = module(x, drop_connect)\n\n            features.append(x)\n\n        return features\n\n    def load_state_dict(self, state_dict, **kwargs):\n        state_dict.pop(\"_fc.bias\")\n        state_dict.pop(\"_fc.weight\")\n        super().load_state_dict(state_dict, **kwargs)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/efficientnet/#src.thaw_slump_segmentation.models.encoders.efficientnet.EfficientNetEncoder.__init__","title":"<code>__init__(stage_idxs, out_channels, model_name, depth=5)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/efficientnet.py</code> <pre><code>def __init__(self, stage_idxs, out_channels, model_name, depth=5):\n\n    blocks_args, global_params = get_model_params(model_name, override_params=None)\n    super().__init__(blocks_args, global_params)\n\n    self._stage_idxs = stage_idxs\n    self._out_channels = out_channels\n    self._depth = depth\n    self._in_channels = 3\n\n    del self._fc\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/efficientnet/#src.thaw_slump_segmentation.models.encoders.efficientnet.EfficientNetEncoder.forward","title":"<code>forward(x)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/efficientnet.py</code> <pre><code>def forward(self, x):\n    stages = self.get_stages()\n\n    block_number = 0.\n    drop_connect_rate = self._global_params.drop_connect_rate\n\n    features = []\n    for i in range(self._depth + 1):\n\n        # Identity and Sequential stages\n        if i &lt; 2:\n            x = stages[i](x)\n\n        # Block stages need drop_connect rate\n        else:\n            for module in stages[i]:\n                drop_connect = drop_connect_rate * block_number / len(self._blocks)\n                block_number += 1.\n                x = module(x, drop_connect)\n\n        features.append(x)\n\n    return features\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/efficientnet/#src.thaw_slump_segmentation.models.encoders.efficientnet.EfficientNetEncoder.get_stages","title":"<code>get_stages()</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/efficientnet.py</code> <pre><code>def get_stages(self):\n    return [\n        nn.Identity(),\n        nn.Sequential(self._conv_stem, self._bn0, self._swish),\n        self._blocks[:self._stage_idxs[0]],\n        self._blocks[self._stage_idxs[0]:self._stage_idxs[1]],\n        self._blocks[self._stage_idxs[1]:self._stage_idxs[2]],\n        self._blocks[self._stage_idxs[2]:],\n    ]\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/efficientnet/#src.thaw_slump_segmentation.models.encoders.efficientnet.EfficientNetEncoder.load_state_dict","title":"<code>load_state_dict(state_dict, **kwargs)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/efficientnet.py</code> <pre><code>def load_state_dict(self, state_dict, **kwargs):\n    state_dict.pop(\"_fc.bias\")\n    state_dict.pop(\"_fc.weight\")\n    super().load_state_dict(state_dict, **kwargs)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/inceptionresnetv2/","title":"Inceptionresnetv2","text":"<p>Each encoder should have following attributes and methods and be inherited from <code>_base.EncoderMixin</code></p> <p>Attributes:</p> <pre><code>_out_channels (list of int): specify number of channels for each encoder feature tensor\n_depth (int): specify number of stages in decoder (in other words number of downsampling operations)\n_in_channels (int): default number of input channels in first Conv2d layer for encoder (usually 3)\n</code></pre> <p>Methods:</p> <pre><code>forward(self, x: torch.Tensor)\n    produce list of features of different spatial resolutions, each feature is a 4D torch.tensor of\n    shape NCHW (features should be sorted in descending order according to spatial resolution, starting\n    with resolution same as input `x` tensor).\n\n    Input: `x` with shape (1, 3, 64, 64)\n    Output: [f0, f1, f2, f3, f4, f5] - features with corresponding shapes\n            [(1, 3, 64, 64), (1, 64, 32, 32), (1, 128, 16, 16), (1, 256, 8, 8),\n            (1, 512, 4, 4), (1, 1024, 2, 2)] (C - dim may differ)\n\n    also should support number of features according to specified depth, e.g. if depth = 5,\n    number of feature tensors = 6 (one with same resolution as input and 5 downsampled),\n    depth = 3 -&gt; number of feature tensors = 4 (one with same resolution as input and 3 downsampled).\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/inceptionresnetv2/#src.thaw_slump_segmentation.models.encoders.inceptionresnetv2.inceptionresnetv2_encoders","title":"<code>inceptionresnetv2_encoders = {'inceptionresnetv2': {'encoder': InceptionResNetV2Encoder, 'pretrained_settings': pretrained_settings['inceptionresnetv2'], 'params': {'out_channels': (3, 64, 192, 320, 1088, 1536), 'num_classes': 1000}}}</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/encoders/inceptionresnetv2/#src.thaw_slump_segmentation.models.encoders.inceptionresnetv2.InceptionResNetV2Encoder","title":"<code>InceptionResNetV2Encoder</code>","text":"<p>               Bases: <code>InceptionResNetV2</code>, <code>EncoderMixin</code></p> Source code in <code>src/thaw_slump_segmentation/models/encoders/inceptionresnetv2.py</code> <pre><code>class InceptionResNetV2Encoder(InceptionResNetV2, EncoderMixin):\n    def __init__(self, out_channels, depth=5, **kwargs):\n        super().__init__(**kwargs)\n\n        self._out_channels = out_channels\n        self._depth = depth\n        self._in_channels = 3\n\n        # correct paddings\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                if m.kernel_size == (3, 3):\n                    m.padding = (1, 1)\n            if isinstance(m, nn.MaxPool2d):\n                m.padding = (1, 1)\n\n        # remove linear layers\n        del self.avgpool_1a\n        del self.last_linear\n\n    def make_dilated(self, stage_list, dilation_list):\n        raise ValueError(\"InceptionResnetV2 encoder does not support dilated mode \"\n                         \"due to pooling operation for downsampling!\")\n\n    def get_stages(self):\n        return [\n            nn.Identity(),\n            nn.Sequential(self.conv2d_1a, self.conv2d_2a, self.conv2d_2b),\n            nn.Sequential(self.maxpool_3a, self.conv2d_3b, self.conv2d_4a),\n            nn.Sequential(self.maxpool_5a, self.mixed_5b, self.repeat),\n            nn.Sequential(self.mixed_6a, self.repeat_1),\n            nn.Sequential(self.mixed_7a, self.repeat_2, self.block8, self.conv2d_7b),\n        ]\n\n    def forward(self, x):\n\n        stages = self.get_stages()\n\n        features = []\n        for i in range(self._depth + 1):\n            x = stages[i](x)\n            features.append(x)\n\n        return features\n\n    def load_state_dict(self, state_dict, **kwargs):\n        state_dict.pop(\"last_linear.bias\")\n        state_dict.pop(\"last_linear.weight\")\n        super().load_state_dict(state_dict, **kwargs)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/inceptionresnetv2/#src.thaw_slump_segmentation.models.encoders.inceptionresnetv2.InceptionResNetV2Encoder.__init__","title":"<code>__init__(out_channels, depth=5, **kwargs)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/inceptionresnetv2.py</code> <pre><code>def __init__(self, out_channels, depth=5, **kwargs):\n    super().__init__(**kwargs)\n\n    self._out_channels = out_channels\n    self._depth = depth\n    self._in_channels = 3\n\n    # correct paddings\n    for m in self.modules():\n        if isinstance(m, nn.Conv2d):\n            if m.kernel_size == (3, 3):\n                m.padding = (1, 1)\n        if isinstance(m, nn.MaxPool2d):\n            m.padding = (1, 1)\n\n    # remove linear layers\n    del self.avgpool_1a\n    del self.last_linear\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/inceptionresnetv2/#src.thaw_slump_segmentation.models.encoders.inceptionresnetv2.InceptionResNetV2Encoder.forward","title":"<code>forward(x)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/inceptionresnetv2.py</code> <pre><code>def forward(self, x):\n\n    stages = self.get_stages()\n\n    features = []\n    for i in range(self._depth + 1):\n        x = stages[i](x)\n        features.append(x)\n\n    return features\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/inceptionresnetv2/#src.thaw_slump_segmentation.models.encoders.inceptionresnetv2.InceptionResNetV2Encoder.get_stages","title":"<code>get_stages()</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/inceptionresnetv2.py</code> <pre><code>def get_stages(self):\n    return [\n        nn.Identity(),\n        nn.Sequential(self.conv2d_1a, self.conv2d_2a, self.conv2d_2b),\n        nn.Sequential(self.maxpool_3a, self.conv2d_3b, self.conv2d_4a),\n        nn.Sequential(self.maxpool_5a, self.mixed_5b, self.repeat),\n        nn.Sequential(self.mixed_6a, self.repeat_1),\n        nn.Sequential(self.mixed_7a, self.repeat_2, self.block8, self.conv2d_7b),\n    ]\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/inceptionresnetv2/#src.thaw_slump_segmentation.models.encoders.inceptionresnetv2.InceptionResNetV2Encoder.load_state_dict","title":"<code>load_state_dict(state_dict, **kwargs)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/inceptionresnetv2.py</code> <pre><code>def load_state_dict(self, state_dict, **kwargs):\n    state_dict.pop(\"last_linear.bias\")\n    state_dict.pop(\"last_linear.weight\")\n    super().load_state_dict(state_dict, **kwargs)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/inceptionresnetv2/#src.thaw_slump_segmentation.models.encoders.inceptionresnetv2.InceptionResNetV2Encoder.make_dilated","title":"<code>make_dilated(stage_list, dilation_list)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/inceptionresnetv2.py</code> <pre><code>def make_dilated(self, stage_list, dilation_list):\n    raise ValueError(\"InceptionResnetV2 encoder does not support dilated mode \"\n                     \"due to pooling operation for downsampling!\")\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/inceptionv4/","title":"Inceptionv4","text":"<p>Each encoder should have following attributes and methods and be inherited from <code>_base.EncoderMixin</code></p> <p>Attributes:</p> <pre><code>_out_channels (list of int): specify number of channels for each encoder feature tensor\n_depth (int): specify number of stages in decoder (in other words number of downsampling operations)\n_in_channels (int): default number of input channels in first Conv2d layer for encoder (usually 3)\n</code></pre> <p>Methods:</p> <pre><code>forward(self, x: torch.Tensor)\n    produce list of features of different spatial resolutions, each feature is a 4D torch.tensor of\n    shape NCHW (features should be sorted in descending order according to spatial resolution, starting\n    with resolution same as input `x` tensor).\n\n    Input: `x` with shape (1, 3, 64, 64)\n    Output: [f0, f1, f2, f3, f4, f5] - features with corresponding shapes\n            [(1, 3, 64, 64), (1, 64, 32, 32), (1, 128, 16, 16), (1, 256, 8, 8),\n            (1, 512, 4, 4), (1, 1024, 2, 2)] (C - dim may differ)\n\n    also should support number of features according to specified depth, e.g. if depth = 5,\n    number of feature tensors = 6 (one with same resolution as input and 5 downsampled),\n    depth = 3 -&gt; number of feature tensors = 4 (one with same resolution as input and 3 downsampled).\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/inceptionv4/#src.thaw_slump_segmentation.models.encoders.inceptionv4.inceptionv4_encoders","title":"<code>inceptionv4_encoders = {'inceptionv4': {'encoder': InceptionV4Encoder, 'pretrained_settings': pretrained_settings['inceptionv4'], 'params': {'stage_idxs': (3, 5, 9, 15), 'out_channels': (3, 64, 192, 384, 1024, 1536), 'num_classes': 1001}}}</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/encoders/inceptionv4/#src.thaw_slump_segmentation.models.encoders.inceptionv4.InceptionV4Encoder","title":"<code>InceptionV4Encoder</code>","text":"<p>               Bases: <code>InceptionV4</code>, <code>EncoderMixin</code></p> Source code in <code>src/thaw_slump_segmentation/models/encoders/inceptionv4.py</code> <pre><code>class InceptionV4Encoder(InceptionV4, EncoderMixin):\n    def __init__(self, stage_idxs, out_channels, depth=5, **kwargs):\n        super().__init__(**kwargs)\n        self._stage_idxs = stage_idxs\n        self._out_channels = out_channels\n        self._depth = depth\n        self._in_channels = 3\n\n        # correct paddings\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                if m.kernel_size == (3, 3):\n                    m.padding = (1, 1)\n            if isinstance(m, nn.MaxPool2d):\n                m.padding = (1, 1)\n\n        # remove linear layers\n        del self.last_linear\n\n    def make_dilated(self, stage_list, dilation_list):\n        raise ValueError(\"InceptionV4 encoder does not support dilated mode \"\n                         \"due to pooling operation for downsampling!\")\n\n    def get_stages(self):\n        return [\n            nn.Identity(),\n            self.features[: self._stage_idxs[0]],\n            self.features[self._stage_idxs[0]: self._stage_idxs[1]],\n            self.features[self._stage_idxs[1]: self._stage_idxs[2]],\n            self.features[self._stage_idxs[2]: self._stage_idxs[3]],\n            self.features[self._stage_idxs[3]:],\n        ]\n\n    def forward(self, x):\n\n        stages = self.get_stages()\n\n        features = []\n        for i in range(self._depth + 1):\n            x = stages[i](x)\n            features.append(x)\n\n        return features\n\n    def load_state_dict(self, state_dict, **kwargs):\n        state_dict.pop(\"last_linear.bias\")\n        state_dict.pop(\"last_linear.weight\")\n        super().load_state_dict(state_dict, **kwargs)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/inceptionv4/#src.thaw_slump_segmentation.models.encoders.inceptionv4.InceptionV4Encoder.__init__","title":"<code>__init__(stage_idxs, out_channels, depth=5, **kwargs)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/inceptionv4.py</code> <pre><code>def __init__(self, stage_idxs, out_channels, depth=5, **kwargs):\n    super().__init__(**kwargs)\n    self._stage_idxs = stage_idxs\n    self._out_channels = out_channels\n    self._depth = depth\n    self._in_channels = 3\n\n    # correct paddings\n    for m in self.modules():\n        if isinstance(m, nn.Conv2d):\n            if m.kernel_size == (3, 3):\n                m.padding = (1, 1)\n        if isinstance(m, nn.MaxPool2d):\n            m.padding = (1, 1)\n\n    # remove linear layers\n    del self.last_linear\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/inceptionv4/#src.thaw_slump_segmentation.models.encoders.inceptionv4.InceptionV4Encoder.forward","title":"<code>forward(x)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/inceptionv4.py</code> <pre><code>def forward(self, x):\n\n    stages = self.get_stages()\n\n    features = []\n    for i in range(self._depth + 1):\n        x = stages[i](x)\n        features.append(x)\n\n    return features\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/inceptionv4/#src.thaw_slump_segmentation.models.encoders.inceptionv4.InceptionV4Encoder.get_stages","title":"<code>get_stages()</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/inceptionv4.py</code> <pre><code>def get_stages(self):\n    return [\n        nn.Identity(),\n        self.features[: self._stage_idxs[0]],\n        self.features[self._stage_idxs[0]: self._stage_idxs[1]],\n        self.features[self._stage_idxs[1]: self._stage_idxs[2]],\n        self.features[self._stage_idxs[2]: self._stage_idxs[3]],\n        self.features[self._stage_idxs[3]:],\n    ]\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/inceptionv4/#src.thaw_slump_segmentation.models.encoders.inceptionv4.InceptionV4Encoder.load_state_dict","title":"<code>load_state_dict(state_dict, **kwargs)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/inceptionv4.py</code> <pre><code>def load_state_dict(self, state_dict, **kwargs):\n    state_dict.pop(\"last_linear.bias\")\n    state_dict.pop(\"last_linear.weight\")\n    super().load_state_dict(state_dict, **kwargs)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/inceptionv4/#src.thaw_slump_segmentation.models.encoders.inceptionv4.InceptionV4Encoder.make_dilated","title":"<code>make_dilated(stage_list, dilation_list)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/inceptionv4.py</code> <pre><code>def make_dilated(self, stage_list, dilation_list):\n    raise ValueError(\"InceptionV4 encoder does not support dilated mode \"\n                     \"due to pooling operation for downsampling!\")\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/mobilenet/","title":"Mobilenet","text":"<p>Each encoder should have following attributes and methods and be inherited from <code>_base.EncoderMixin</code></p> <p>Attributes:</p> <pre><code>_out_channels (list of int): specify number of channels for each encoder feature tensor\n_depth (int): specify number of stages in decoder (in other words number of downsampling operations)\n_in_channels (int): default number of input channels in first Conv2d layer for encoder (usually 3)\n</code></pre> <p>Methods:</p> <pre><code>forward(self, x: torch.Tensor)\n    produce list of features of different spatial resolutions, each feature is a 4D torch.tensor of\n    shape NCHW (features should be sorted in descending order according to spatial resolution, starting\n    with resolution same as input `x` tensor).\n\n    Input: `x` with shape (1, 3, 64, 64)\n    Output: [f0, f1, f2, f3, f4, f5] - features with corresponding shapes\n            [(1, 3, 64, 64), (1, 64, 32, 32), (1, 128, 16, 16), (1, 256, 8, 8),\n            (1, 512, 4, 4), (1, 1024, 2, 2)] (C - dim may differ)\n\n    also should support number of features according to specified depth, e.g. if depth = 5,\n    number of feature tensors = 6 (one with same resolution as input and 5 downsampled),\n    depth = 3 -&gt; number of feature tensors = 4 (one with same resolution as input and 3 downsampled).\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/mobilenet/#src.thaw_slump_segmentation.models.encoders.mobilenet.mobilenet_encoders","title":"<code>mobilenet_encoders = {'mobilenet_v2': {'encoder': MobileNetV2Encoder, 'pretrained_settings': {'imagenet': {'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225], 'url': 'https://download.pytorch.org/models/mobilenet_v2-b0353104.pth', 'input_space': 'RGB', 'input_range': [0, 1]}}, 'params': {'out_channels': (3, 16, 24, 32, 96, 1280)}}}</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/encoders/mobilenet/#src.thaw_slump_segmentation.models.encoders.mobilenet.MobileNetV2Encoder","title":"<code>MobileNetV2Encoder</code>","text":"<p>               Bases: <code>MobileNetV2</code>, <code>EncoderMixin</code></p> Source code in <code>src/thaw_slump_segmentation/models/encoders/mobilenet.py</code> <pre><code>class MobileNetV2Encoder(torchvision.models.MobileNetV2, EncoderMixin):\n\n    def __init__(self, out_channels, depth=5, **kwargs):\n        super().__init__(**kwargs)\n        self._depth = depth\n        self._out_channels = out_channels\n        self._in_channels = 3\n        del self.classifier\n\n    def get_stages(self):\n        return [\n            nn.Identity(),\n            self.features[:2],\n            self.features[2:4],\n            self.features[4:7],\n            self.features[7:14],\n            self.features[14:],\n        ]\n\n    def forward(self, x):\n        stages = self.get_stages()\n\n        features = []\n        for i in range(self._depth + 1):\n            x = stages[i](x)\n            features.append(x)\n\n        return features\n\n    def load_state_dict(self, state_dict, **kwargs):\n        state_dict.pop(\"classifier.1.bias\")\n        state_dict.pop(\"classifier.1.weight\")\n        super().load_state_dict(state_dict, **kwargs)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/mobilenet/#src.thaw_slump_segmentation.models.encoders.mobilenet.MobileNetV2Encoder.__init__","title":"<code>__init__(out_channels, depth=5, **kwargs)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/mobilenet.py</code> <pre><code>def __init__(self, out_channels, depth=5, **kwargs):\n    super().__init__(**kwargs)\n    self._depth = depth\n    self._out_channels = out_channels\n    self._in_channels = 3\n    del self.classifier\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/mobilenet/#src.thaw_slump_segmentation.models.encoders.mobilenet.MobileNetV2Encoder.forward","title":"<code>forward(x)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/mobilenet.py</code> <pre><code>def forward(self, x):\n    stages = self.get_stages()\n\n    features = []\n    for i in range(self._depth + 1):\n        x = stages[i](x)\n        features.append(x)\n\n    return features\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/mobilenet/#src.thaw_slump_segmentation.models.encoders.mobilenet.MobileNetV2Encoder.get_stages","title":"<code>get_stages()</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/mobilenet.py</code> <pre><code>def get_stages(self):\n    return [\n        nn.Identity(),\n        self.features[:2],\n        self.features[2:4],\n        self.features[4:7],\n        self.features[7:14],\n        self.features[14:],\n    ]\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/mobilenet/#src.thaw_slump_segmentation.models.encoders.mobilenet.MobileNetV2Encoder.load_state_dict","title":"<code>load_state_dict(state_dict, **kwargs)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/mobilenet.py</code> <pre><code>def load_state_dict(self, state_dict, **kwargs):\n    state_dict.pop(\"classifier.1.bias\")\n    state_dict.pop(\"classifier.1.weight\")\n    super().load_state_dict(state_dict, **kwargs)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/resnet/","title":"Resnet","text":"<p>Each encoder should have following attributes and methods and be inherited from <code>_base.EncoderMixin</code></p> <p>Attributes:</p> <pre><code>_out_channels (list of int): specify number of channels for each encoder feature tensor\n_depth (int): specify number of stages in decoder (in other words number of downsampling operations)\n_in_channels (int): default number of input channels in first Conv2d layer for encoder (usually 3)\n</code></pre> <p>Methods:</p> <pre><code>forward(self, x: torch.Tensor)\n    produce list of features of different spatial resolutions, each feature is a 4D torch.tensor of\n    shape NCHW (features should be sorted in descending order according to spatial resolution, starting\n    with resolution same as input `x` tensor).\n\n    Input: `x` with shape (1, 3, 64, 64)\n    Output: [f0, f1, f2, f3, f4, f5] - features with corresponding shapes\n            [(1, 3, 64, 64), (1, 64, 32, 32), (1, 128, 16, 16), (1, 256, 8, 8),\n            (1, 512, 4, 4), (1, 1024, 2, 2)] (C - dim may differ)\n\n    also should support number of features according to specified depth, e.g. if depth = 5,\n    number of feature tensors = 6 (one with same resolution as input and 5 downsampled),\n    depth = 3 -&gt; number of feature tensors = 4 (one with same resolution as input and 3 downsampled).\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/resnet/#src.thaw_slump_segmentation.models.encoders.resnet.new_settings","title":"<code>new_settings = {'resnet18': {'ssl': 'https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_supervised_resnet18-d92f0530.pth', 'swsl': 'https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_weakly_supervised_resnet18-118f1556.pth'}, 'resnet50': {'ssl': 'https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_supervised_resnet50-08389792.pth', 'swsl': 'https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_weakly_supervised_resnet50-16a12f1b.pth'}, 'resnext50_32x4d': {'imagenet': 'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth', 'ssl': 'https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_supervised_resnext50_32x4-ddb3e555.pth', 'swsl': 'https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_weakly_supervised_resnext50_32x4-72679e44.pth'}, 'resnext101_32x4d': {'ssl': 'https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_supervised_resnext101_32x4-dc43570a.pth', 'swsl': 'https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_weakly_supervised_resnext101_32x4-3f87e46b.pth'}, 'resnext101_32x8d': {'imagenet': 'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth', 'instagram': 'https://download.pytorch.org/models/ig_resnext101_32x8-c38310e5.pth', 'ssl': 'https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_supervised_resnext101_32x8-2cfe2f8b.pth', 'swsl': 'https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_weakly_supervised_resnext101_32x8-b4712904.pth'}, 'resnext101_32x16d': {'instagram': 'https://download.pytorch.org/models/ig_resnext101_32x16-c6f796b0.pth', 'ssl': 'https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_supervised_resnext101_32x16-15fffa57.pth', 'swsl': 'https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_weakly_supervised_resnext101_32x16-f3559a9c.pth'}, 'resnext101_32x32d': {'instagram': 'https://download.pytorch.org/models/ig_resnext101_32x32-e4b90b00.pth'}, 'resnext101_32x48d': {'instagram': 'https://download.pytorch.org/models/ig_resnext101_32x48-3e41cc8a.pth'}}</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/encoders/resnet/#src.thaw_slump_segmentation.models.encoders.resnet.resnet_encoders","title":"<code>resnet_encoders = {'resnet18': {'encoder': ResNetEncoder, 'pretrained_settings': pretrained_settings['resnet18'], 'params': {'out_channels': (3, 64, 64, 128, 256, 512), 'block': BasicBlock, 'layers': [2, 2, 2, 2]}}, 'resnet34': {'encoder': ResNetEncoder, 'pretrained_settings': pretrained_settings['resnet34'], 'params': {'out_channels': (3, 64, 64, 128, 256, 512), 'block': BasicBlock, 'layers': [3, 4, 6, 3]}}, 'resnet50': {'encoder': ResNetEncoder, 'pretrained_settings': pretrained_settings['resnet50'], 'params': {'out_channels': (3, 64, 256, 512, 1024, 2048), 'block': Bottleneck, 'layers': [3, 4, 6, 3]}}, 'resnet101': {'encoder': ResNetEncoder, 'pretrained_settings': pretrained_settings['resnet101'], 'params': {'out_channels': (3, 64, 256, 512, 1024, 2048), 'block': Bottleneck, 'layers': [3, 4, 23, 3]}}, 'resnet152': {'encoder': ResNetEncoder, 'pretrained_settings': pretrained_settings['resnet152'], 'params': {'out_channels': (3, 64, 256, 512, 1024, 2048), 'block': Bottleneck, 'layers': [3, 8, 36, 3]}}, 'resnext50_32x4d': {'encoder': ResNetEncoder, 'pretrained_settings': pretrained_settings['resnext50_32x4d'], 'params': {'out_channels': (3, 64, 256, 512, 1024, 2048), 'block': Bottleneck, 'layers': [3, 4, 6, 3], 'groups': 32, 'width_per_group': 4}}, 'resnext101_32x4d': {'encoder': ResNetEncoder, 'pretrained_settings': pretrained_settings['resnext101_32x4d'], 'params': {'out_channels': (3, 64, 256, 512, 1024, 2048), 'block': Bottleneck, 'layers': [3, 4, 23, 3], 'groups': 32, 'width_per_group': 4}}, 'resnext101_32x8d': {'encoder': ResNetEncoder, 'pretrained_settings': pretrained_settings['resnext101_32x8d'], 'params': {'out_channels': (3, 64, 256, 512, 1024, 2048), 'block': Bottleneck, 'layers': [3, 4, 23, 3], 'groups': 32, 'width_per_group': 8}}, 'resnext101_32x16d': {'encoder': ResNetEncoder, 'pretrained_settings': pretrained_settings['resnext101_32x16d'], 'params': {'out_channels': (3, 64, 256, 512, 1024, 2048), 'block': Bottleneck, 'layers': [3, 4, 23, 3], 'groups': 32, 'width_per_group': 16}}, 'resnext101_32x32d': {'encoder': ResNetEncoder, 'pretrained_settings': pretrained_settings['resnext101_32x32d'], 'params': {'out_channels': (3, 64, 256, 512, 1024, 2048), 'block': Bottleneck, 'layers': [3, 4, 23, 3], 'groups': 32, 'width_per_group': 32}}, 'resnext101_32x48d': {'encoder': ResNetEncoder, 'pretrained_settings': pretrained_settings['resnext101_32x48d'], 'params': {'out_channels': (3, 64, 256, 512, 1024, 2048), 'block': Bottleneck, 'layers': [3, 4, 23, 3], 'groups': 32, 'width_per_group': 48}}}</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/encoders/resnet/#src.thaw_slump_segmentation.models.encoders.resnet.ResNetEncoder","title":"<code>ResNetEncoder</code>","text":"<p>               Bases: <code>ResNet</code>, <code>EncoderMixin</code></p> Source code in <code>src/thaw_slump_segmentation/models/encoders/resnet.py</code> <pre><code>class ResNetEncoder(ResNet, EncoderMixin):\n    def __init__(self, out_channels, depth=5, **kwargs):\n        super().__init__(**kwargs)\n        self._depth = depth\n        self._out_channels = out_channels\n        self._in_channels = 3\n\n        del self.fc\n        del self.avgpool\n\n    def get_stages(self):\n        return [\n            nn.Identity(),\n            nn.Sequential(self.conv1, self.bn1, self.relu),\n            nn.Sequential(self.maxpool, self.layer1),\n            self.layer2,\n            self.layer3,\n            self.layer4,\n        ]\n\n    def forward(self, x):\n        stages = self.get_stages()\n\n        features = []\n        for i in range(self._depth + 1):\n            x = stages[i](x)\n            features.append(x)\n\n        return features\n\n    def load_state_dict(self, state_dict, **kwargs):\n        state_dict.pop(\"fc.bias\")\n        state_dict.pop(\"fc.weight\")\n        super().load_state_dict(state_dict, **kwargs)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/resnet/#src.thaw_slump_segmentation.models.encoders.resnet.ResNetEncoder.__init__","title":"<code>__init__(out_channels, depth=5, **kwargs)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/resnet.py</code> <pre><code>def __init__(self, out_channels, depth=5, **kwargs):\n    super().__init__(**kwargs)\n    self._depth = depth\n    self._out_channels = out_channels\n    self._in_channels = 3\n\n    del self.fc\n    del self.avgpool\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/resnet/#src.thaw_slump_segmentation.models.encoders.resnet.ResNetEncoder.forward","title":"<code>forward(x)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/resnet.py</code> <pre><code>def forward(self, x):\n    stages = self.get_stages()\n\n    features = []\n    for i in range(self._depth + 1):\n        x = stages[i](x)\n        features.append(x)\n\n    return features\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/resnet/#src.thaw_slump_segmentation.models.encoders.resnet.ResNetEncoder.get_stages","title":"<code>get_stages()</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/resnet.py</code> <pre><code>def get_stages(self):\n    return [\n        nn.Identity(),\n        nn.Sequential(self.conv1, self.bn1, self.relu),\n        nn.Sequential(self.maxpool, self.layer1),\n        self.layer2,\n        self.layer3,\n        self.layer4,\n    ]\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/resnet/#src.thaw_slump_segmentation.models.encoders.resnet.ResNetEncoder.load_state_dict","title":"<code>load_state_dict(state_dict, **kwargs)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/resnet.py</code> <pre><code>def load_state_dict(self, state_dict, **kwargs):\n    state_dict.pop(\"fc.bias\")\n    state_dict.pop(\"fc.weight\")\n    super().load_state_dict(state_dict, **kwargs)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/senet/","title":"Senet","text":"<p>Each encoder should have following attributes and methods and be inherited from <code>_base.EncoderMixin</code></p> <p>Attributes:</p> <pre><code>_out_channels (list of int): specify number of channels for each encoder feature tensor\n_depth (int): specify number of stages in decoder (in other words number of downsampling operations)\n_in_channels (int): default number of input channels in first Conv2d layer for encoder (usually 3)\n</code></pre> <p>Methods:</p> <pre><code>forward(self, x: torch.Tensor)\n    produce list of features of different spatial resolutions, each feature is a 4D torch.tensor of\n    shape NCHW (features should be sorted in descending order according to spatial resolution, starting\n    with resolution same as input `x` tensor).\n\n    Input: `x` with shape (1, 3, 64, 64)\n    Output: [f0, f1, f2, f3, f4, f5] - features with corresponding shapes\n            [(1, 3, 64, 64), (1, 64, 32, 32), (1, 128, 16, 16), (1, 256, 8, 8),\n            (1, 512, 4, 4), (1, 1024, 2, 2)] (C - dim may differ)\n\n    also should support number of features according to specified depth, e.g. if depth = 5,\n    number of feature tensors = 6 (one with same resolution as input and 5 downsampled),\n    depth = 3 -&gt; number of feature tensors = 4 (one with same resolution as input and 3 downsampled).\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/senet/#src.thaw_slump_segmentation.models.encoders.senet.senet_encoders","title":"<code>senet_encoders = {'senet154': {'encoder': SENetEncoder, 'pretrained_settings': pretrained_settings['senet154'], 'params': {'out_channels': (3, 128, 256, 512, 1024, 2048), 'block': SEBottleneck, 'dropout_p': 0.2, 'groups': 64, 'layers': [3, 8, 36, 3], 'num_classes': 1000, 'reduction': 16}}, 'se_resnet50': {'encoder': SENetEncoder, 'pretrained_settings': pretrained_settings['se_resnet50'], 'params': {'out_channels': (3, 64, 256, 512, 1024, 2048), 'block': SEResNetBottleneck, 'layers': [3, 4, 6, 3], 'downsample_kernel_size': 1, 'downsample_padding': 0, 'dropout_p': None, 'groups': 1, 'inplanes': 64, 'input_3x3': False, 'num_classes': 1000, 'reduction': 16}}, 'se_resnet101': {'encoder': SENetEncoder, 'pretrained_settings': pretrained_settings['se_resnet101'], 'params': {'out_channels': (3, 64, 256, 512, 1024, 2048), 'block': SEResNetBottleneck, 'layers': [3, 4, 23, 3], 'downsample_kernel_size': 1, 'downsample_padding': 0, 'dropout_p': None, 'groups': 1, 'inplanes': 64, 'input_3x3': False, 'num_classes': 1000, 'reduction': 16}}, 'se_resnet152': {'encoder': SENetEncoder, 'pretrained_settings': pretrained_settings['se_resnet152'], 'params': {'out_channels': (3, 64, 256, 512, 1024, 2048), 'block': SEResNetBottleneck, 'layers': [3, 8, 36, 3], 'downsample_kernel_size': 1, 'downsample_padding': 0, 'dropout_p': None, 'groups': 1, 'inplanes': 64, 'input_3x3': False, 'num_classes': 1000, 'reduction': 16}}, 'se_resnext50_32x4d': {'encoder': SENetEncoder, 'pretrained_settings': pretrained_settings['se_resnext50_32x4d'], 'params': {'out_channels': (3, 64, 256, 512, 1024, 2048), 'block': SEResNeXtBottleneck, 'layers': [3, 4, 6, 3], 'downsample_kernel_size': 1, 'downsample_padding': 0, 'dropout_p': None, 'groups': 32, 'inplanes': 64, 'input_3x3': False, 'num_classes': 1000, 'reduction': 16}}, 'se_resnext101_32x4d': {'encoder': SENetEncoder, 'pretrained_settings': pretrained_settings['se_resnext101_32x4d'], 'params': {'out_channels': (3, 64, 256, 512, 1024, 2048), 'block': SEResNeXtBottleneck, 'layers': [3, 4, 23, 3], 'downsample_kernel_size': 1, 'downsample_padding': 0, 'dropout_p': None, 'groups': 32, 'inplanes': 64, 'input_3x3': False, 'num_classes': 1000, 'reduction': 16}}}</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/encoders/senet/#src.thaw_slump_segmentation.models.encoders.senet.SENetEncoder","title":"<code>SENetEncoder</code>","text":"<p>               Bases: <code>SENet</code>, <code>EncoderMixin</code></p> Source code in <code>src/thaw_slump_segmentation/models/encoders/senet.py</code> <pre><code>class SENetEncoder(SENet, EncoderMixin):\n    def __init__(self, out_channels, depth=5, **kwargs):\n        super().__init__(**kwargs)\n\n        self._out_channels = out_channels\n        self._depth = depth\n        self._in_channels = 3\n\n        del self.last_linear\n        del self.avg_pool\n\n    def get_stages(self):\n        return [\n            nn.Identity(),\n            self.layer0[:-1],\n            nn.Sequential(self.layer0[-1], self.layer1),\n            self.layer2,\n            self.layer3,\n            self.layer4,\n        ]\n\n    def forward(self, x):\n        stages = self.get_stages()\n\n        features = []\n        for i in range(self._depth + 1):\n            x = stages[i](x)\n            features.append(x)\n\n        return features\n\n    def load_state_dict(self, state_dict, **kwargs):\n        state_dict.pop(\"last_linear.bias\")\n        state_dict.pop(\"last_linear.weight\")\n        super().load_state_dict(state_dict, **kwargs)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/senet/#src.thaw_slump_segmentation.models.encoders.senet.SENetEncoder.__init__","title":"<code>__init__(out_channels, depth=5, **kwargs)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/senet.py</code> <pre><code>def __init__(self, out_channels, depth=5, **kwargs):\n    super().__init__(**kwargs)\n\n    self._out_channels = out_channels\n    self._depth = depth\n    self._in_channels = 3\n\n    del self.last_linear\n    del self.avg_pool\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/senet/#src.thaw_slump_segmentation.models.encoders.senet.SENetEncoder.forward","title":"<code>forward(x)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/senet.py</code> <pre><code>def forward(self, x):\n    stages = self.get_stages()\n\n    features = []\n    for i in range(self._depth + 1):\n        x = stages[i](x)\n        features.append(x)\n\n    return features\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/senet/#src.thaw_slump_segmentation.models.encoders.senet.SENetEncoder.get_stages","title":"<code>get_stages()</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/senet.py</code> <pre><code>def get_stages(self):\n    return [\n        nn.Identity(),\n        self.layer0[:-1],\n        nn.Sequential(self.layer0[-1], self.layer1),\n        self.layer2,\n        self.layer3,\n        self.layer4,\n    ]\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/senet/#src.thaw_slump_segmentation.models.encoders.senet.SENetEncoder.load_state_dict","title":"<code>load_state_dict(state_dict, **kwargs)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/senet.py</code> <pre><code>def load_state_dict(self, state_dict, **kwargs):\n    state_dict.pop(\"last_linear.bias\")\n    state_dict.pop(\"last_linear.weight\")\n    super().load_state_dict(state_dict, **kwargs)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/timm_efficientnet/","title":"Timm efficientnet","text":""},{"location":"reference/src/thaw_slump_segmentation/models/encoders/timm_efficientnet/#src.thaw_slump_segmentation.models.encoders.timm_efficientnet.timm_efficientnet_encoders","title":"<code>timm_efficientnet_encoders = {'timm-efficientnet-b0': {'encoder': EfficientNetEncoder, 'pretrained_settings': {'imagenet': prepare_settings(default_cfgs['tf_efficientnet_b0']), 'advprop': prepare_settings(default_cfgs['tf_efficientnet_b0_ap']), 'noisy-student': prepare_settings(default_cfgs['tf_efficientnet_b0_ns'])}, 'params': {'out_channels': (3, 32, 24, 40, 112, 320), 'stage_idxs': (2, 3, 5), 'channel_multiplier': 1.0, 'depth_multiplier': 1.0, 'drop_rate': 0.2}}, 'timm-efficientnet-b1': {'encoder': EfficientNetEncoder, 'pretrained_settings': {'imagenet': prepare_settings(default_cfgs['tf_efficientnet_b1']), 'advprop': prepare_settings(default_cfgs['tf_efficientnet_b1_ap']), 'noisy-student': prepare_settings(default_cfgs['tf_efficientnet_b1_ns'])}, 'params': {'out_channels': (3, 32, 24, 40, 112, 320), 'stage_idxs': (2, 3, 5), 'channel_multiplier': 1.0, 'depth_multiplier': 1.1, 'drop_rate': 0.2}}, 'timm-efficientnet-b2': {'encoder': EfficientNetEncoder, 'pretrained_settings': {'imagenet': prepare_settings(default_cfgs['tf_efficientnet_b2']), 'advprop': prepare_settings(default_cfgs['tf_efficientnet_b2_ap']), 'noisy-student': prepare_settings(default_cfgs['tf_efficientnet_b2_ns'])}, 'params': {'out_channels': (3, 32, 24, 48, 120, 352), 'stage_idxs': (2, 3, 5), 'channel_multiplier': 1.1, 'depth_multiplier': 1.2, 'drop_rate': 0.3}}, 'timm-efficientnet-b3': {'encoder': EfficientNetEncoder, 'pretrained_settings': {'imagenet': prepare_settings(default_cfgs['tf_efficientnet_b3']), 'advprop': prepare_settings(default_cfgs['tf_efficientnet_b3_ap']), 'noisy-student': prepare_settings(default_cfgs['tf_efficientnet_b3_ns'])}, 'params': {'out_channels': (3, 40, 32, 48, 136, 384), 'stage_idxs': (2, 3, 5), 'channel_multiplier': 1.2, 'depth_multiplier': 1.4, 'drop_rate': 0.3}}, 'timm-efficientnet-b4': {'encoder': EfficientNetEncoder, 'pretrained_settings': {'imagenet': prepare_settings(default_cfgs['tf_efficientnet_b4']), 'advprop': prepare_settings(default_cfgs['tf_efficientnet_b4_ap']), 'noisy-student': prepare_settings(default_cfgs['tf_efficientnet_b4_ns'])}, 'params': {'out_channels': (3, 48, 32, 56, 160, 448), 'stage_idxs': (2, 3, 5), 'channel_multiplier': 1.4, 'depth_multiplier': 1.8, 'drop_rate': 0.4}}, 'timm-efficientnet-b5': {'encoder': EfficientNetEncoder, 'pretrained_settings': {'imagenet': prepare_settings(default_cfgs['tf_efficientnet_b5']), 'advprop': prepare_settings(default_cfgs['tf_efficientnet_b5_ap']), 'noisy-student': prepare_settings(default_cfgs['tf_efficientnet_b5_ns'])}, 'params': {'out_channels': (3, 48, 40, 64, 176, 512), 'stage_idxs': (2, 3, 5), 'channel_multiplier': 1.6, 'depth_multiplier': 2.2, 'drop_rate': 0.4}}, 'timm-efficientnet-b6': {'encoder': EfficientNetEncoder, 'pretrained_settings': {'imagenet': prepare_settings(default_cfgs['tf_efficientnet_b6']), 'advprop': prepare_settings(default_cfgs['tf_efficientnet_b6_ap']), 'noisy-student': prepare_settings(default_cfgs['tf_efficientnet_b6_ns'])}, 'params': {'out_channels': (3, 56, 40, 72, 200, 576), 'stage_idxs': (2, 3, 5), 'channel_multiplier': 1.8, 'depth_multiplier': 2.6, 'drop_rate': 0.5}}, 'timm-efficientnet-b7': {'encoder': EfficientNetEncoder, 'pretrained_settings': {'imagenet': prepare_settings(default_cfgs['tf_efficientnet_b7']), 'advprop': prepare_settings(default_cfgs['tf_efficientnet_b7_ap']), 'noisy-student': prepare_settings(default_cfgs['tf_efficientnet_b7_ns'])}, 'params': {'out_channels': (3, 64, 48, 80, 224, 640), 'stage_idxs': (2, 3, 5), 'channel_multiplier': 2.0, 'depth_multiplier': 3.1, 'drop_rate': 0.5}}, 'timm-efficientnet-b8': {'encoder': EfficientNetEncoder, 'pretrained_settings': {'imagenet': prepare_settings(default_cfgs['tf_efficientnet_b8']), 'advprop': prepare_settings(default_cfgs['tf_efficientnet_b8_ap'])}, 'params': {'out_channels': (3, 72, 56, 88, 248, 704), 'stage_idxs': (2, 3, 5), 'channel_multiplier': 2.2, 'depth_multiplier': 3.6, 'drop_rate': 0.5}}, 'timm-efficientnet-l2': {'encoder': EfficientNetEncoder, 'pretrained_settings': {'noisy-student': prepare_settings(default_cfgs['tf_efficientnet_l2_ns'])}, 'params': {'out_channels': (3, 136, 104, 176, 480, 1376), 'stage_idxs': (2, 3, 5), 'channel_multiplier': 4.3, 'depth_multiplier': 5.3, 'drop_rate': 0.5}}, 'timm-tf_efficientnet_lite0': {'encoder': EfficientNetLiteEncoder, 'pretrained_settings': {'imagenet': prepare_settings(default_cfgs['tf_efficientnet_lite0'])}, 'params': {'out_channels': (3, 32, 24, 40, 112, 320), 'stage_idxs': (2, 3, 5), 'channel_multiplier': 1.0, 'depth_multiplier': 1.0, 'drop_rate': 0.2}}, 'timm-tf_efficientnet_lite1': {'encoder': EfficientNetLiteEncoder, 'pretrained_settings': {'imagenet': prepare_settings(default_cfgs['tf_efficientnet_lite1'])}, 'params': {'out_channels': (3, 32, 24, 40, 112, 320), 'stage_idxs': (2, 3, 5), 'channel_multiplier': 1.0, 'depth_multiplier': 1.1, 'drop_rate': 0.2}}, 'timm-tf_efficientnet_lite2': {'encoder': EfficientNetLiteEncoder, 'pretrained_settings': {'imagenet': prepare_settings(default_cfgs['tf_efficientnet_lite2'])}, 'params': {'out_channels': (3, 32, 24, 48, 120, 352), 'stage_idxs': (2, 3, 5), 'channel_multiplier': 1.1, 'depth_multiplier': 1.2, 'drop_rate': 0.3}}, 'timm-tf_efficientnet_lite3': {'encoder': EfficientNetLiteEncoder, 'pretrained_settings': {'imagenet': prepare_settings(default_cfgs['tf_efficientnet_lite3'])}, 'params': {'out_channels': (3, 32, 32, 48, 136, 384), 'stage_idxs': (2, 3, 5), 'channel_multiplier': 1.2, 'depth_multiplier': 1.4, 'drop_rate': 0.3}}, 'timm-tf_efficientnet_lite4': {'encoder': EfficientNetLiteEncoder, 'pretrained_settings': {'imagenet': prepare_settings(default_cfgs['tf_efficientnet_lite4'])}, 'params': {'out_channels': (3, 32, 32, 56, 160, 448), 'stage_idxs': (2, 3, 5), 'channel_multiplier': 1.4, 'depth_multiplier': 1.8, 'drop_rate': 0.4}}}</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/encoders/timm_efficientnet/#src.thaw_slump_segmentation.models.encoders.timm_efficientnet.EfficientNetBaseEncoder","title":"<code>EfficientNetBaseEncoder</code>","text":"<p>               Bases: <code>EfficientNet</code>, <code>EncoderMixin</code></p> Source code in <code>src/thaw_slump_segmentation/models/encoders/timm_efficientnet.py</code> <pre><code>class EfficientNetBaseEncoder(EfficientNet, EncoderMixin):\n\n    def __init__(self, stage_idxs, out_channels, depth=5, **kwargs):\n        super().__init__(**kwargs)\n\n        self._stage_idxs = stage_idxs\n        self._out_channels = out_channels\n        self._depth = depth\n        self._in_channels = 3\n\n        del self.classifier\n\n    def get_stages(self):\n        return [\n            nn.Identity(),\n            nn.Sequential(self.conv_stem, self.bn1, self.act1),\n            self.blocks[:self._stage_idxs[0]],\n            self.blocks[self._stage_idxs[0]:self._stage_idxs[1]],\n            self.blocks[self._stage_idxs[1]:self._stage_idxs[2]],\n            self.blocks[self._stage_idxs[2]:],\n        ]\n\n    def forward(self, x):\n        stages = self.get_stages()\n\n        features = []\n        for i in range(self._depth + 1):\n            x = stages[i](x)\n            features.append(x)\n\n        return features\n\n    def load_state_dict(self, state_dict, **kwargs):\n        state_dict.pop(\"classifier.bias\")\n        state_dict.pop(\"classifier.weight\")\n        super().load_state_dict(state_dict, **kwargs)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/timm_efficientnet/#src.thaw_slump_segmentation.models.encoders.timm_efficientnet.EfficientNetBaseEncoder.__init__","title":"<code>__init__(stage_idxs, out_channels, depth=5, **kwargs)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/timm_efficientnet.py</code> <pre><code>def __init__(self, stage_idxs, out_channels, depth=5, **kwargs):\n    super().__init__(**kwargs)\n\n    self._stage_idxs = stage_idxs\n    self._out_channels = out_channels\n    self._depth = depth\n    self._in_channels = 3\n\n    del self.classifier\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/timm_efficientnet/#src.thaw_slump_segmentation.models.encoders.timm_efficientnet.EfficientNetBaseEncoder.forward","title":"<code>forward(x)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/timm_efficientnet.py</code> <pre><code>def forward(self, x):\n    stages = self.get_stages()\n\n    features = []\n    for i in range(self._depth + 1):\n        x = stages[i](x)\n        features.append(x)\n\n    return features\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/timm_efficientnet/#src.thaw_slump_segmentation.models.encoders.timm_efficientnet.EfficientNetBaseEncoder.get_stages","title":"<code>get_stages()</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/timm_efficientnet.py</code> <pre><code>def get_stages(self):\n    return [\n        nn.Identity(),\n        nn.Sequential(self.conv_stem, self.bn1, self.act1),\n        self.blocks[:self._stage_idxs[0]],\n        self.blocks[self._stage_idxs[0]:self._stage_idxs[1]],\n        self.blocks[self._stage_idxs[1]:self._stage_idxs[2]],\n        self.blocks[self._stage_idxs[2]:],\n    ]\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/timm_efficientnet/#src.thaw_slump_segmentation.models.encoders.timm_efficientnet.EfficientNetBaseEncoder.load_state_dict","title":"<code>load_state_dict(state_dict, **kwargs)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/timm_efficientnet.py</code> <pre><code>def load_state_dict(self, state_dict, **kwargs):\n    state_dict.pop(\"classifier.bias\")\n    state_dict.pop(\"classifier.weight\")\n    super().load_state_dict(state_dict, **kwargs)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/timm_efficientnet/#src.thaw_slump_segmentation.models.encoders.timm_efficientnet.EfficientNetEncoder","title":"<code>EfficientNetEncoder</code>","text":"<p>               Bases: <code>EfficientNetBaseEncoder</code></p> Source code in <code>src/thaw_slump_segmentation/models/encoders/timm_efficientnet.py</code> <pre><code>class EfficientNetEncoder(EfficientNetBaseEncoder):\n\n    def __init__(self, stage_idxs, out_channels, depth=5, channel_multiplier=1.0, depth_multiplier=1.0, drop_rate=0.2):\n        kwargs = get_efficientnet_kwargs(channel_multiplier, depth_multiplier, drop_rate)\n        super().__init__(stage_idxs, out_channels, depth, **kwargs)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/timm_efficientnet/#src.thaw_slump_segmentation.models.encoders.timm_efficientnet.EfficientNetEncoder.__init__","title":"<code>__init__(stage_idxs, out_channels, depth=5, channel_multiplier=1.0, depth_multiplier=1.0, drop_rate=0.2)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/timm_efficientnet.py</code> <pre><code>def __init__(self, stage_idxs, out_channels, depth=5, channel_multiplier=1.0, depth_multiplier=1.0, drop_rate=0.2):\n    kwargs = get_efficientnet_kwargs(channel_multiplier, depth_multiplier, drop_rate)\n    super().__init__(stage_idxs, out_channels, depth, **kwargs)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/timm_efficientnet/#src.thaw_slump_segmentation.models.encoders.timm_efficientnet.EfficientNetLiteEncoder","title":"<code>EfficientNetLiteEncoder</code>","text":"<p>               Bases: <code>EfficientNetBaseEncoder</code></p> Source code in <code>src/thaw_slump_segmentation/models/encoders/timm_efficientnet.py</code> <pre><code>class EfficientNetLiteEncoder(EfficientNetBaseEncoder):\n\n    def __init__(self, stage_idxs, out_channels, depth=5, channel_multiplier=1.0, depth_multiplier=1.0, drop_rate=0.2):\n        kwargs = gen_efficientnet_lite_kwargs(channel_multiplier, depth_multiplier, drop_rate)\n        super().__init__(stage_idxs, out_channels, depth, **kwargs)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/timm_efficientnet/#src.thaw_slump_segmentation.models.encoders.timm_efficientnet.EfficientNetLiteEncoder.__init__","title":"<code>__init__(stage_idxs, out_channels, depth=5, channel_multiplier=1.0, depth_multiplier=1.0, drop_rate=0.2)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/timm_efficientnet.py</code> <pre><code>def __init__(self, stage_idxs, out_channels, depth=5, channel_multiplier=1.0, depth_multiplier=1.0, drop_rate=0.2):\n    kwargs = gen_efficientnet_lite_kwargs(channel_multiplier, depth_multiplier, drop_rate)\n    super().__init__(stage_idxs, out_channels, depth, **kwargs)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/timm_efficientnet/#src.thaw_slump_segmentation.models.encoders.timm_efficientnet.gen_efficientnet_lite_kwargs","title":"<code>gen_efficientnet_lite_kwargs(channel_multiplier=1.0, depth_multiplier=1.0, drop_rate=0.2)</code>","text":"<p>Creates an EfficientNet-Lite model.</p> <p>Ref impl: https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet/lite Paper: https://arxiv.org/abs/1905.11946</p> <p>EfficientNet params name: (channel_multiplier, depth_multiplier, resolution, dropout_rate)   'efficientnet-lite0': (1.0, 1.0, 224, 0.2),   'efficientnet-lite1': (1.0, 1.1, 240, 0.2),   'efficientnet-lite2': (1.1, 1.2, 260, 0.3),   'efficientnet-lite3': (1.2, 1.4, 280, 0.3),   'efficientnet-lite4': (1.4, 1.8, 300, 0.3),</p> <p>Parameters:</p> Name Type Description Default <code>channel_multiplier</code> <p>multiplier to number of channels per layer</p> <code>1.0</code> <code>depth_multiplier</code> <p>multiplier to number of repeats per stage</p> <code>1.0</code> Source code in <code>src/thaw_slump_segmentation/models/encoders/timm_efficientnet.py</code> <pre><code>def gen_efficientnet_lite_kwargs(channel_multiplier=1.0, depth_multiplier=1.0, drop_rate=0.2):\n    \"\"\"Creates an EfficientNet-Lite model.\n\n    Ref impl: https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet/lite\n    Paper: https://arxiv.org/abs/1905.11946\n\n    EfficientNet params\n    name: (channel_multiplier, depth_multiplier, resolution, dropout_rate)\n      'efficientnet-lite0': (1.0, 1.0, 224, 0.2),\n      'efficientnet-lite1': (1.0, 1.1, 240, 0.2),\n      'efficientnet-lite2': (1.1, 1.2, 260, 0.3),\n      'efficientnet-lite3': (1.2, 1.4, 280, 0.3),\n      'efficientnet-lite4': (1.4, 1.8, 300, 0.3),\n\n    Args:\n      channel_multiplier: multiplier to number of channels per layer\n      depth_multiplier: multiplier to number of repeats per stage\n    \"\"\"\n    arch_def = [\n        ['ds_r1_k3_s1_e1_c16'],\n        ['ir_r2_k3_s2_e6_c24'],\n        ['ir_r2_k5_s2_e6_c40'],\n        ['ir_r3_k3_s2_e6_c80'],\n        ['ir_r3_k5_s1_e6_c112'],\n        ['ir_r4_k5_s2_e6_c192'],\n        ['ir_r1_k3_s1_e6_c320'],\n    ]\n    model_kwargs = dict(\n        block_args=decode_arch_def(arch_def, depth_multiplier, fix_first_last=True),\n        num_features=1280,\n        stem_size=32,\n        fix_stem=True,\n        channel_multiplier=channel_multiplier,\n        act_layer=nn.ReLU6,\n        norm_kwargs={},\n        drop_rate=drop_rate,\n        drop_path_rate=0.2,\n    )\n    return model_kwargs\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/timm_efficientnet/#src.thaw_slump_segmentation.models.encoders.timm_efficientnet.get_efficientnet_kwargs","title":"<code>get_efficientnet_kwargs(channel_multiplier=1.0, depth_multiplier=1.0, drop_rate=0.2)</code>","text":"<p>Creates an EfficientNet model. Ref impl: https://github.com/tensorflow/tpu/blob/master/models/official/efficientnet/efficientnet_model.py Paper: https://arxiv.org/abs/1905.11946 EfficientNet params name: (channel_multiplier, depth_multiplier, resolution, dropout_rate) 'efficientnet-b0': (1.0, 1.0, 224, 0.2), 'efficientnet-b1': (1.0, 1.1, 240, 0.2), 'efficientnet-b2': (1.1, 1.2, 260, 0.3), 'efficientnet-b3': (1.2, 1.4, 300, 0.3), 'efficientnet-b4': (1.4, 1.8, 380, 0.4), 'efficientnet-b5': (1.6, 2.2, 456, 0.4), 'efficientnet-b6': (1.8, 2.6, 528, 0.5), 'efficientnet-b7': (2.0, 3.1, 600, 0.5), 'efficientnet-b8': (2.2, 3.6, 672, 0.5), 'efficientnet-l2': (4.3, 5.3, 800, 0.5), Args:   channel_multiplier: multiplier to number of channels per layer   depth_multiplier: multiplier to number of repeats per stage</p> Source code in <code>src/thaw_slump_segmentation/models/encoders/timm_efficientnet.py</code> <pre><code>def get_efficientnet_kwargs(channel_multiplier=1.0, depth_multiplier=1.0, drop_rate=0.2):\n    \"\"\"Creates an EfficientNet model.\n    Ref impl: https://github.com/tensorflow/tpu/blob/master/models/official/efficientnet/efficientnet_model.py\n    Paper: https://arxiv.org/abs/1905.11946\n    EfficientNet params\n    name: (channel_multiplier, depth_multiplier, resolution, dropout_rate)\n    'efficientnet-b0': (1.0, 1.0, 224, 0.2),\n    'efficientnet-b1': (1.0, 1.1, 240, 0.2),\n    'efficientnet-b2': (1.1, 1.2, 260, 0.3),\n    'efficientnet-b3': (1.2, 1.4, 300, 0.3),\n    'efficientnet-b4': (1.4, 1.8, 380, 0.4),\n    'efficientnet-b5': (1.6, 2.2, 456, 0.4),\n    'efficientnet-b6': (1.8, 2.6, 528, 0.5),\n    'efficientnet-b7': (2.0, 3.1, 600, 0.5),\n    'efficientnet-b8': (2.2, 3.6, 672, 0.5),\n    'efficientnet-l2': (4.3, 5.3, 800, 0.5),\n    Args:\n      channel_multiplier: multiplier to number of channels per layer\n      depth_multiplier: multiplier to number of repeats per stage\n    \"\"\"\n    arch_def = [\n        ['ds_r1_k3_s1_e1_c16_se0.25'],\n        ['ir_r2_k3_s2_e6_c24_se0.25'],\n        ['ir_r2_k5_s2_e6_c40_se0.25'],\n        ['ir_r3_k3_s2_e6_c80_se0.25'],\n        ['ir_r3_k5_s1_e6_c112_se0.25'],\n        ['ir_r4_k5_s2_e6_c192_se0.25'],\n        ['ir_r1_k3_s1_e6_c320_se0.25'],\n    ]\n    model_kwargs = dict(\n        block_args=decode_arch_def(arch_def, depth_multiplier),\n        num_features=round_channels(1280, channel_multiplier, 8, None),\n        stem_size=32,\n        channel_multiplier=channel_multiplier,\n        act_layer=Swish,\n        norm_kwargs={},  # TODO: check\n        drop_rate=drop_rate,\n        drop_path_rate=0.2,\n    )\n    return model_kwargs\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/timm_efficientnet/#src.thaw_slump_segmentation.models.encoders.timm_efficientnet.prepare_settings","title":"<code>prepare_settings(settings)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/timm_efficientnet.py</code> <pre><code>def prepare_settings(settings):\n    return {\n        \"mean\": settings[\"mean\"],\n        \"std\": settings[\"std\"],\n        \"url\": settings[\"url\"],\n        \"input_range\": (0, 1),\n        \"input_space\": \"RGB\",\n    }\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/timm_regnet/","title":"Timm regnet","text":""},{"location":"reference/src/thaw_slump_segmentation/models/encoders/timm_regnet/#src.thaw_slump_segmentation.models.encoders.timm_regnet.pretrained_settings","title":"<code>pretrained_settings = {}</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/encoders/timm_regnet/#src.thaw_slump_segmentation.models.encoders.timm_regnet.regnet_weights","title":"<code>regnet_weights = {'timm-regnetx_002': {'imagenet': 'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnetx_002-e7e85e5c.pth'}, 'timm-regnetx_004': {'imagenet': 'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnetx_004-7d0e9424.pth'}, 'timm-regnetx_006': {'imagenet': 'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnetx_006-85ec1baa.pth'}, 'timm-regnetx_008': {'imagenet': 'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnetx_008-d8b470eb.pth'}, 'timm-regnetx_016': {'imagenet': 'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnetx_016-65ca972a.pth'}, 'timm-regnetx_032': {'imagenet': 'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnetx_032-ed0c7f7e.pth'}, 'timm-regnetx_040': {'imagenet': 'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnetx_040-73c2a654.pth'}, 'timm-regnetx_064': {'imagenet': 'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnetx_064-29278baa.pth'}, 'timm-regnetx_080': {'imagenet': 'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnetx_080-7c7fcab1.pth'}, 'timm-regnetx_120': {'imagenet': 'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnetx_120-65d5521e.pth'}, 'timm-regnetx_160': {'imagenet': 'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnetx_160-c98c4112.pth'}, 'timm-regnetx_320': {'imagenet': 'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnetx_320-8ea38b93.pth'}, 'timm-regnety_002': {'imagenet': 'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_002-e68ca334.pth'}, 'timm-regnety_004': {'imagenet': 'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_004-0db870e6.pth'}, 'timm-regnety_006': {'imagenet': 'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_006-c67e57ec.pth'}, 'timm-regnety_008': {'imagenet': 'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_008-dc900dbe.pth'}, 'timm-regnety_016': {'imagenet': 'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_016-54367f74.pth'}, 'timm-regnety_032': {'imagenet': 'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/regnety_032_ra-7f2439f9.pth'}, 'timm-regnety_040': {'imagenet': 'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_040-f0d569f9.pth'}, 'timm-regnety_064': {'imagenet': 'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_064-0a48325c.pth'}, 'timm-regnety_080': {'imagenet': 'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_080-e7f3eb93.pth'}, 'timm-regnety_120': {'imagenet': 'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_120-721ba79a.pth'}, 'timm-regnety_160': {'imagenet': 'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_160-d64013cd.pth'}, 'timm-regnety_320': {'imagenet': 'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_320-ba464b29.pth'}}</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/encoders/timm_regnet/#src.thaw_slump_segmentation.models.encoders.timm_regnet.timm_regnet_encoders","title":"<code>timm_regnet_encoders = {'timm-regnetx_002': {'encoder': RegNetEncoder, 'pretrained_settings': pretrained_settings['timm-regnetx_002'], 'params': {'out_channels': (3, 32, 24, 56, 152, 368), 'cfg': _mcfg(w0=24, wa=36.44, wm=2.49, group_w=8, depth=13)}}, 'timm-regnetx_004': {'encoder': RegNetEncoder, 'pretrained_settings': pretrained_settings['timm-regnetx_004'], 'params': {'out_channels': (3, 32, 32, 64, 160, 384), 'cfg': _mcfg(w0=24, wa=24.48, wm=2.54, group_w=16, depth=22)}}, 'timm-regnetx_006': {'encoder': RegNetEncoder, 'pretrained_settings': pretrained_settings['timm-regnetx_006'], 'params': {'out_channels': (3, 32, 48, 96, 240, 528), 'cfg': _mcfg(w0=48, wa=36.97, wm=2.24, group_w=24, depth=16)}}, 'timm-regnetx_008': {'encoder': RegNetEncoder, 'pretrained_settings': pretrained_settings['timm-regnetx_008'], 'params': {'out_channels': (3, 32, 64, 128, 288, 672), 'cfg': _mcfg(w0=56, wa=35.73, wm=2.28, group_w=16, depth=16)}}, 'timm-regnetx_016': {'encoder': RegNetEncoder, 'pretrained_settings': pretrained_settings['timm-regnetx_016'], 'params': {'out_channels': (3, 32, 72, 168, 408, 912), 'cfg': _mcfg(w0=80, wa=34.01, wm=2.25, group_w=24, depth=18)}}, 'timm-regnetx_032': {'encoder': RegNetEncoder, 'pretrained_settings': pretrained_settings['timm-regnetx_032'], 'params': {'out_channels': (3, 32, 96, 192, 432, 1008), 'cfg': _mcfg(w0=88, wa=26.31, wm=2.25, group_w=48, depth=25)}}, 'timm-regnetx_040': {'encoder': RegNetEncoder, 'pretrained_settings': pretrained_settings['timm-regnetx_040'], 'params': {'out_channels': (3, 32, 80, 240, 560, 1360), 'cfg': _mcfg(w0=96, wa=38.65, wm=2.43, group_w=40, depth=23)}}, 'timm-regnetx_064': {'encoder': RegNetEncoder, 'pretrained_settings': pretrained_settings['timm-regnetx_064'], 'params': {'out_channels': (3, 32, 168, 392, 784, 1624), 'cfg': _mcfg(w0=184, wa=60.83, wm=2.07, group_w=56, depth=17)}}, 'timm-regnetx_080': {'encoder': RegNetEncoder, 'pretrained_settings': pretrained_settings['timm-regnetx_080'], 'params': {'out_channels': (3, 32, 80, 240, 720, 1920), 'cfg': _mcfg(w0=80, wa=49.56, wm=2.88, group_w=120, depth=23)}}, 'timm-regnetx_120': {'encoder': RegNetEncoder, 'pretrained_settings': pretrained_settings['timm-regnetx_120'], 'params': {'out_channels': (3, 32, 224, 448, 896, 2240), 'cfg': _mcfg(w0=168, wa=73.36, wm=2.37, group_w=112, depth=19)}}, 'timm-regnetx_160': {'encoder': RegNetEncoder, 'pretrained_settings': pretrained_settings['timm-regnetx_160'], 'params': {'out_channels': (3, 32, 256, 512, 896, 2048), 'cfg': _mcfg(w0=216, wa=55.59, wm=2.1, group_w=128, depth=22)}}, 'timm-regnetx_320': {'encoder': RegNetEncoder, 'pretrained_settings': pretrained_settings['timm-regnetx_320'], 'params': {'out_channels': (3, 32, 336, 672, 1344, 2520), 'cfg': _mcfg(w0=320, wa=69.86, wm=2.0, group_w=168, depth=23)}}, 'timm-regnety_002': {'encoder': RegNetEncoder, 'pretrained_settings': pretrained_settings['timm-regnety_002'], 'params': {'out_channels': (3, 32, 24, 56, 152, 368), 'cfg': _mcfg(w0=24, wa=36.44, wm=2.49, group_w=8, depth=13, se_ratio=0.25)}}, 'timm-regnety_004': {'encoder': RegNetEncoder, 'pretrained_settings': pretrained_settings['timm-regnety_004'], 'params': {'out_channels': (3, 32, 48, 104, 208, 440), 'cfg': _mcfg(w0=48, wa=27.89, wm=2.09, group_w=8, depth=16, se_ratio=0.25)}}, 'timm-regnety_006': {'encoder': RegNetEncoder, 'pretrained_settings': pretrained_settings['timm-regnety_006'], 'params': {'out_channels': (3, 32, 48, 112, 256, 608), 'cfg': _mcfg(w0=48, wa=32.54, wm=2.32, group_w=16, depth=15, se_ratio=0.25)}}, 'timm-regnety_008': {'encoder': RegNetEncoder, 'pretrained_settings': pretrained_settings['timm-regnety_008'], 'params': {'out_channels': (3, 32, 64, 128, 320, 768), 'cfg': _mcfg(w0=56, wa=38.84, wm=2.4, group_w=16, depth=14, se_ratio=0.25)}}, 'timm-regnety_016': {'encoder': RegNetEncoder, 'pretrained_settings': pretrained_settings['timm-regnety_016'], 'params': {'out_channels': (3, 32, 48, 120, 336, 888), 'cfg': _mcfg(w0=48, wa=20.71, wm=2.65, group_w=24, depth=27, se_ratio=0.25)}}, 'timm-regnety_032': {'encoder': RegNetEncoder, 'pretrained_settings': pretrained_settings['timm-regnety_032'], 'params': {'out_channels': (3, 32, 72, 216, 576, 1512), 'cfg': _mcfg(w0=80, wa=42.63, wm=2.66, group_w=24, depth=21, se_ratio=0.25)}}, 'timm-regnety_040': {'encoder': RegNetEncoder, 'pretrained_settings': pretrained_settings['timm-regnety_040'], 'params': {'out_channels': (3, 32, 128, 192, 512, 1088), 'cfg': _mcfg(w0=96, wa=31.41, wm=2.24, group_w=64, depth=22, se_ratio=0.25)}}, 'timm-regnety_064': {'encoder': RegNetEncoder, 'pretrained_settings': pretrained_settings['timm-regnety_064'], 'params': {'out_channels': (3, 32, 144, 288, 576, 1296), 'cfg': _mcfg(w0=112, wa=33.22, wm=2.27, group_w=72, depth=25, se_ratio=0.25)}}, 'timm-regnety_080': {'encoder': RegNetEncoder, 'pretrained_settings': pretrained_settings['timm-regnety_080'], 'params': {'out_channels': (3, 32, 168, 448, 896, 2016), 'cfg': _mcfg(w0=192, wa=76.82, wm=2.19, group_w=56, depth=17, se_ratio=0.25)}}, 'timm-regnety_120': {'encoder': RegNetEncoder, 'pretrained_settings': pretrained_settings['timm-regnety_120'], 'params': {'out_channels': (3, 32, 224, 448, 896, 2240), 'cfg': _mcfg(w0=168, wa=73.36, wm=2.37, group_w=112, depth=19, se_ratio=0.25)}}, 'timm-regnety_160': {'encoder': RegNetEncoder, 'pretrained_settings': pretrained_settings['timm-regnety_160'], 'params': {'out_channels': (3, 32, 224, 448, 1232, 3024), 'cfg': _mcfg(w0=200, wa=106.23, wm=2.48, group_w=112, depth=18, se_ratio=0.25)}}, 'timm-regnety_320': {'encoder': RegNetEncoder, 'pretrained_settings': pretrained_settings['timm-regnety_320'], 'params': {'out_channels': (3, 32, 232, 696, 1392, 3712), 'cfg': _mcfg(w0=232, wa=115.89, wm=2.53, group_w=232, depth=20, se_ratio=0.25)}}}</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/encoders/timm_regnet/#src.thaw_slump_segmentation.models.encoders.timm_regnet.RegNetEncoder","title":"<code>RegNetEncoder</code>","text":"<p>               Bases: <code>RegNet</code>, <code>EncoderMixin</code></p> Source code in <code>src/thaw_slump_segmentation/models/encoders/timm_regnet.py</code> <pre><code>class RegNetEncoder(RegNet, EncoderMixin):\n    def __init__(self, out_channels, depth=5, **kwargs):\n        super().__init__(**kwargs)\n        self._depth = depth\n        self._out_channels = out_channels\n        self._in_channels = 3\n\n        del self.head\n\n    def get_stages(self):\n        return [\n            nn.Identity(),\n            self.stem,\n            self.s1,\n            self.s2,\n            self.s3,\n            self.s4,\n        ]\n\n    def forward(self, x):\n        stages = self.get_stages()\n\n        features = []\n        for i in range(self._depth + 1):\n            x = stages[i](x)\n            features.append(x)\n\n        return features\n\n    def load_state_dict(self, state_dict, **kwargs):\n        state_dict.pop(\"head.fc.weight\")\n        state_dict.pop(\"head.fc.bias\")\n        super().load_state_dict(state_dict, **kwargs)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/timm_regnet/#src.thaw_slump_segmentation.models.encoders.timm_regnet.RegNetEncoder.__init__","title":"<code>__init__(out_channels, depth=5, **kwargs)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/timm_regnet.py</code> <pre><code>def __init__(self, out_channels, depth=5, **kwargs):\n    super().__init__(**kwargs)\n    self._depth = depth\n    self._out_channels = out_channels\n    self._in_channels = 3\n\n    del self.head\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/timm_regnet/#src.thaw_slump_segmentation.models.encoders.timm_regnet.RegNetEncoder.forward","title":"<code>forward(x)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/timm_regnet.py</code> <pre><code>def forward(self, x):\n    stages = self.get_stages()\n\n    features = []\n    for i in range(self._depth + 1):\n        x = stages[i](x)\n        features.append(x)\n\n    return features\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/timm_regnet/#src.thaw_slump_segmentation.models.encoders.timm_regnet.RegNetEncoder.get_stages","title":"<code>get_stages()</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/timm_regnet.py</code> <pre><code>def get_stages(self):\n    return [\n        nn.Identity(),\n        self.stem,\n        self.s1,\n        self.s2,\n        self.s3,\n        self.s4,\n    ]\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/timm_regnet/#src.thaw_slump_segmentation.models.encoders.timm_regnet.RegNetEncoder.load_state_dict","title":"<code>load_state_dict(state_dict, **kwargs)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/timm_regnet.py</code> <pre><code>def load_state_dict(self, state_dict, **kwargs):\n    state_dict.pop(\"head.fc.weight\")\n    state_dict.pop(\"head.fc.bias\")\n    super().load_state_dict(state_dict, **kwargs)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/timm_res2net/","title":"Timm res2net","text":""},{"location":"reference/src/thaw_slump_segmentation/models/encoders/timm_res2net/#src.thaw_slump_segmentation.models.encoders.timm_res2net.pretrained_settings","title":"<code>pretrained_settings = {}</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/encoders/timm_res2net/#src.thaw_slump_segmentation.models.encoders.timm_res2net.res2net_weights","title":"<code>res2net_weights = {'timm-res2net50_26w_4s': {'imagenet': 'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-res2net/res2net50_26w_4s-06e79181.pth'}, 'timm-res2net50_48w_2s': {'imagenet': 'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-res2net/res2net50_48w_2s-afed724a.pth'}, 'timm-res2net50_14w_8s': {'imagenet': 'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-res2net/res2net50_14w_8s-6527dddc.pth'}, 'timm-res2net50_26w_6s': {'imagenet': 'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-res2net/res2net50_26w_6s-19041792.pth'}, 'timm-res2net50_26w_8s': {'imagenet': 'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-res2net/res2net50_26w_8s-2c7c9f12.pth'}, 'timm-res2net101_26w_4s': {'imagenet': 'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-res2net/res2net101_26w_4s-02a759a1.pth'}, 'timm-res2next50': {'imagenet': 'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-res2net/res2next50_4s-6ef7e7bf.pth'}}</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/encoders/timm_res2net/#src.thaw_slump_segmentation.models.encoders.timm_res2net.timm_res2net_encoders","title":"<code>timm_res2net_encoders = {'timm-res2net50_26w_4s': {'encoder': Res2NetEncoder, 'pretrained_settings': pretrained_settings['timm-res2net50_26w_4s'], 'params': {'out_channels': (3, 64, 256, 512, 1024, 2048), 'block': Bottle2neck, 'layers': [3, 4, 6, 3], 'base_width': 26, 'block_args': {'scale': 4}}}, 'timm-res2net101_26w_4s': {'encoder': Res2NetEncoder, 'pretrained_settings': pretrained_settings['timm-res2net101_26w_4s'], 'params': {'out_channels': (3, 64, 256, 512, 1024, 2048), 'block': Bottle2neck, 'layers': [3, 4, 23, 3], 'base_width': 26, 'block_args': {'scale': 4}}}, 'timm-res2net50_26w_6s': {'encoder': Res2NetEncoder, 'pretrained_settings': pretrained_settings['timm-res2net50_26w_6s'], 'params': {'out_channels': (3, 64, 256, 512, 1024, 2048), 'block': Bottle2neck, 'layers': [3, 4, 6, 3], 'base_width': 26, 'block_args': {'scale': 6}}}, 'timm-res2net50_26w_8s': {'encoder': Res2NetEncoder, 'pretrained_settings': pretrained_settings['timm-res2net50_26w_8s'], 'params': {'out_channels': (3, 64, 256, 512, 1024, 2048), 'block': Bottle2neck, 'layers': [3, 4, 6, 3], 'base_width': 26, 'block_args': {'scale': 8}}}, 'timm-res2net50_48w_2s': {'encoder': Res2NetEncoder, 'pretrained_settings': pretrained_settings['timm-res2net50_48w_2s'], 'params': {'out_channels': (3, 64, 256, 512, 1024, 2048), 'block': Bottle2neck, 'layers': [3, 4, 6, 3], 'base_width': 48, 'block_args': {'scale': 2}}}, 'timm-res2net50_14w_8s': {'encoder': Res2NetEncoder, 'pretrained_settings': pretrained_settings['timm-res2net50_14w_8s'], 'params': {'out_channels': (3, 64, 256, 512, 1024, 2048), 'block': Bottle2neck, 'layers': [3, 4, 6, 3], 'base_width': 14, 'block_args': {'scale': 8}}}, 'timm-res2next50': {'encoder': Res2NetEncoder, 'pretrained_settings': pretrained_settings['timm-res2next50'], 'params': {'out_channels': (3, 64, 256, 512, 1024, 2048), 'block': Bottle2neck, 'layers': [3, 4, 6, 3], 'base_width': 4, 'cardinality': 8, 'block_args': {'scale': 4}}}}</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/encoders/timm_res2net/#src.thaw_slump_segmentation.models.encoders.timm_res2net.Res2NetEncoder","title":"<code>Res2NetEncoder</code>","text":"<p>               Bases: <code>ResNet</code>, <code>EncoderMixin</code></p> Source code in <code>src/thaw_slump_segmentation/models/encoders/timm_res2net.py</code> <pre><code>class Res2NetEncoder(ResNet, EncoderMixin):\n    def __init__(self, out_channels, depth=5, **kwargs):\n        super().__init__(**kwargs)\n        self._depth = depth\n        self._out_channels = out_channels\n        self._in_channels = 3\n\n        del self.fc\n        del self.global_pool\n\n    def get_stages(self):\n        return [\n            nn.Identity(),\n            nn.Sequential(self.conv1, self.bn1, self.act1),\n            nn.Sequential(self.maxpool, self.layer1),\n            self.layer2,\n            self.layer3,\n            self.layer4,\n        ]\n\n    def make_dilated(self, stage_list, dilation_list):\n        raise ValueError(\"Res2Net encoders do not support dilated mode\")\n\n    def forward(self, x):\n        stages = self.get_stages()\n\n        features = []\n        for i in range(self._depth + 1):\n            x = stages[i](x)\n            features.append(x)\n\n        return features\n\n    def load_state_dict(self, state_dict, **kwargs):\n        state_dict.pop(\"fc.bias\")\n        state_dict.pop(\"fc.weight\")\n        super().load_state_dict(state_dict, **kwargs)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/timm_res2net/#src.thaw_slump_segmentation.models.encoders.timm_res2net.Res2NetEncoder.__init__","title":"<code>__init__(out_channels, depth=5, **kwargs)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/timm_res2net.py</code> <pre><code>def __init__(self, out_channels, depth=5, **kwargs):\n    super().__init__(**kwargs)\n    self._depth = depth\n    self._out_channels = out_channels\n    self._in_channels = 3\n\n    del self.fc\n    del self.global_pool\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/timm_res2net/#src.thaw_slump_segmentation.models.encoders.timm_res2net.Res2NetEncoder.forward","title":"<code>forward(x)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/timm_res2net.py</code> <pre><code>def forward(self, x):\n    stages = self.get_stages()\n\n    features = []\n    for i in range(self._depth + 1):\n        x = stages[i](x)\n        features.append(x)\n\n    return features\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/timm_res2net/#src.thaw_slump_segmentation.models.encoders.timm_res2net.Res2NetEncoder.get_stages","title":"<code>get_stages()</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/timm_res2net.py</code> <pre><code>def get_stages(self):\n    return [\n        nn.Identity(),\n        nn.Sequential(self.conv1, self.bn1, self.act1),\n        nn.Sequential(self.maxpool, self.layer1),\n        self.layer2,\n        self.layer3,\n        self.layer4,\n    ]\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/timm_res2net/#src.thaw_slump_segmentation.models.encoders.timm_res2net.Res2NetEncoder.load_state_dict","title":"<code>load_state_dict(state_dict, **kwargs)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/timm_res2net.py</code> <pre><code>def load_state_dict(self, state_dict, **kwargs):\n    state_dict.pop(\"fc.bias\")\n    state_dict.pop(\"fc.weight\")\n    super().load_state_dict(state_dict, **kwargs)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/timm_res2net/#src.thaw_slump_segmentation.models.encoders.timm_res2net.Res2NetEncoder.make_dilated","title":"<code>make_dilated(stage_list, dilation_list)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/timm_res2net.py</code> <pre><code>def make_dilated(self, stage_list, dilation_list):\n    raise ValueError(\"Res2Net encoders do not support dilated mode\")\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/timm_resnest/","title":"Timm resnest","text":""},{"location":"reference/src/thaw_slump_segmentation/models/encoders/timm_resnest/#src.thaw_slump_segmentation.models.encoders.timm_resnest.pretrained_settings","title":"<code>pretrained_settings = {}</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/encoders/timm_resnest/#src.thaw_slump_segmentation.models.encoders.timm_resnest.resnest_weights","title":"<code>resnest_weights = {'timm-resnest14d': {'imagenet': 'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/gluon_resnest14-9c8fe254.pth'}, 'timm-resnest26d': {'imagenet': 'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/gluon_resnest26-50eb607c.pth'}, 'timm-resnest50d': {'imagenet': 'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-resnest/resnest50-528c19ca.pth'}, 'timm-resnest101e': {'imagenet': 'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-resnest/resnest101-22405ba7.pth'}, 'timm-resnest200e': {'imagenet': 'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-resnest/resnest200-75117900.pth'}, 'timm-resnest269e': {'imagenet': 'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-resnest/resnest269-0cc87c48.pth'}, 'timm-resnest50d_4s2x40d': {'imagenet': 'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-resnest/resnest50_fast_4s2x40d-41d14ed0.pth'}, 'timm-resnest50d_1s4x24d': {'imagenet': 'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-resnest/resnest50_fast_1s4x24d-d4a4f76f.pth'}}</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/encoders/timm_resnest/#src.thaw_slump_segmentation.models.encoders.timm_resnest.timm_resnest_encoders","title":"<code>timm_resnest_encoders = {'timm-resnest14d': {'encoder': ResNestEncoder, 'pretrained_settings': pretrained_settings['timm-resnest14d'], 'params': {'out_channels': (3, 64, 256, 512, 1024, 2048), 'block': ResNestBottleneck, 'layers': [1, 1, 1, 1], 'stem_type': 'deep', 'stem_width': 32, 'avg_down': True, 'base_width': 64, 'cardinality': 1, 'block_args': {'radix': 2, 'avd': True, 'avd_first': False}}}, 'timm-resnest26d': {'encoder': ResNestEncoder, 'pretrained_settings': pretrained_settings['timm-resnest26d'], 'params': {'out_channels': (3, 64, 256, 512, 1024, 2048), 'block': ResNestBottleneck, 'layers': [2, 2, 2, 2], 'stem_type': 'deep', 'stem_width': 32, 'avg_down': True, 'base_width': 64, 'cardinality': 1, 'block_args': {'radix': 2, 'avd': True, 'avd_first': False}}}, 'timm-resnest50d': {'encoder': ResNestEncoder, 'pretrained_settings': pretrained_settings['timm-resnest50d'], 'params': {'out_channels': (3, 64, 256, 512, 1024, 2048), 'block': ResNestBottleneck, 'layers': [3, 4, 6, 3], 'stem_type': 'deep', 'stem_width': 32, 'avg_down': True, 'base_width': 64, 'cardinality': 1, 'block_args': {'radix': 2, 'avd': True, 'avd_first': False}}}, 'timm-resnest101e': {'encoder': ResNestEncoder, 'pretrained_settings': pretrained_settings['timm-resnest101e'], 'params': {'out_channels': (3, 128, 256, 512, 1024, 2048), 'block': ResNestBottleneck, 'layers': [3, 4, 23, 3], 'stem_type': 'deep', 'stem_width': 64, 'avg_down': True, 'base_width': 64, 'cardinality': 1, 'block_args': {'radix': 2, 'avd': True, 'avd_first': False}}}, 'timm-resnest200e': {'encoder': ResNestEncoder, 'pretrained_settings': pretrained_settings['timm-resnest200e'], 'params': {'out_channels': (3, 128, 256, 512, 1024, 2048), 'block': ResNestBottleneck, 'layers': [3, 24, 36, 3], 'stem_type': 'deep', 'stem_width': 64, 'avg_down': True, 'base_width': 64, 'cardinality': 1, 'block_args': {'radix': 2, 'avd': True, 'avd_first': False}}}, 'timm-resnest269e': {'encoder': ResNestEncoder, 'pretrained_settings': pretrained_settings['timm-resnest269e'], 'params': {'out_channels': (3, 128, 256, 512, 1024, 2048), 'block': ResNestBottleneck, 'layers': [3, 30, 48, 8], 'stem_type': 'deep', 'stem_width': 64, 'avg_down': True, 'base_width': 64, 'cardinality': 1, 'block_args': {'radix': 2, 'avd': True, 'avd_first': False}}}, 'timm-resnest50d_4s2x40d': {'encoder': ResNestEncoder, 'pretrained_settings': pretrained_settings['timm-resnest50d_4s2x40d'], 'params': {'out_channels': (3, 64, 256, 512, 1024, 2048), 'block': ResNestBottleneck, 'layers': [3, 4, 6, 3], 'stem_type': 'deep', 'stem_width': 32, 'avg_down': True, 'base_width': 40, 'cardinality': 2, 'block_args': {'radix': 4, 'avd': True, 'avd_first': True}}}, 'timm-resnest50d_1s4x24d': {'encoder': ResNestEncoder, 'pretrained_settings': pretrained_settings['timm-resnest50d_1s4x24d'], 'params': {'out_channels': (3, 64, 256, 512, 1024, 2048), 'block': ResNestBottleneck, 'layers': [3, 4, 6, 3], 'stem_type': 'deep', 'stem_width': 32, 'avg_down': True, 'base_width': 24, 'cardinality': 4, 'block_args': {'radix': 1, 'avd': True, 'avd_first': True}}}}</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/encoders/timm_resnest/#src.thaw_slump_segmentation.models.encoders.timm_resnest.ResNestEncoder","title":"<code>ResNestEncoder</code>","text":"<p>               Bases: <code>ResNet</code>, <code>EncoderMixin</code></p> Source code in <code>src/thaw_slump_segmentation/models/encoders/timm_resnest.py</code> <pre><code>class ResNestEncoder(ResNet, EncoderMixin):\n    def __init__(self, out_channels, depth=5, **kwargs):\n        super().__init__(**kwargs)\n        self._depth = depth\n        self._out_channels = out_channels\n        self._in_channels = 3\n\n        del self.fc\n        del self.global_pool\n\n    def get_stages(self):\n        return [\n            nn.Identity(),\n            nn.Sequential(self.conv1, self.bn1, self.act1),\n            nn.Sequential(self.maxpool, self.layer1),\n            self.layer2,\n            self.layer3,\n            self.layer4,\n        ]\n\n    def make_dilated(self, stage_list, dilation_list):\n        raise ValueError(\"ResNest encoders do not support dilated mode\")\n\n    def forward(self, x):\n        stages = self.get_stages()\n\n        features = []\n        for i in range(self._depth + 1):\n            x = stages[i](x)\n            features.append(x)\n\n        return features\n\n    def load_state_dict(self, state_dict, **kwargs):\n        state_dict.pop(\"fc.bias\")\n        state_dict.pop(\"fc.weight\")\n        super().load_state_dict(state_dict, **kwargs)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/timm_resnest/#src.thaw_slump_segmentation.models.encoders.timm_resnest.ResNestEncoder.__init__","title":"<code>__init__(out_channels, depth=5, **kwargs)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/timm_resnest.py</code> <pre><code>def __init__(self, out_channels, depth=5, **kwargs):\n    super().__init__(**kwargs)\n    self._depth = depth\n    self._out_channels = out_channels\n    self._in_channels = 3\n\n    del self.fc\n    del self.global_pool\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/timm_resnest/#src.thaw_slump_segmentation.models.encoders.timm_resnest.ResNestEncoder.forward","title":"<code>forward(x)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/timm_resnest.py</code> <pre><code>def forward(self, x):\n    stages = self.get_stages()\n\n    features = []\n    for i in range(self._depth + 1):\n        x = stages[i](x)\n        features.append(x)\n\n    return features\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/timm_resnest/#src.thaw_slump_segmentation.models.encoders.timm_resnest.ResNestEncoder.get_stages","title":"<code>get_stages()</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/timm_resnest.py</code> <pre><code>def get_stages(self):\n    return [\n        nn.Identity(),\n        nn.Sequential(self.conv1, self.bn1, self.act1),\n        nn.Sequential(self.maxpool, self.layer1),\n        self.layer2,\n        self.layer3,\n        self.layer4,\n    ]\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/timm_resnest/#src.thaw_slump_segmentation.models.encoders.timm_resnest.ResNestEncoder.load_state_dict","title":"<code>load_state_dict(state_dict, **kwargs)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/timm_resnest.py</code> <pre><code>def load_state_dict(self, state_dict, **kwargs):\n    state_dict.pop(\"fc.bias\")\n    state_dict.pop(\"fc.weight\")\n    super().load_state_dict(state_dict, **kwargs)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/timm_resnest/#src.thaw_slump_segmentation.models.encoders.timm_resnest.ResNestEncoder.make_dilated","title":"<code>make_dilated(stage_list, dilation_list)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/timm_resnest.py</code> <pre><code>def make_dilated(self, stage_list, dilation_list):\n    raise ValueError(\"ResNest encoders do not support dilated mode\")\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/timm_sknet/","title":"Timm sknet","text":""},{"location":"reference/src/thaw_slump_segmentation/models/encoders/timm_sknet/#src.thaw_slump_segmentation.models.encoders.timm_sknet.pretrained_settings","title":"<code>pretrained_settings = {}</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/encoders/timm_sknet/#src.thaw_slump_segmentation.models.encoders.timm_sknet.sknet_weights","title":"<code>sknet_weights = {'timm-skresnet18': {'imagenet': 'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/skresnet18_ra-4eec2804.pth'}, 'timm-skresnet34': {'imagenet': 'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/skresnet34_ra-bdc0ccde.pth'}, 'timm-skresnext50_32x4d': {'imagenet': 'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/skresnext50_ra-f40e40bf.pth'}}</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/encoders/timm_sknet/#src.thaw_slump_segmentation.models.encoders.timm_sknet.timm_sknet_encoders","title":"<code>timm_sknet_encoders = {'timm-skresnet18': {'encoder': SkNetEncoder, 'pretrained_settings': pretrained_settings['timm-skresnet18'], 'params': {'out_channels': (3, 64, 64, 128, 256, 512), 'block': SelectiveKernelBasic, 'layers': [2, 2, 2, 2], 'zero_init_last_bn': False, 'block_args': {'sk_kwargs': {'min_attn_channels': 16, 'attn_reduction': 8, 'split_input': True}}}}, 'timm-skresnet34': {'encoder': SkNetEncoder, 'pretrained_settings': pretrained_settings['timm-skresnet34'], 'params': {'out_channels': (3, 64, 64, 128, 256, 512), 'block': SelectiveKernelBasic, 'layers': [3, 4, 6, 3], 'zero_init_last_bn': False, 'block_args': {'sk_kwargs': {'min_attn_channels': 16, 'attn_reduction': 8, 'split_input': True}}}}, 'timm-skresnext50_32x4d': {'encoder': SkNetEncoder, 'pretrained_settings': pretrained_settings['timm-skresnext50_32x4d'], 'params': {'out_channels': (3, 64, 256, 512, 1024, 2048), 'block': SelectiveKernelBottleneck, 'layers': [3, 4, 6, 3], 'zero_init_last_bn': False, 'cardinality': 32, 'base_width': 4}}}</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/encoders/timm_sknet/#src.thaw_slump_segmentation.models.encoders.timm_sknet.SkNetEncoder","title":"<code>SkNetEncoder</code>","text":"<p>               Bases: <code>ResNet</code>, <code>EncoderMixin</code></p> Source code in <code>src/thaw_slump_segmentation/models/encoders/timm_sknet.py</code> <pre><code>class SkNetEncoder(ResNet, EncoderMixin):\n    def __init__(self, out_channels, depth=5, **kwargs):\n        super().__init__(**kwargs)\n        self._depth = depth\n        self._out_channels = out_channels\n        self._in_channels = 3\n\n        del self.fc\n        del self.global_pool\n\n    def get_stages(self):\n        return [\n            nn.Identity(),\n            nn.Sequential(self.conv1, self.bn1, self.act1),\n            nn.Sequential(self.maxpool, self.layer1),\n            self.layer2,\n            self.layer3,\n            self.layer4,\n        ]\n\n    def forward(self, x):\n        stages = self.get_stages()\n\n        features = []\n        for i in range(self._depth + 1):\n            x = stages[i](x)\n            features.append(x)\n\n        return features\n\n    def load_state_dict(self, state_dict, **kwargs):\n        state_dict.pop(\"fc.bias\")\n        state_dict.pop(\"fc.weight\")\n        super().load_state_dict(state_dict, **kwargs)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/timm_sknet/#src.thaw_slump_segmentation.models.encoders.timm_sknet.SkNetEncoder.__init__","title":"<code>__init__(out_channels, depth=5, **kwargs)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/timm_sknet.py</code> <pre><code>def __init__(self, out_channels, depth=5, **kwargs):\n    super().__init__(**kwargs)\n    self._depth = depth\n    self._out_channels = out_channels\n    self._in_channels = 3\n\n    del self.fc\n    del self.global_pool\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/timm_sknet/#src.thaw_slump_segmentation.models.encoders.timm_sknet.SkNetEncoder.forward","title":"<code>forward(x)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/timm_sknet.py</code> <pre><code>def forward(self, x):\n    stages = self.get_stages()\n\n    features = []\n    for i in range(self._depth + 1):\n        x = stages[i](x)\n        features.append(x)\n\n    return features\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/timm_sknet/#src.thaw_slump_segmentation.models.encoders.timm_sknet.SkNetEncoder.get_stages","title":"<code>get_stages()</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/timm_sknet.py</code> <pre><code>def get_stages(self):\n    return [\n        nn.Identity(),\n        nn.Sequential(self.conv1, self.bn1, self.act1),\n        nn.Sequential(self.maxpool, self.layer1),\n        self.layer2,\n        self.layer3,\n        self.layer4,\n    ]\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/timm_sknet/#src.thaw_slump_segmentation.models.encoders.timm_sknet.SkNetEncoder.load_state_dict","title":"<code>load_state_dict(state_dict, **kwargs)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/timm_sknet.py</code> <pre><code>def load_state_dict(self, state_dict, **kwargs):\n    state_dict.pop(\"fc.bias\")\n    state_dict.pop(\"fc.weight\")\n    super().load_state_dict(state_dict, **kwargs)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/vgg/","title":"Vgg","text":"<p>Each encoder should have following attributes and methods and be inherited from <code>_base.EncoderMixin</code></p> <p>Attributes:</p> <pre><code>_out_channels (list of int): specify number of channels for each encoder feature tensor\n_depth (int): specify number of stages in decoder (in other words number of downsampling operations)\n_in_channels (int): default number of input channels in first Conv2d layer for encoder (usually 3)\n</code></pre> <p>Methods:</p> <pre><code>forward(self, x: torch.Tensor)\n    produce list of features of different spatial resolutions, each feature is a 4D torch.tensor of\n    shape NCHW (features should be sorted in descending order according to spatial resolution, starting\n    with resolution same as input `x` tensor).\n\n    Input: `x` with shape (1, 3, 64, 64)\n    Output: [f0, f1, f2, f3, f4, f5] - features with corresponding shapes\n            [(1, 3, 64, 64), (1, 64, 32, 32), (1, 128, 16, 16), (1, 256, 8, 8),\n            (1, 512, 4, 4), (1, 1024, 2, 2)] (C - dim may differ)\n\n    also should support number of features according to specified depth, e.g. if depth = 5,\n    number of feature tensors = 6 (one with same resolution as input and 5 downsampled),\n    depth = 3 -&gt; number of feature tensors = 4 (one with same resolution as input and 3 downsampled).\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/vgg/#src.thaw_slump_segmentation.models.encoders.vgg.cfg","title":"<code>cfg = {'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'], 'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'], 'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'], 'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M']}</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/encoders/vgg/#src.thaw_slump_segmentation.models.encoders.vgg.vgg_encoders","title":"<code>vgg_encoders = {'vgg11': {'encoder': VGGEncoder, 'pretrained_settings': pretrained_settings['vgg11'], 'params': {'out_channels': (64, 128, 256, 512, 512, 512), 'config': cfg['A'], 'batch_norm': False}}, 'vgg11_bn': {'encoder': VGGEncoder, 'pretrained_settings': pretrained_settings['vgg11_bn'], 'params': {'out_channels': (64, 128, 256, 512, 512, 512), 'config': cfg['A'], 'batch_norm': True}}, 'vgg13': {'encoder': VGGEncoder, 'pretrained_settings': pretrained_settings['vgg13'], 'params': {'out_channels': (64, 128, 256, 512, 512, 512), 'config': cfg['B'], 'batch_norm': False}}, 'vgg13_bn': {'encoder': VGGEncoder, 'pretrained_settings': pretrained_settings['vgg13_bn'], 'params': {'out_channels': (64, 128, 256, 512, 512, 512), 'config': cfg['B'], 'batch_norm': True}}, 'vgg16': {'encoder': VGGEncoder, 'pretrained_settings': pretrained_settings['vgg16'], 'params': {'out_channels': (64, 128, 256, 512, 512, 512), 'config': cfg['D'], 'batch_norm': False}}, 'vgg16_bn': {'encoder': VGGEncoder, 'pretrained_settings': pretrained_settings['vgg16_bn'], 'params': {'out_channels': (64, 128, 256, 512, 512, 512), 'config': cfg['D'], 'batch_norm': True}}, 'vgg19': {'encoder': VGGEncoder, 'pretrained_settings': pretrained_settings['vgg19'], 'params': {'out_channels': (64, 128, 256, 512, 512, 512), 'config': cfg['E'], 'batch_norm': False}}, 'vgg19_bn': {'encoder': VGGEncoder, 'pretrained_settings': pretrained_settings['vgg19_bn'], 'params': {'out_channels': (64, 128, 256, 512, 512, 512), 'config': cfg['E'], 'batch_norm': True}}}</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/encoders/vgg/#src.thaw_slump_segmentation.models.encoders.vgg.VGGEncoder","title":"<code>VGGEncoder</code>","text":"<p>               Bases: <code>VGG</code>, <code>EncoderMixin</code></p> Source code in <code>src/thaw_slump_segmentation/models/encoders/vgg.py</code> <pre><code>class VGGEncoder(VGG, EncoderMixin):\n    def __init__(self, out_channels, config, batch_norm=False, depth=5, **kwargs):\n        super().__init__(make_layers(config, batch_norm=batch_norm), **kwargs)\n        self._out_channels = out_channels\n        self._depth = depth\n        self._in_channels = 3\n        del self.classifier\n\n    def make_dilated(self, stage_list, dilation_list):\n        raise ValueError(\"'VGG' models do not support dilated mode due to Max Pooling\"\n                         \" operations for downsampling!\")\n\n    def get_stages(self):\n        stages = []\n        stage_modules = []\n        for module in self.features:\n            if isinstance(module, nn.MaxPool2d):\n                stages.append(nn.Sequential(*stage_modules))\n                stage_modules = []\n            stage_modules.append(module)\n        stages.append(nn.Sequential(*stage_modules))\n        return stages\n\n    def forward(self, x):\n        stages = self.get_stages()\n\n        features = []\n        for i in range(self._depth + 1):\n            x = stages[i](x)\n            features.append(x)\n\n        return features\n\n    def load_state_dict(self, state_dict, **kwargs):\n        keys = list(state_dict.keys())\n        for k in keys:\n            if k.startswith(\"classifier\"):\n                state_dict.pop(k)\n        super().load_state_dict(state_dict, **kwargs)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/vgg/#src.thaw_slump_segmentation.models.encoders.vgg.VGGEncoder.__init__","title":"<code>__init__(out_channels, config, batch_norm=False, depth=5, **kwargs)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/vgg.py</code> <pre><code>def __init__(self, out_channels, config, batch_norm=False, depth=5, **kwargs):\n    super().__init__(make_layers(config, batch_norm=batch_norm), **kwargs)\n    self._out_channels = out_channels\n    self._depth = depth\n    self._in_channels = 3\n    del self.classifier\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/vgg/#src.thaw_slump_segmentation.models.encoders.vgg.VGGEncoder.forward","title":"<code>forward(x)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/vgg.py</code> <pre><code>def forward(self, x):\n    stages = self.get_stages()\n\n    features = []\n    for i in range(self._depth + 1):\n        x = stages[i](x)\n        features.append(x)\n\n    return features\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/vgg/#src.thaw_slump_segmentation.models.encoders.vgg.VGGEncoder.get_stages","title":"<code>get_stages()</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/vgg.py</code> <pre><code>def get_stages(self):\n    stages = []\n    stage_modules = []\n    for module in self.features:\n        if isinstance(module, nn.MaxPool2d):\n            stages.append(nn.Sequential(*stage_modules))\n            stage_modules = []\n        stage_modules.append(module)\n    stages.append(nn.Sequential(*stage_modules))\n    return stages\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/vgg/#src.thaw_slump_segmentation.models.encoders.vgg.VGGEncoder.load_state_dict","title":"<code>load_state_dict(state_dict, **kwargs)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/vgg.py</code> <pre><code>def load_state_dict(self, state_dict, **kwargs):\n    keys = list(state_dict.keys())\n    for k in keys:\n        if k.startswith(\"classifier\"):\n            state_dict.pop(k)\n    super().load_state_dict(state_dict, **kwargs)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/vgg/#src.thaw_slump_segmentation.models.encoders.vgg.VGGEncoder.make_dilated","title":"<code>make_dilated(stage_list, dilation_list)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/vgg.py</code> <pre><code>def make_dilated(self, stage_list, dilation_list):\n    raise ValueError(\"'VGG' models do not support dilated mode due to Max Pooling\"\n                     \" operations for downsampling!\")\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/xception/","title":"Xception","text":""},{"location":"reference/src/thaw_slump_segmentation/models/encoders/xception/#src.thaw_slump_segmentation.models.encoders.xception.xception_encoders","title":"<code>xception_encoders = {'xception': {'encoder': XceptionEncoder, 'pretrained_settings': pretrained_settings['xception'], 'params': {'out_channels': (3, 64, 128, 256, 728, 2048)}}}</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/encoders/xception/#src.thaw_slump_segmentation.models.encoders.xception.XceptionEncoder","title":"<code>XceptionEncoder</code>","text":"<p>               Bases: <code>Xception</code>, <code>EncoderMixin</code></p> Source code in <code>src/thaw_slump_segmentation/models/encoders/xception.py</code> <pre><code>class XceptionEncoder(Xception, EncoderMixin):\n\n    def __init__(self, out_channels, *args, depth=5, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        self._out_channels = out_channels\n        self._depth = depth\n        self._in_channels = 3\n\n        # modify padding to maintain output shape\n        self.conv1.padding = (1, 1)\n        self.conv2.padding = (1, 1)\n\n        del self.fc\n\n    def make_dilated(self, stage_list, dilation_list):\n        raise ValueError(\"Xception encoder does not support dilated mode \"\n                         \"due to pooling operation for downsampling!\")\n\n    def get_stages(self):\n        return [\n            nn.Identity(),\n            nn.Sequential(self.conv1, self.bn1, self.relu, self.conv2, self.bn2, self.relu),\n            self.block1,\n            self.block2,\n            nn.Sequential(self.block3, self.block4, self.block5, self.block6, self.block7,\n                          self.block8, self.block9, self.block10, self.block11),\n            nn.Sequential(self.block12, self.conv3, self.bn3, self.relu, self.conv4, self.bn4),\n        ]\n\n    def forward(self, x):\n        stages = self.get_stages()\n\n        features = []\n        for i in range(self._depth + 1):\n            x = stages[i](x)\n            features.append(x)\n\n        return features\n\n    def load_state_dict(self, state_dict):\n        # remove linear\n        state_dict.pop('fc.bias')\n        state_dict.pop('fc.weight')\n\n        super().load_state_dict(state_dict)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/xception/#src.thaw_slump_segmentation.models.encoders.xception.XceptionEncoder.__init__","title":"<code>__init__(out_channels, *args, depth=5, **kwargs)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/xception.py</code> <pre><code>def __init__(self, out_channels, *args, depth=5, **kwargs):\n    super().__init__(*args, **kwargs)\n\n    self._out_channels = out_channels\n    self._depth = depth\n    self._in_channels = 3\n\n    # modify padding to maintain output shape\n    self.conv1.padding = (1, 1)\n    self.conv2.padding = (1, 1)\n\n    del self.fc\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/xception/#src.thaw_slump_segmentation.models.encoders.xception.XceptionEncoder.forward","title":"<code>forward(x)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/xception.py</code> <pre><code>def forward(self, x):\n    stages = self.get_stages()\n\n    features = []\n    for i in range(self._depth + 1):\n        x = stages[i](x)\n        features.append(x)\n\n    return features\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/xception/#src.thaw_slump_segmentation.models.encoders.xception.XceptionEncoder.get_stages","title":"<code>get_stages()</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/xception.py</code> <pre><code>def get_stages(self):\n    return [\n        nn.Identity(),\n        nn.Sequential(self.conv1, self.bn1, self.relu, self.conv2, self.bn2, self.relu),\n        self.block1,\n        self.block2,\n        nn.Sequential(self.block3, self.block4, self.block5, self.block6, self.block7,\n                      self.block8, self.block9, self.block10, self.block11),\n        nn.Sequential(self.block12, self.conv3, self.bn3, self.relu, self.conv4, self.bn4),\n    ]\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/xception/#src.thaw_slump_segmentation.models.encoders.xception.XceptionEncoder.load_state_dict","title":"<code>load_state_dict(state_dict)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/xception.py</code> <pre><code>def load_state_dict(self, state_dict):\n    # remove linear\n    state_dict.pop('fc.bias')\n    state_dict.pop('fc.weight')\n\n    super().load_state_dict(state_dict)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/encoders/xception/#src.thaw_slump_segmentation.models.encoders.xception.XceptionEncoder.make_dilated","title":"<code>make_dilated(stage_list, dilation_list)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/encoders/xception.py</code> <pre><code>def make_dilated(self, stage_list, dilation_list):\n    raise ValueError(\"Xception encoder does not support dilated mode \"\n                     \"due to pooling operation for downsampling!\")\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/fpn/decoder/","title":"Decoder","text":""},{"location":"reference/src/thaw_slump_segmentation/models/fpn/decoder/#src.thaw_slump_segmentation.models.fpn.decoder.Conv3x3GNReLU","title":"<code>Conv3x3GNReLU</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/thaw_slump_segmentation/models/fpn/decoder.py</code> <pre><code>class Conv3x3GNReLU(nn.Module):\n    def __init__(self, in_channels, out_channels, upsample=False):\n        super().__init__()\n        self.upsample = upsample\n        self.block = nn.Sequential(\n            nn.Conv2d(\n                in_channels, out_channels, (3, 3), stride=1, padding=1, bias=False\n            ),\n            nn.GroupNorm(32, out_channels),\n            nn.ReLU(inplace=True),\n        )\n\n    def forward(self, x):\n        x = self.block(x)\n        if self.upsample:\n            x = F.interpolate(x, scale_factor=2, mode=\"bilinear\", align_corners=True)\n        return x\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/fpn/decoder/#src.thaw_slump_segmentation.models.fpn.decoder.Conv3x3GNReLU.block","title":"<code>block = nn.Sequential(nn.Conv2d(in_channels, out_channels, (3, 3), stride=1, padding=1, bias=False), nn.GroupNorm(32, out_channels), nn.ReLU(inplace=True))</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/fpn/decoder/#src.thaw_slump_segmentation.models.fpn.decoder.Conv3x3GNReLU.upsample","title":"<code>upsample = upsample</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/fpn/decoder/#src.thaw_slump_segmentation.models.fpn.decoder.Conv3x3GNReLU.__init__","title":"<code>__init__(in_channels, out_channels, upsample=False)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/fpn/decoder.py</code> <pre><code>def __init__(self, in_channels, out_channels, upsample=False):\n    super().__init__()\n    self.upsample = upsample\n    self.block = nn.Sequential(\n        nn.Conv2d(\n            in_channels, out_channels, (3, 3), stride=1, padding=1, bias=False\n        ),\n        nn.GroupNorm(32, out_channels),\n        nn.ReLU(inplace=True),\n    )\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/fpn/decoder/#src.thaw_slump_segmentation.models.fpn.decoder.Conv3x3GNReLU.forward","title":"<code>forward(x)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/fpn/decoder.py</code> <pre><code>def forward(self, x):\n    x = self.block(x)\n    if self.upsample:\n        x = F.interpolate(x, scale_factor=2, mode=\"bilinear\", align_corners=True)\n    return x\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/fpn/decoder/#src.thaw_slump_segmentation.models.fpn.decoder.FPNBlock","title":"<code>FPNBlock</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/thaw_slump_segmentation/models/fpn/decoder.py</code> <pre><code>class FPNBlock(nn.Module):\n    def __init__(self, pyramid_channels, skip_channels):\n        super().__init__()\n        self.skip_conv = nn.Conv2d(skip_channels, pyramid_channels, kernel_size=1)\n\n    def forward(self, x, skip=None):\n        x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n        skip = self.skip_conv(skip)\n        x = x + skip\n        return x\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/fpn/decoder/#src.thaw_slump_segmentation.models.fpn.decoder.FPNBlock.skip_conv","title":"<code>skip_conv = nn.Conv2d(skip_channels, pyramid_channels, kernel_size=1)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/fpn/decoder/#src.thaw_slump_segmentation.models.fpn.decoder.FPNBlock.__init__","title":"<code>__init__(pyramid_channels, skip_channels)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/fpn/decoder.py</code> <pre><code>def __init__(self, pyramid_channels, skip_channels):\n    super().__init__()\n    self.skip_conv = nn.Conv2d(skip_channels, pyramid_channels, kernel_size=1)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/fpn/decoder/#src.thaw_slump_segmentation.models.fpn.decoder.FPNBlock.forward","title":"<code>forward(x, skip=None)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/fpn/decoder.py</code> <pre><code>def forward(self, x, skip=None):\n    x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n    skip = self.skip_conv(skip)\n    x = x + skip\n    return x\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/fpn/decoder/#src.thaw_slump_segmentation.models.fpn.decoder.FPNDecoder","title":"<code>FPNDecoder</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/thaw_slump_segmentation/models/fpn/decoder.py</code> <pre><code>class FPNDecoder(nn.Module):\n    def __init__(\n            self,\n            encoder_channels,\n            encoder_depth=5,\n            pyramid_channels=256,\n            segmentation_channels=128,\n            dropout=0.2,\n            merge_policy=\"add\",\n    ):\n        super().__init__()\n\n        self.out_channels = segmentation_channels if merge_policy == \"add\" else segmentation_channels * 4\n        if encoder_depth &lt; 3:\n            raise ValueError(\"Encoder depth for FPN decoder cannot be less than 3, got {}.\".format(encoder_depth))\n\n        encoder_channels = encoder_channels[::-1]\n        encoder_channels = encoder_channels[:encoder_depth + 1]\n\n        self.p5 = nn.Conv2d(encoder_channels[0], pyramid_channels, kernel_size=1)\n        self.p4 = FPNBlock(pyramid_channels, encoder_channels[1])\n        self.p3 = FPNBlock(pyramid_channels, encoder_channels[2])\n        self.p2 = FPNBlock(pyramid_channels, encoder_channels[3])\n\n        self.seg_blocks = nn.ModuleList([\n            SegmentationBlock(pyramid_channels, segmentation_channels, n_upsamples=n_upsamples)\n            for n_upsamples in [3, 2, 1, 0]\n        ])\n\n        self.merge = MergeBlock(merge_policy)\n        self.dropout = nn.Dropout2d(p=dropout, inplace=True)\n\n    def forward(self, *features):\n        c2, c3, c4, c5 = features[-4:]\n\n        p5 = self.p5(c5)\n        p4 = self.p4(p5, c4)\n        p3 = self.p3(p4, c3)\n        p2 = self.p2(p3, c2)\n\n        feature_pyramid = [seg_block(p) for seg_block, p in zip(self.seg_blocks, [p5, p4, p3, p2])]\n        x = self.merge(feature_pyramid)\n        x = self.dropout(x)\n\n        return x\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/fpn/decoder/#src.thaw_slump_segmentation.models.fpn.decoder.FPNDecoder.dropout","title":"<code>dropout = nn.Dropout2d(p=dropout, inplace=True)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/fpn/decoder/#src.thaw_slump_segmentation.models.fpn.decoder.FPNDecoder.merge","title":"<code>merge = MergeBlock(merge_policy)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/fpn/decoder/#src.thaw_slump_segmentation.models.fpn.decoder.FPNDecoder.out_channels","title":"<code>out_channels = segmentation_channels if merge_policy == 'add' else segmentation_channels * 4</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/fpn/decoder/#src.thaw_slump_segmentation.models.fpn.decoder.FPNDecoder.p2","title":"<code>p2 = FPNBlock(pyramid_channels, encoder_channels[3])</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/fpn/decoder/#src.thaw_slump_segmentation.models.fpn.decoder.FPNDecoder.p3","title":"<code>p3 = FPNBlock(pyramid_channels, encoder_channels[2])</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/fpn/decoder/#src.thaw_slump_segmentation.models.fpn.decoder.FPNDecoder.p4","title":"<code>p4 = FPNBlock(pyramid_channels, encoder_channels[1])</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/fpn/decoder/#src.thaw_slump_segmentation.models.fpn.decoder.FPNDecoder.p5","title":"<code>p5 = nn.Conv2d(encoder_channels[0], pyramid_channels, kernel_size=1)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/fpn/decoder/#src.thaw_slump_segmentation.models.fpn.decoder.FPNDecoder.seg_blocks","title":"<code>seg_blocks = nn.ModuleList([SegmentationBlock(pyramid_channels, segmentation_channels, n_upsamples=n_upsamples) for n_upsamples in [3, 2, 1, 0]])</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/fpn/decoder/#src.thaw_slump_segmentation.models.fpn.decoder.FPNDecoder.__init__","title":"<code>__init__(encoder_channels, encoder_depth=5, pyramid_channels=256, segmentation_channels=128, dropout=0.2, merge_policy='add')</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/fpn/decoder.py</code> <pre><code>def __init__(\n        self,\n        encoder_channels,\n        encoder_depth=5,\n        pyramid_channels=256,\n        segmentation_channels=128,\n        dropout=0.2,\n        merge_policy=\"add\",\n):\n    super().__init__()\n\n    self.out_channels = segmentation_channels if merge_policy == \"add\" else segmentation_channels * 4\n    if encoder_depth &lt; 3:\n        raise ValueError(\"Encoder depth for FPN decoder cannot be less than 3, got {}.\".format(encoder_depth))\n\n    encoder_channels = encoder_channels[::-1]\n    encoder_channels = encoder_channels[:encoder_depth + 1]\n\n    self.p5 = nn.Conv2d(encoder_channels[0], pyramid_channels, kernel_size=1)\n    self.p4 = FPNBlock(pyramid_channels, encoder_channels[1])\n    self.p3 = FPNBlock(pyramid_channels, encoder_channels[2])\n    self.p2 = FPNBlock(pyramid_channels, encoder_channels[3])\n\n    self.seg_blocks = nn.ModuleList([\n        SegmentationBlock(pyramid_channels, segmentation_channels, n_upsamples=n_upsamples)\n        for n_upsamples in [3, 2, 1, 0]\n    ])\n\n    self.merge = MergeBlock(merge_policy)\n    self.dropout = nn.Dropout2d(p=dropout, inplace=True)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/fpn/decoder/#src.thaw_slump_segmentation.models.fpn.decoder.FPNDecoder.forward","title":"<code>forward(*features)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/fpn/decoder.py</code> <pre><code>def forward(self, *features):\n    c2, c3, c4, c5 = features[-4:]\n\n    p5 = self.p5(c5)\n    p4 = self.p4(p5, c4)\n    p3 = self.p3(p4, c3)\n    p2 = self.p2(p3, c2)\n\n    feature_pyramid = [seg_block(p) for seg_block, p in zip(self.seg_blocks, [p5, p4, p3, p2])]\n    x = self.merge(feature_pyramid)\n    x = self.dropout(x)\n\n    return x\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/fpn/decoder/#src.thaw_slump_segmentation.models.fpn.decoder.MergeBlock","title":"<code>MergeBlock</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/thaw_slump_segmentation/models/fpn/decoder.py</code> <pre><code>class MergeBlock(nn.Module):\n    def __init__(self, policy):\n        super().__init__()\n        if policy not in [\"add\", \"cat\"]:\n            raise ValueError(\n                \"`merge_policy` must be one of: ['add', 'cat'], got {}\".format(\n                    policy\n                )\n            )\n        self.policy = policy\n\n    def forward(self, x):\n        if self.policy == 'add':\n            return sum(x)\n        elif self.policy == 'cat':\n            return torch.cat(x, dim=1)\n        else:\n            raise ValueError(\n                \"`merge_policy` must be one of: ['add', 'cat'], got {}\".format(self.policy)\n            )\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/fpn/decoder/#src.thaw_slump_segmentation.models.fpn.decoder.MergeBlock.policy","title":"<code>policy = policy</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/fpn/decoder/#src.thaw_slump_segmentation.models.fpn.decoder.MergeBlock.__init__","title":"<code>__init__(policy)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/fpn/decoder.py</code> <pre><code>def __init__(self, policy):\n    super().__init__()\n    if policy not in [\"add\", \"cat\"]:\n        raise ValueError(\n            \"`merge_policy` must be one of: ['add', 'cat'], got {}\".format(\n                policy\n            )\n        )\n    self.policy = policy\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/fpn/decoder/#src.thaw_slump_segmentation.models.fpn.decoder.MergeBlock.forward","title":"<code>forward(x)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/fpn/decoder.py</code> <pre><code>def forward(self, x):\n    if self.policy == 'add':\n        return sum(x)\n    elif self.policy == 'cat':\n        return torch.cat(x, dim=1)\n    else:\n        raise ValueError(\n            \"`merge_policy` must be one of: ['add', 'cat'], got {}\".format(self.policy)\n        )\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/fpn/decoder/#src.thaw_slump_segmentation.models.fpn.decoder.SegmentationBlock","title":"<code>SegmentationBlock</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/thaw_slump_segmentation/models/fpn/decoder.py</code> <pre><code>class SegmentationBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, n_upsamples=0):\n        super().__init__()\n\n        blocks = [Conv3x3GNReLU(in_channels, out_channels, upsample=bool(n_upsamples))]\n\n        if n_upsamples &gt; 1:\n            for _ in range(1, n_upsamples):\n                blocks.append(Conv3x3GNReLU(out_channels, out_channels, upsample=True))\n\n        self.block = nn.Sequential(*blocks)\n\n    def forward(self, x):\n        return self.block(x)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/fpn/decoder/#src.thaw_slump_segmentation.models.fpn.decoder.SegmentationBlock.block","title":"<code>block = nn.Sequential(*blocks)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/fpn/decoder/#src.thaw_slump_segmentation.models.fpn.decoder.SegmentationBlock.__init__","title":"<code>__init__(in_channels, out_channels, n_upsamples=0)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/fpn/decoder.py</code> <pre><code>def __init__(self, in_channels, out_channels, n_upsamples=0):\n    super().__init__()\n\n    blocks = [Conv3x3GNReLU(in_channels, out_channels, upsample=bool(n_upsamples))]\n\n    if n_upsamples &gt; 1:\n        for _ in range(1, n_upsamples):\n            blocks.append(Conv3x3GNReLU(out_channels, out_channels, upsample=True))\n\n    self.block = nn.Sequential(*blocks)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/fpn/decoder/#src.thaw_slump_segmentation.models.fpn.decoder.SegmentationBlock.forward","title":"<code>forward(x)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/fpn/decoder.py</code> <pre><code>def forward(self, x):\n    return self.block(x)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/fpn/model/","title":"Model","text":""},{"location":"reference/src/thaw_slump_segmentation/models/fpn/model/#src.thaw_slump_segmentation.models.fpn.model.FPN","title":"<code>FPN</code>","text":"<p>               Bases: <code>SegmentationModel</code></p> <p>FPN_ is a fully convolution neural network for image semantic segmentation.</p> <p>Parameters:</p> Name Type Description Default <code>encoder_name</code> <code>str</code> <p>Name of the classification model that will be used as an encoder (a.k.a backbone) to extract features of different spatial resolution</p> <code>'resnet34'</code> <code>encoder_depth</code> <code>int</code> <p>A number of stages used in encoder in range [3, 5]. Each stage generate features  two times smaller in spatial dimensions than previous one (e.g. for depth 0 we will have features with shapes [(N, C, H, W),], for depth 1 - [(N, C, H, W), (N, C, H // 2, W // 2)] and so on). Default is 5</p> <code>5</code> <code>encoder_weights</code> <code>Optional[str]</code> <p>One of None (random initialization), \"imagenet\" (pre-training on ImageNet) and  other pretrained weights (see table with available weights for each encoder_name)</p> <code>'imagenet'</code> <code>decoder_pyramid_channels</code> <code>int</code> <p>A number of convolution filters in Feature Pyramid of FPN_</p> <code>256</code> <code>decoder_segmentation_channels</code> <code>int</code> <p>A number of convolution filters in segmentation blocks of FPN_</p> <code>128</code> <code>decoder_merge_policy</code> <code>str</code> <p>Determines how to merge pyramid features inside FPN. Available options are add and cat</p> <code>'add'</code> <code>decoder_dropout</code> <code>float</code> <p>Spatial dropout rate in range (0, 1) for feature pyramid in FPN_</p> <code>0.2</code> <code>in_channels</code> <code>int</code> <p>A number of input channels for the model, default is 3 (RGB images)</p> <code>3</code> <code>classes</code> <code>int</code> <p>A number of classes for output mask (or you can think as a number of channels of output mask)</p> <code>1</code> <code>activation</code> <code>Optional[str]</code> <p>An activation function to apply after the final convolution layer. Available options are \"sigmoid\", \"softmax\", \"logsoftmax\", \"tanh\", \"identity\", callable and None. Default is None</p> <code>None</code> <code>upsampling</code> <code>int</code> <p>Final upsampling factor. Default is 4 to preserve input-output spatial shape identity</p> <code>4</code> <code>aux_params</code> <code>Optional[dict]</code> <p>Dictionary with parameters of the auxiliary output (classification head). Auxiliary output is build  on top of encoder if aux_params is not None (default). Supported params:     - classes (int): A number of classes     - pooling (str): One of \"max\", \"avg\". Default is \"avg\"     - dropout (float): Dropout factor in [0, 1)     - activation (str): An activation function to apply \"sigmoid\"/\"softmax\" (could be None to return logits)</p> <code>None</code> <p>Returns:</p> Type Description <p><code>torch.nn.Module</code>: FPN</p> <p>.. _FPN:     http://presentations.cocodataset.org/COCO17-Stuff-FAIR.pdf</p> Source code in <code>src/thaw_slump_segmentation/models/fpn/model.py</code> <pre><code>class FPN(SegmentationModel):\n    \"\"\"FPN_ is a fully convolution neural network for image semantic segmentation.\n\n    Args:\n        encoder_name: Name of the classification model that will be used as an encoder (a.k.a backbone)\n            to extract features of different spatial resolution\n        encoder_depth: A number of stages used in encoder in range [3, 5]. Each stage generate features \n            two times smaller in spatial dimensions than previous one (e.g. for depth 0 we will have features\n            with shapes [(N, C, H, W),], for depth 1 - [(N, C, H, W), (N, C, H // 2, W // 2)] and so on).\n            Default is 5\n        encoder_weights: One of **None** (random initialization), **\"imagenet\"** (pre-training on ImageNet) and \n            other pretrained weights (see table with available weights for each encoder_name)\n        decoder_pyramid_channels: A number of convolution filters in Feature Pyramid of FPN_\n        decoder_segmentation_channels: A number of convolution filters in segmentation blocks of FPN_\n        decoder_merge_policy: Determines how to merge pyramid features inside FPN. Available options are **add** and **cat**\n        decoder_dropout: Spatial dropout rate in range (0, 1) for feature pyramid in FPN_\n        in_channels: A number of input channels for the model, default is 3 (RGB images)\n        classes: A number of classes for output mask (or you can think as a number of channels of output mask)\n        activation: An activation function to apply after the final convolution layer.\n            Available options are **\"sigmoid\"**, **\"softmax\"**, **\"logsoftmax\"**, **\"tanh\"**, **\"identity\"**, **callable** and **None**.\n            Default is **None**\n        upsampling: Final upsampling factor. Default is 4 to preserve input-output spatial shape identity\n        aux_params: Dictionary with parameters of the auxiliary output (classification head). Auxiliary output is build \n            on top of encoder if **aux_params** is not **None** (default). Supported params:\n                - classes (int): A number of classes\n                - pooling (str): One of \"max\", \"avg\". Default is \"avg\"\n                - dropout (float): Dropout factor in [0, 1)\n                - activation (str): An activation function to apply \"sigmoid\"/\"softmax\" (could be **None** to return logits)\n\n    Returns:\n        ``torch.nn.Module``: **FPN**\n\n    .. _FPN:\n        http://presentations.cocodataset.org/COCO17-Stuff-FAIR.pdf\n\n    \"\"\"\n\n    def __init__(\n        self,\n        encoder_name: str = \"resnet34\",\n        encoder_depth: int = 5,\n        encoder_weights: Optional[str] = \"imagenet\",\n        decoder_pyramid_channels: int = 256,\n        decoder_segmentation_channels: int = 128,\n        decoder_merge_policy: str = \"add\",\n        decoder_dropout: float = 0.2,\n        in_channels: int = 3,\n        classes: int = 1,\n        activation: Optional[str] = None,\n        upsampling: int = 4,\n        aux_params: Optional[dict] = None,\n    ):\n        super().__init__()\n\n        self.encoder = get_encoder(\n            encoder_name,\n            in_channels=in_channels,\n            depth=encoder_depth,\n            weights=encoder_weights,\n        )\n\n        self.decoder = FPNDecoder(\n            encoder_channels=self.encoder.out_channels,\n            encoder_depth=encoder_depth,\n            pyramid_channels=decoder_pyramid_channels,\n            segmentation_channels=decoder_segmentation_channels,\n            dropout=decoder_dropout,\n            merge_policy=decoder_merge_policy,\n        )\n\n        self.segmentation_head = SegmentationHead(\n            in_channels=self.decoder.out_channels,\n            out_channels=classes,\n            activation=activation,\n            kernel_size=1,\n            upsampling=upsampling,\n        )\n\n        if aux_params is not None:\n            self.classification_head = ClassificationHead(\n                in_channels=self.encoder.out_channels[-1], **aux_params\n            )\n        else:\n            self.classification_head = None\n\n        self.name = \"fpn-{}\".format(encoder_name)\n        self.initialize()\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/fpn/model/#src.thaw_slump_segmentation.models.fpn.model.FPN.classification_head","title":"<code>classification_head = ClassificationHead(in_channels=self.encoder.out_channels[-1], **aux_params)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/fpn/model/#src.thaw_slump_segmentation.models.fpn.model.FPN.decoder","title":"<code>decoder = FPNDecoder(encoder_channels=self.encoder.out_channels, encoder_depth=encoder_depth, pyramid_channels=decoder_pyramid_channels, segmentation_channels=decoder_segmentation_channels, dropout=decoder_dropout, merge_policy=decoder_merge_policy)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/fpn/model/#src.thaw_slump_segmentation.models.fpn.model.FPN.encoder","title":"<code>encoder = get_encoder(encoder_name, in_channels=in_channels, depth=encoder_depth, weights=encoder_weights)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/fpn/model/#src.thaw_slump_segmentation.models.fpn.model.FPN.name","title":"<code>name = 'fpn-{}'.format(encoder_name)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/fpn/model/#src.thaw_slump_segmentation.models.fpn.model.FPN.segmentation_head","title":"<code>segmentation_head = SegmentationHead(in_channels=self.decoder.out_channels, out_channels=classes, activation=activation, kernel_size=1, upsampling=upsampling)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/fpn/model/#src.thaw_slump_segmentation.models.fpn.model.FPN.__init__","title":"<code>__init__(encoder_name='resnet34', encoder_depth=5, encoder_weights='imagenet', decoder_pyramid_channels=256, decoder_segmentation_channels=128, decoder_merge_policy='add', decoder_dropout=0.2, in_channels=3, classes=1, activation=None, upsampling=4, aux_params=None)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/fpn/model.py</code> <pre><code>def __init__(\n    self,\n    encoder_name: str = \"resnet34\",\n    encoder_depth: int = 5,\n    encoder_weights: Optional[str] = \"imagenet\",\n    decoder_pyramid_channels: int = 256,\n    decoder_segmentation_channels: int = 128,\n    decoder_merge_policy: str = \"add\",\n    decoder_dropout: float = 0.2,\n    in_channels: int = 3,\n    classes: int = 1,\n    activation: Optional[str] = None,\n    upsampling: int = 4,\n    aux_params: Optional[dict] = None,\n):\n    super().__init__()\n\n    self.encoder = get_encoder(\n        encoder_name,\n        in_channels=in_channels,\n        depth=encoder_depth,\n        weights=encoder_weights,\n    )\n\n    self.decoder = FPNDecoder(\n        encoder_channels=self.encoder.out_channels,\n        encoder_depth=encoder_depth,\n        pyramid_channels=decoder_pyramid_channels,\n        segmentation_channels=decoder_segmentation_channels,\n        dropout=decoder_dropout,\n        merge_policy=decoder_merge_policy,\n    )\n\n    self.segmentation_head = SegmentationHead(\n        in_channels=self.decoder.out_channels,\n        out_channels=classes,\n        activation=activation,\n        kernel_size=1,\n        upsampling=upsampling,\n    )\n\n    if aux_params is not None:\n        self.classification_head = ClassificationHead(\n            in_channels=self.encoder.out_channels[-1], **aux_params\n        )\n    else:\n        self.classification_head = None\n\n    self.name = \"fpn-{}\".format(encoder_name)\n    self.initialize()\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/linknet/decoder/","title":"Decoder","text":""},{"location":"reference/src/thaw_slump_segmentation/models/linknet/decoder/#src.thaw_slump_segmentation.models.linknet.decoder.DecoderBlock","title":"<code>DecoderBlock</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/thaw_slump_segmentation/models/linknet/decoder.py</code> <pre><code>class DecoderBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, use_batchnorm=True):\n        super().__init__()\n\n        self.block = nn.Sequential(\n            modules.Conv2dReLU(in_channels, in_channels // 4, kernel_size=1, use_batchnorm=use_batchnorm),\n            TransposeX2(in_channels // 4, in_channels // 4, use_batchnorm=use_batchnorm),\n            modules.Conv2dReLU(in_channels // 4, out_channels, kernel_size=1, use_batchnorm=use_batchnorm),\n        )\n\n    def forward(self, x, skip=None):\n        x = self.block(x)\n        if skip is not None:\n            x = x + skip\n        return x\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/linknet/decoder/#src.thaw_slump_segmentation.models.linknet.decoder.DecoderBlock.block","title":"<code>block = nn.Sequential(modules.Conv2dReLU(in_channels, in_channels // 4, kernel_size=1, use_batchnorm=use_batchnorm), TransposeX2(in_channels // 4, in_channels // 4, use_batchnorm=use_batchnorm), modules.Conv2dReLU(in_channels // 4, out_channels, kernel_size=1, use_batchnorm=use_batchnorm))</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/linknet/decoder/#src.thaw_slump_segmentation.models.linknet.decoder.DecoderBlock.__init__","title":"<code>__init__(in_channels, out_channels, use_batchnorm=True)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/linknet/decoder.py</code> <pre><code>def __init__(self, in_channels, out_channels, use_batchnorm=True):\n    super().__init__()\n\n    self.block = nn.Sequential(\n        modules.Conv2dReLU(in_channels, in_channels // 4, kernel_size=1, use_batchnorm=use_batchnorm),\n        TransposeX2(in_channels // 4, in_channels // 4, use_batchnorm=use_batchnorm),\n        modules.Conv2dReLU(in_channels // 4, out_channels, kernel_size=1, use_batchnorm=use_batchnorm),\n    )\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/linknet/decoder/#src.thaw_slump_segmentation.models.linknet.decoder.DecoderBlock.forward","title":"<code>forward(x, skip=None)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/linknet/decoder.py</code> <pre><code>def forward(self, x, skip=None):\n    x = self.block(x)\n    if skip is not None:\n        x = x + skip\n    return x\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/linknet/decoder/#src.thaw_slump_segmentation.models.linknet.decoder.LinknetDecoder","title":"<code>LinknetDecoder</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/thaw_slump_segmentation/models/linknet/decoder.py</code> <pre><code>class LinknetDecoder(nn.Module):\n\n    def __init__(\n            self,\n            encoder_channels,\n            prefinal_channels=32,\n            n_blocks=5,\n            use_batchnorm=True,\n    ):\n        super().__init__()\n\n        encoder_channels = encoder_channels[1:]  # remove first skip\n        encoder_channels = encoder_channels[::-1]  # reverse channels to start from head of encoder\n\n        channels = list(encoder_channels) + [prefinal_channels]\n\n        self.blocks = nn.ModuleList([\n            DecoderBlock(channels[i], channels[i + 1], use_batchnorm=use_batchnorm)\n            for i in range(n_blocks)\n        ])\n\n    def forward(self, *features):\n        features = features[1:]  # remove first skip\n        features = features[::-1]  # reverse channels to start from head of encoder\n\n        x = features[0]\n        skips = features[1:]\n\n        for i, decoder_block in enumerate(self.blocks):\n            skip = skips[i] if i &lt; len(skips) else None\n            x = decoder_block(x, skip)\n\n        return x\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/linknet/decoder/#src.thaw_slump_segmentation.models.linknet.decoder.LinknetDecoder.blocks","title":"<code>blocks = nn.ModuleList([DecoderBlock(channels[i], channels[i + 1], use_batchnorm=use_batchnorm) for i in range(n_blocks)])</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/linknet/decoder/#src.thaw_slump_segmentation.models.linknet.decoder.LinknetDecoder.__init__","title":"<code>__init__(encoder_channels, prefinal_channels=32, n_blocks=5, use_batchnorm=True)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/linknet/decoder.py</code> <pre><code>def __init__(\n        self,\n        encoder_channels,\n        prefinal_channels=32,\n        n_blocks=5,\n        use_batchnorm=True,\n):\n    super().__init__()\n\n    encoder_channels = encoder_channels[1:]  # remove first skip\n    encoder_channels = encoder_channels[::-1]  # reverse channels to start from head of encoder\n\n    channels = list(encoder_channels) + [prefinal_channels]\n\n    self.blocks = nn.ModuleList([\n        DecoderBlock(channels[i], channels[i + 1], use_batchnorm=use_batchnorm)\n        for i in range(n_blocks)\n    ])\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/linknet/decoder/#src.thaw_slump_segmentation.models.linknet.decoder.LinknetDecoder.forward","title":"<code>forward(*features)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/linknet/decoder.py</code> <pre><code>def forward(self, *features):\n    features = features[1:]  # remove first skip\n    features = features[::-1]  # reverse channels to start from head of encoder\n\n    x = features[0]\n    skips = features[1:]\n\n    for i, decoder_block in enumerate(self.blocks):\n        skip = skips[i] if i &lt; len(skips) else None\n        x = decoder_block(x, skip)\n\n    return x\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/linknet/decoder/#src.thaw_slump_segmentation.models.linknet.decoder.TransposeX2","title":"<code>TransposeX2</code>","text":"<p>               Bases: <code>Sequential</code></p> Source code in <code>src/thaw_slump_segmentation/models/linknet/decoder.py</code> <pre><code>class TransposeX2(nn.Sequential):\n\n    def __init__(self, in_channels, out_channels, use_batchnorm=True):\n        super().__init__()\n        layers = [\n            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1),\n            nn.ReLU(inplace=True)\n        ]\n\n        if use_batchnorm:\n            layers.insert(1, nn.BatchNorm2d(out_channels))\n\n        super().__init__(*layers)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/linknet/decoder/#src.thaw_slump_segmentation.models.linknet.decoder.TransposeX2.__init__","title":"<code>__init__(in_channels, out_channels, use_batchnorm=True)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/linknet/decoder.py</code> <pre><code>def __init__(self, in_channels, out_channels, use_batchnorm=True):\n    super().__init__()\n    layers = [\n        nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1),\n        nn.ReLU(inplace=True)\n    ]\n\n    if use_batchnorm:\n        layers.insert(1, nn.BatchNorm2d(out_channels))\n\n    super().__init__(*layers)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/linknet/model/","title":"Model","text":""},{"location":"reference/src/thaw_slump_segmentation/models/linknet/model/#src.thaw_slump_segmentation.models.linknet.model.Linknet","title":"<code>Linknet</code>","text":"<p>               Bases: <code>SegmentationModel</code></p> <p>Linknet_ is a fully convolution neural network for image semantic segmentation. Consist of encoder  and decoder parts connected with skip connections. Encoder extract features of different spatial  resolution (skip connections) which are used by decoder to define accurate segmentation mask. Use sum for fusing decoder blocks with skip connections.</p> Note <p>This implementation by default has 4 skip connections (original - 3).</p> <p>Parameters:</p> Name Type Description Default <code>encoder_name</code> <code>str</code> <p>Name of the classification model that will be used as an encoder (a.k.a backbone) to extract features of different spatial resolution</p> <code>'resnet34'</code> <code>encoder_depth</code> <code>int</code> <p>A number of stages used in encoder in range [3, 5]. Each stage generate features  two times smaller in spatial dimensions than previous one (e.g. for depth 0 we will have features with shapes [(N, C, H, W),], for depth 1 - [(N, C, H, W), (N, C, H // 2, W // 2)] and so on). Default is 5</p> <code>5</code> <code>encoder_weights</code> <code>Optional[str]</code> <p>One of None (random initialization), \"imagenet\" (pre-training on ImageNet) and  other pretrained weights (see table with available weights for each encoder_name)</p> <code>'imagenet'</code> <code>decoder_use_batchnorm</code> <code>bool</code> <p>If True, BatchNorm2d layer between Conv2D and Activation layers is used. If \"inplace\" InplaceABN will be used, allows to decrease memory consumption. Available options are True, False, \"inplace\"</p> <code>True</code> <code>in_channels</code> <code>int</code> <p>A number of input channels for the model, default is 3 (RGB images)</p> <code>3</code> <code>classes</code> <code>int</code> <p>A number of classes for output mask (or you can think as a number of channels of output mask)</p> <code>1</code> <code>activation</code> <code>Optional[Union[str, callable]]</code> <p>An activation function to apply after the final convolution layer. Available options are \"sigmoid\", \"softmax\", \"logsoftmax\", \"tanh\", \"identity\", callable and None. Default is None</p> <code>None</code> <code>aux_params</code> <code>Optional[dict]</code> <p>Dictionary with parameters of the auxiliary output (classification head). Auxiliary output is build  on top of encoder if aux_params is not None (default). Supported params:     - classes (int): A number of classes     - pooling (str): One of \"max\", \"avg\". Default is \"avg\"     - dropout (float): Dropout factor in [0, 1)     - activation (str): An activation function to apply \"sigmoid\"/\"softmax\" (could be None to return logits)</p> <code>None</code> <p>Returns:</p> Type Description <p><code>torch.nn.Module</code>: Linknet</p> <p>.. _Linknet:     https://arxiv.org/abs/1707.03718</p> Source code in <code>src/thaw_slump_segmentation/models/linknet/model.py</code> <pre><code>class Linknet(SegmentationModel):\n    \"\"\"Linknet_ is a fully convolution neural network for image semantic segmentation. Consist of *encoder* \n    and *decoder* parts connected with *skip connections*. Encoder extract features of different spatial \n    resolution (skip connections) which are used by decoder to define accurate segmentation mask. Use *sum*\n    for fusing decoder blocks with skip connections.\n\n    Note:\n        This implementation by default has 4 skip connections (original - 3).\n\n    Args:\n        encoder_name: Name of the classification model that will be used as an encoder (a.k.a backbone)\n            to extract features of different spatial resolution\n        encoder_depth: A number of stages used in encoder in range [3, 5]. Each stage generate features \n            two times smaller in spatial dimensions than previous one (e.g. for depth 0 we will have features\n            with shapes [(N, C, H, W),], for depth 1 - [(N, C, H, W), (N, C, H // 2, W // 2)] and so on).\n            Default is 5\n        encoder_weights: One of **None** (random initialization), **\"imagenet\"** (pre-training on ImageNet) and \n            other pretrained weights (see table with available weights for each encoder_name)\n        decoder_use_batchnorm: If **True**, BatchNorm2d layer between Conv2D and Activation layers\n            is used. If **\"inplace\"** InplaceABN will be used, allows to decrease memory consumption.\n            Available options are **True, False, \"inplace\"**\n        in_channels: A number of input channels for the model, default is 3 (RGB images)\n        classes: A number of classes for output mask (or you can think as a number of channels of output mask)\n        activation: An activation function to apply after the final convolution layer.\n            Available options are **\"sigmoid\"**, **\"softmax\"**, **\"logsoftmax\"**, **\"tanh\"**, **\"identity\"**, **callable** and **None**.\n            Default is **None**\n        aux_params: Dictionary with parameters of the auxiliary output (classification head). Auxiliary output is build \n            on top of encoder if **aux_params** is not **None** (default). Supported params:\n                - classes (int): A number of classes\n                - pooling (str): One of \"max\", \"avg\". Default is \"avg\"\n                - dropout (float): Dropout factor in [0, 1)\n                - activation (str): An activation function to apply \"sigmoid\"/\"softmax\" (could be **None** to return logits)\n\n    Returns:\n        ``torch.nn.Module``: **Linknet**\n\n    .. _Linknet:\n        https://arxiv.org/abs/1707.03718\n    \"\"\"\n\n    def __init__(\n        self,\n        encoder_name: str = \"resnet34\",\n        encoder_depth: int = 5,\n        encoder_weights: Optional[str] = \"imagenet\",\n        decoder_use_batchnorm: bool = True,\n        in_channels: int = 3,\n        classes: int = 1,\n        activation: Optional[Union[str, callable]] = None,\n        aux_params: Optional[dict] = None,\n    ):\n        super().__init__()\n\n        self.encoder = get_encoder(\n            encoder_name,\n            in_channels=in_channels,\n            depth=encoder_depth,\n            weights=encoder_weights,\n        )\n\n        self.decoder = LinknetDecoder(\n            encoder_channels=self.encoder.out_channels,\n            n_blocks=encoder_depth,\n            prefinal_channels=32,\n            use_batchnorm=decoder_use_batchnorm,\n        )\n\n        self.segmentation_head = SegmentationHead(\n            in_channels=32, out_channels=classes, activation=activation, kernel_size=1\n        )\n\n        if aux_params is not None:\n            self.classification_head = ClassificationHead(\n                in_channels=self.encoder.out_channels[-1], **aux_params\n            )\n        else:\n            self.classification_head = None\n\n        self.name = \"link-{}\".format(encoder_name)\n        self.initialize()\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/linknet/model/#src.thaw_slump_segmentation.models.linknet.model.Linknet.classification_head","title":"<code>classification_head = ClassificationHead(in_channels=self.encoder.out_channels[-1], **aux_params)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/linknet/model/#src.thaw_slump_segmentation.models.linknet.model.Linknet.decoder","title":"<code>decoder = LinknetDecoder(encoder_channels=self.encoder.out_channels, n_blocks=encoder_depth, prefinal_channels=32, use_batchnorm=decoder_use_batchnorm)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/linknet/model/#src.thaw_slump_segmentation.models.linknet.model.Linknet.encoder","title":"<code>encoder = get_encoder(encoder_name, in_channels=in_channels, depth=encoder_depth, weights=encoder_weights)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/linknet/model/#src.thaw_slump_segmentation.models.linknet.model.Linknet.name","title":"<code>name = 'link-{}'.format(encoder_name)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/linknet/model/#src.thaw_slump_segmentation.models.linknet.model.Linknet.segmentation_head","title":"<code>segmentation_head = SegmentationHead(in_channels=32, out_channels=classes, activation=activation, kernel_size=1)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/linknet/model/#src.thaw_slump_segmentation.models.linknet.model.Linknet.__init__","title":"<code>__init__(encoder_name='resnet34', encoder_depth=5, encoder_weights='imagenet', decoder_use_batchnorm=True, in_channels=3, classes=1, activation=None, aux_params=None)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/linknet/model.py</code> <pre><code>def __init__(\n    self,\n    encoder_name: str = \"resnet34\",\n    encoder_depth: int = 5,\n    encoder_weights: Optional[str] = \"imagenet\",\n    decoder_use_batchnorm: bool = True,\n    in_channels: int = 3,\n    classes: int = 1,\n    activation: Optional[Union[str, callable]] = None,\n    aux_params: Optional[dict] = None,\n):\n    super().__init__()\n\n    self.encoder = get_encoder(\n        encoder_name,\n        in_channels=in_channels,\n        depth=encoder_depth,\n        weights=encoder_weights,\n    )\n\n    self.decoder = LinknetDecoder(\n        encoder_channels=self.encoder.out_channels,\n        n_blocks=encoder_depth,\n        prefinal_channels=32,\n        use_batchnorm=decoder_use_batchnorm,\n    )\n\n    self.segmentation_head = SegmentationHead(\n        in_channels=32, out_channels=classes, activation=activation, kernel_size=1\n    )\n\n    if aux_params is not None:\n        self.classification_head = ClassificationHead(\n            in_channels=self.encoder.out_channels[-1], **aux_params\n        )\n    else:\n        self.classification_head = None\n\n    self.name = \"link-{}\".format(encoder_name)\n    self.initialize()\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/losses/_functional/","title":"functional","text":""},{"location":"reference/src/thaw_slump_segmentation/models/losses/_functional/#src.thaw_slump_segmentation.models.losses._functional.__all__","title":"<code>__all__ = ['focal_loss_with_logits', 'softmax_focal_loss_with_logits', 'soft_jaccard_score', 'soft_dice_score', 'wing_loss']</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/losses/_functional/#src.thaw_slump_segmentation.models.losses._functional.focal_loss_with_logits","title":"<code>focal_loss_with_logits(output, target, gamma=2.0, alpha=0.25, reduction='mean', normalized=False, reduced_threshold=None, eps=1e-06)</code>","text":"<p>Compute binary focal loss between target and output logits. See :class:<code>~pytorch_toolbelt.losses.FocalLoss</code> for details.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>Tensor</code> <p>Tensor of arbitrary shape (predictions of the model)</p> required <code>target</code> <code>Tensor</code> <p>Tensor of the same shape as input</p> required <code>gamma</code> <code>float</code> <p>Focal loss power factor</p> <code>2.0</code> <code>alpha</code> <code>Optional[float]</code> <p>Weight factor to balance positive and negative samples. Alpha must be in [0...1] range, high values will give more weight to positive class.</p> <code>0.25</code> <code>reduction</code> <code>string</code> <p>Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum' | 'batchwise_mean'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: :attr:<code>size_average</code> and :attr:<code>reduce</code> are in the process of being deprecated, and in the meantime, specifying either of those two args will override :attr:<code>reduction</code>. 'batchwise_mean' computes mean loss per sample in batch. Default: 'mean'</p> <code>'mean'</code> <code>normalized</code> <code>bool</code> <p>Compute normalized focal loss (https://arxiv.org/pdf/1909.07829.pdf).</p> <code>False</code> <code>reduced_threshold</code> <code>float</code> <p>Compute reduced focal loss (https://arxiv.org/abs/1903.01347).</p> <code>None</code> References <p>https://github.com/open-mmlab/mmdetection/blob/master/mmdet/core/loss/losses.py</p> Source code in <code>src/thaw_slump_segmentation/models/losses/_functional.py</code> <pre><code>def focal_loss_with_logits(\n    output: torch.Tensor,\n    target: torch.Tensor,\n    gamma: float = 2.0,\n    alpha: Optional[float] = 0.25,\n    reduction: str = \"mean\",\n    normalized: bool = False,\n    reduced_threshold: Optional[float] = None,\n    eps: float = 1e-6,\n) -&gt; torch.Tensor:\n    \"\"\"Compute binary focal loss between target and output logits.\n    See :class:`~pytorch_toolbelt.losses.FocalLoss` for details.\n\n    Args:\n        output: Tensor of arbitrary shape (predictions of the model)\n        target: Tensor of the same shape as input\n        gamma: Focal loss power factor\n        alpha: Weight factor to balance positive and negative samples. Alpha must be in [0...1] range,\n            high values will give more weight to positive class.\n        reduction (string, optional): Specifies the reduction to apply to the output:\n            'none' | 'mean' | 'sum' | 'batchwise_mean'. 'none': no reduction will be applied,\n            'mean': the sum of the output will be divided by the number of\n            elements in the output, 'sum': the output will be summed. Note: :attr:`size_average`\n            and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n            specifying either of those two args will override :attr:`reduction`.\n            'batchwise_mean' computes mean loss per sample in batch. Default: 'mean'\n        normalized (bool): Compute normalized focal loss (https://arxiv.org/pdf/1909.07829.pdf).\n        reduced_threshold (float, optional): Compute reduced focal loss (https://arxiv.org/abs/1903.01347).\n\n    References:\n        https://github.com/open-mmlab/mmdetection/blob/master/mmdet/core/loss/losses.py\n    \"\"\"\n    target = target.type(output.type())\n\n    logpt = F.binary_cross_entropy_with_logits(output, target, reduction=\"none\")\n    pt = torch.exp(-logpt)\n\n    # compute the loss\n    if reduced_threshold is None:\n        focal_term = (1.0 - pt).pow(gamma)\n    else:\n        focal_term = ((1.0 - pt) / reduced_threshold).pow(gamma)\n        focal_term[pt &lt; reduced_threshold] = 1\n\n    loss = focal_term * logpt\n\n    if alpha is not None:\n        loss *= alpha * target + (1 - alpha) * (1 - target)\n\n    if normalized:\n        norm_factor = focal_term.sum().clamp_min(eps)\n        loss /= norm_factor\n\n    if reduction == \"mean\":\n        loss = loss.mean()\n    if reduction == \"sum\":\n        loss = loss.sum()\n    if reduction == \"batchwise_mean\":\n        loss = loss.sum(0)\n\n    return loss\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/losses/_functional/#src.thaw_slump_segmentation.models.losses._functional.label_smoothed_nll_loss","title":"<code>label_smoothed_nll_loss(lprobs, target, epsilon, ignore_index=None, reduction='mean', dim=-1)</code>","text":"<p>Source: https://github.com/pytorch/fairseq/blob/master/fairseq/criterions/label_smoothed_cross_entropy.py :param lprobs: Log-probabilities of predictions (e.g after log_softmax) :param target: :param epsilon: :param ignore_index: :param reduction: :return:</p> Source code in <code>src/thaw_slump_segmentation/models/losses/_functional.py</code> <pre><code>def label_smoothed_nll_loss(\n    lprobs: torch.Tensor, target: torch.Tensor, epsilon: float, ignore_index=None, reduction=\"mean\", dim=-1\n) -&gt; torch.Tensor:\n    \"\"\"\n    Source: https://github.com/pytorch/fairseq/blob/master/fairseq/criterions/label_smoothed_cross_entropy.py\n    :param lprobs: Log-probabilities of predictions (e.g after log_softmax)\n    :param target:\n    :param epsilon:\n    :param ignore_index:\n    :param reduction:\n    :return:\n    \"\"\"\n    if target.dim() == lprobs.dim() - 1:\n        target = target.unsqueeze(dim)\n\n    if ignore_index is not None:\n        pad_mask = target.eq(ignore_index)\n        target = target.masked_fill(pad_mask, 0)\n        nll_loss = -lprobs.gather(dim=dim, index=target)\n        smooth_loss = -lprobs.sum(dim=dim, keepdim=True)\n\n        # nll_loss.masked_fill_(pad_mask, 0.0)\n        # smooth_loss.masked_fill_(pad_mask, 0.0)\n        nll_loss = nll_loss.masked_fill(pad_mask, 0.0)\n        smooth_loss = smooth_loss.masked_fill(pad_mask, 0.0)\n    else:\n        nll_loss = -lprobs.gather(dim=dim, index=target)\n        smooth_loss = -lprobs.sum(dim=dim, keepdim=True)\n\n        nll_loss = nll_loss.squeeze(dim)\n        smooth_loss = smooth_loss.squeeze(dim)\n\n    if reduction == \"sum\":\n        nll_loss = nll_loss.sum()\n        smooth_loss = smooth_loss.sum()\n    if reduction == \"mean\":\n        nll_loss = nll_loss.mean()\n        smooth_loss = smooth_loss.mean()\n\n    eps_i = epsilon / lprobs.size(dim)\n    loss = (1.0 - epsilon) * nll_loss + eps_i * smooth_loss\n    return loss\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/losses/_functional/#src.thaw_slump_segmentation.models.losses._functional.soft_dice_score","title":"<code>soft_dice_score(output, target, smooth=0.0, eps=1e-07, dims=None)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/losses/_functional.py</code> <pre><code>def soft_dice_score(\n    output: torch.Tensor, target: torch.Tensor, smooth: float = 0.0, eps: float = 1e-7, dims=None\n) -&gt; torch.Tensor:\n    assert output.size() == target.size()\n    if dims is not None:\n        intersection = torch.sum(output * target, dim=dims)\n        cardinality = torch.sum(output + target, dim=dims)\n    else:\n        intersection = torch.sum(output * target)\n        cardinality = torch.sum(output + target)\n    dice_score = (2.0 * intersection + smooth) / (cardinality + smooth).clamp_min(eps)\n    return dice_score\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/losses/_functional/#src.thaw_slump_segmentation.models.losses._functional.soft_jaccard_score","title":"<code>soft_jaccard_score(output, target, smooth=0.0, eps=1e-07, dims=None)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/losses/_functional.py</code> <pre><code>def soft_jaccard_score(\n    output: torch.Tensor, target: torch.Tensor, smooth: float = 0.0, eps: float = 1e-7, dims=None\n) -&gt; torch.Tensor:\n    assert output.size() == target.size()\n    if dims is not None:\n        intersection = torch.sum(output * target, dim=dims)\n        cardinality = torch.sum(output + target, dim=dims)\n    else:\n        intersection = torch.sum(output * target)\n        cardinality = torch.sum(output + target)\n\n    union = cardinality - intersection\n    jaccard_score = (intersection + smooth) / (union + smooth).clamp_min(eps)\n    return jaccard_score\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/losses/_functional/#src.thaw_slump_segmentation.models.losses._functional.softmax_focal_loss_with_logits","title":"<code>softmax_focal_loss_with_logits(output, target, gamma=2.0, reduction='mean', normalized=False, reduced_threshold=None, eps=1e-06)</code>","text":"<p>Softmax version of focal loss between target and output logits. See :class:<code>~pytorch_toolbelt.losses.FocalLoss</code> for details.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>Tensor</code> <p>Tensor of shape [B, C, *] (Similar to nn.CrossEntropyLoss)</p> required <code>target</code> <code>Tensor</code> <p>Tensor of shape [B, *] (Similar to nn.CrossEntropyLoss)</p> required <code>reduction</code> <code>string</code> <p>Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum' | 'batchwise_mean'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: :attr:<code>size_average</code> and :attr:<code>reduce</code> are in the process of being deprecated, and in the meantime, specifying either of those two args will override :attr:<code>reduction</code>. 'batchwise_mean' computes mean loss per sample in batch. Default: 'mean'</p> <code>'mean'</code> <code>normalized</code> <code>bool</code> <p>Compute normalized focal loss (https://arxiv.org/pdf/1909.07829.pdf).</p> <code>False</code> <code>reduced_threshold</code> <code>float</code> <p>Compute reduced focal loss (https://arxiv.org/abs/1903.01347).</p> <code>None</code> Source code in <code>src/thaw_slump_segmentation/models/losses/_functional.py</code> <pre><code>def softmax_focal_loss_with_logits(\n    output: torch.Tensor,\n    target: torch.Tensor,\n    gamma: float = 2.0,\n    reduction=\"mean\",\n    normalized=False,\n    reduced_threshold: Optional[float] = None,\n    eps: float = 1e-6,\n) -&gt; torch.Tensor:\n    \"\"\"Softmax version of focal loss between target and output logits.\n    See :class:`~pytorch_toolbelt.losses.FocalLoss` for details.\n\n    Args:\n        output: Tensor of shape [B, C, *] (Similar to nn.CrossEntropyLoss)\n        target: Tensor of shape [B, *] (Similar to nn.CrossEntropyLoss)\n        reduction (string, optional): Specifies the reduction to apply to the output:\n            'none' | 'mean' | 'sum' | 'batchwise_mean'. 'none': no reduction will be applied,\n            'mean': the sum of the output will be divided by the number of\n            elements in the output, 'sum': the output will be summed. Note: :attr:`size_average`\n            and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n            specifying either of those two args will override :attr:`reduction`.\n            'batchwise_mean' computes mean loss per sample in batch. Default: 'mean'\n        normalized (bool): Compute normalized focal loss (https://arxiv.org/pdf/1909.07829.pdf).\n        reduced_threshold (float, optional): Compute reduced focal loss (https://arxiv.org/abs/1903.01347).\n    \"\"\"\n    log_softmax = F.log_softmax(output, dim=1)\n\n    loss = F.nll_loss(log_softmax, target, reduction=\"none\")\n    pt = torch.exp(-loss)\n\n    # compute the loss\n    if reduced_threshold is None:\n        focal_term = (1.0 - pt).pow(gamma)\n    else:\n        focal_term = ((1.0 - pt) / reduced_threshold).pow(gamma)\n        focal_term[pt &lt; reduced_threshold] = 1\n\n    loss = focal_term * loss\n\n    if normalized:\n        norm_factor = focal_term.sum().clamp_min(eps)\n        loss = loss / norm_factor\n\n    if reduction == \"mean\":\n        loss = loss.mean()\n    if reduction == \"sum\":\n        loss = loss.sum()\n    if reduction == \"batchwise_mean\":\n        loss = loss.sum(0)\n\n    return loss\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/losses/_functional/#src.thaw_slump_segmentation.models.losses._functional.to_tensor","title":"<code>to_tensor(x, dtype=None)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/losses/_functional.py</code> <pre><code>def to_tensor(x, dtype=None) -&gt; torch.Tensor:\n    if isinstance(x, torch.Tensor):\n        if dtype is not None:\n            x = x.type(dtype)\n        return x\n    if isinstance(x, np.ndarray):\n        x = torch.from_numpy(x)\n        if dtype is not None:\n            x = x.type(dtype)\n        return x\n    if isinstance(x, (list, tuple)):\n        x = np.ndarray(x)\n        x = torch.from_numpy(x)\n        if dtype is not None:\n            x = x.type(dtype)\n        return x\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/losses/_functional/#src.thaw_slump_segmentation.models.losses._functional.wing_loss","title":"<code>wing_loss(output, target, width=5, curvature=0.5, reduction='mean')</code>","text":"<p>https://arxiv.org/pdf/1711.06753.pdf :param output: :param target: :param width: :param curvature: :param reduction: :return:</p> Source code in <code>src/thaw_slump_segmentation/models/losses/_functional.py</code> <pre><code>def wing_loss(output: torch.Tensor, target: torch.Tensor, width=5, curvature=0.5, reduction=\"mean\"):\n    \"\"\"\n    https://arxiv.org/pdf/1711.06753.pdf\n    :param output:\n    :param target:\n    :param width:\n    :param curvature:\n    :param reduction:\n    :return:\n    \"\"\"\n    diff_abs = (target - output).abs()\n    loss = diff_abs.clone()\n\n    idx_smaller = diff_abs &lt; width\n    idx_bigger = diff_abs &gt;= width\n\n    loss[idx_smaller] = width * torch.log(1 + diff_abs[idx_smaller] / curvature)\n\n    C = width - width * math.log(1 + width / curvature)\n    loss[idx_bigger] = loss[idx_bigger] - C\n\n    if reduction == \"sum\":\n        loss = loss.sum()\n\n    if reduction == \"mean\":\n        loss = loss.mean()\n\n    return loss\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/losses/constants/","title":"Constants","text":""},{"location":"reference/src/thaw_slump_segmentation/models/losses/constants/#src.thaw_slump_segmentation.models.losses.constants.BINARY_MODE","title":"<code>BINARY_MODE: str = 'binary'</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/losses/constants/#src.thaw_slump_segmentation.models.losses.constants.MULTICLASS_MODE","title":"<code>MULTICLASS_MODE: str = 'multiclass'</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/losses/constants/#src.thaw_slump_segmentation.models.losses.constants.MULTILABEL_MODE","title":"<code>MULTILABEL_MODE: str = 'multilabel'</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/losses/dice/","title":"Dice","text":""},{"location":"reference/src/thaw_slump_segmentation/models/losses/dice/#src.thaw_slump_segmentation.models.losses.dice.__all__","title":"<code>__all__ = ['DiceLoss']</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/losses/dice/#src.thaw_slump_segmentation.models.losses.dice.DiceLoss","title":"<code>DiceLoss</code>","text":"<p>               Bases: <code>_Loss</code></p> Source code in <code>src/thaw_slump_segmentation/models/losses/dice.py</code> <pre><code>class DiceLoss(_Loss):\n\n    def __init__(\n        self,\n        mode: str,\n        classes: Optional[List[int]] = None,\n        log_loss: bool = False,\n        from_logits: bool = True,\n        smooth: float = 0.0,\n        ignore_index: Optional[int] = None,\n        eps: float = 1e-7,\n    ):\n        \"\"\"Implementation of Dice loss for image segmentation task.\n        It supports binary, multiclass and multilabel cases\n\n        Args:\n            mode: Loss mode 'binary', 'multiclass' or 'multilabel'\n            classes:  List of classes that contribute in loss computation. By default, all channels are included.\n            log_loss: If True, loss computed as `- log(dice_coeff)`, otherwise `1 - dice_coeff`\n            from_logits: If True, assumes input is raw logits\n            smooth: Smoothness constant for dice coefficient (a)\n            ignore_index: Label that indicates ignored pixels (does not contribute to loss)\n            eps: A small epsilon for numerical stability to avoid zero division error \n                (denominator will be always greater or equal to eps)\n\n        Shape\n             - **y_pred** - torch.Tensor of shape (N, C, H, W)\n             - **y_true** - torch.Tensor of shape (N, H, W) or (N, C, H, W)\n\n        Reference\n            https://github.com/BloodAxe/pytorch-toolbelt\n        \"\"\"\n        assert mode in {BINARY_MODE, MULTILABEL_MODE, MULTICLASS_MODE}\n        super(DiceLoss, self).__init__()\n        self.mode = mode\n        if classes is not None:\n            assert mode != BINARY_MODE, \"Masking classes is not supported with mode=binary\"\n            classes = to_tensor(classes, dtype=torch.long)\n\n        self.classes = classes\n        self.from_logits = from_logits\n        self.smooth = smooth\n        self.eps = eps\n        self.log_loss = log_loss\n\n    def forward(self, y_pred: torch.Tensor, y_true: torch.Tensor) -&gt; torch.Tensor:\n\n        assert y_true.size(0) == y_pred.size(0)\n\n        if self.from_logits:\n            # Apply activations to get [0..1] class probabilities\n            # Using Log-Exp as this gives more numerically stable result and does not cause vanishing gradient on\n            # extreme values 0 and 1\n            if self.mode == MULTICLASS_MODE:\n                y_pred = y_pred.log_softmax(dim=1).exp()\n            else:\n                y_pred = F.logsigmoid(y_pred).exp()\n\n        bs = y_true.size(0)\n        num_classes = y_pred.size(1)\n        dims = (0, 2)\n\n        if self.mode == BINARY_MODE:\n            y_true = y_true.view(bs, 1, -1)\n            y_pred = y_pred.view(bs, 1, -1)\n\n        if self.mode == MULTICLASS_MODE:\n            y_true = y_true.view(bs, -1)\n            y_pred = y_pred.view(bs, num_classes, -1)\n\n            y_true = F.one_hot(y_true, num_classes)  # N,H*W -&gt; N,H*W, C\n            y_true = y_true.permute(0, 2, 1)  # H, C, H*W\n\n        if self.mode == MULTILABEL_MODE:\n            y_true = y_true.view(bs, num_classes, -1)\n            y_pred = y_pred.view(bs, num_classes, -1)\n\n        scores = soft_dice_score(y_pred, y_true.type_as(y_pred), smooth=self.smooth, eps=self.eps, dims=dims)\n\n        if self.log_loss:\n            loss = -torch.log(scores.clamp_min(self.eps))\n        else:\n            loss = 1.0 - scores\n\n        # Dice loss is undefined for non-empty classes\n        # So we zero contribution of channel that does not have true pixels\n        # NOTE: A better workaround would be to use loss term `mean(y_pred)`\n        # for this case, however it will be a modified jaccard loss\n\n        mask = y_true.sum(dims) &gt; 0\n        loss *= mask.to(loss.dtype)\n\n        if self.classes is not None:\n            loss = loss[self.classes]\n\n        return loss.mean()\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/losses/dice/#src.thaw_slump_segmentation.models.losses.dice.DiceLoss.classes","title":"<code>classes = classes</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/losses/dice/#src.thaw_slump_segmentation.models.losses.dice.DiceLoss.eps","title":"<code>eps = eps</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/losses/dice/#src.thaw_slump_segmentation.models.losses.dice.DiceLoss.from_logits","title":"<code>from_logits = from_logits</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/losses/dice/#src.thaw_slump_segmentation.models.losses.dice.DiceLoss.log_loss","title":"<code>log_loss = log_loss</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/losses/dice/#src.thaw_slump_segmentation.models.losses.dice.DiceLoss.mode","title":"<code>mode = mode</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/losses/dice/#src.thaw_slump_segmentation.models.losses.dice.DiceLoss.smooth","title":"<code>smooth = smooth</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/losses/dice/#src.thaw_slump_segmentation.models.losses.dice.DiceLoss.__init__","title":"<code>__init__(mode, classes=None, log_loss=False, from_logits=True, smooth=0.0, ignore_index=None, eps=1e-07)</code>","text":"<p>Implementation of Dice loss for image segmentation task. It supports binary, multiclass and multilabel cases</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>Loss mode 'binary', 'multiclass' or 'multilabel'</p> required <code>classes</code> <code>Optional[List[int]]</code> <p>List of classes that contribute in loss computation. By default, all channels are included.</p> <code>None</code> <code>log_loss</code> <code>bool</code> <p>If True, loss computed as <code>- log(dice_coeff)</code>, otherwise <code>1 - dice_coeff</code></p> <code>False</code> <code>from_logits</code> <code>bool</code> <p>If True, assumes input is raw logits</p> <code>True</code> <code>smooth</code> <code>float</code> <p>Smoothness constant for dice coefficient (a)</p> <code>0.0</code> <code>ignore_index</code> <code>Optional[int]</code> <p>Label that indicates ignored pixels (does not contribute to loss)</p> <code>None</code> <code>eps</code> <code>float</code> <p>A small epsilon for numerical stability to avoid zero division error  (denominator will be always greater or equal to eps)</p> <code>1e-07</code> <p>Shape      - y_pred - torch.Tensor of shape (N, C, H, W)      - y_true - torch.Tensor of shape (N, H, W) or (N, C, H, W)</p> <p>Reference     https://github.com/BloodAxe/pytorch-toolbelt</p> Source code in <code>src/thaw_slump_segmentation/models/losses/dice.py</code> <pre><code>def __init__(\n    self,\n    mode: str,\n    classes: Optional[List[int]] = None,\n    log_loss: bool = False,\n    from_logits: bool = True,\n    smooth: float = 0.0,\n    ignore_index: Optional[int] = None,\n    eps: float = 1e-7,\n):\n    \"\"\"Implementation of Dice loss for image segmentation task.\n    It supports binary, multiclass and multilabel cases\n\n    Args:\n        mode: Loss mode 'binary', 'multiclass' or 'multilabel'\n        classes:  List of classes that contribute in loss computation. By default, all channels are included.\n        log_loss: If True, loss computed as `- log(dice_coeff)`, otherwise `1 - dice_coeff`\n        from_logits: If True, assumes input is raw logits\n        smooth: Smoothness constant for dice coefficient (a)\n        ignore_index: Label that indicates ignored pixels (does not contribute to loss)\n        eps: A small epsilon for numerical stability to avoid zero division error \n            (denominator will be always greater or equal to eps)\n\n    Shape\n         - **y_pred** - torch.Tensor of shape (N, C, H, W)\n         - **y_true** - torch.Tensor of shape (N, H, W) or (N, C, H, W)\n\n    Reference\n        https://github.com/BloodAxe/pytorch-toolbelt\n    \"\"\"\n    assert mode in {BINARY_MODE, MULTILABEL_MODE, MULTICLASS_MODE}\n    super(DiceLoss, self).__init__()\n    self.mode = mode\n    if classes is not None:\n        assert mode != BINARY_MODE, \"Masking classes is not supported with mode=binary\"\n        classes = to_tensor(classes, dtype=torch.long)\n\n    self.classes = classes\n    self.from_logits = from_logits\n    self.smooth = smooth\n    self.eps = eps\n    self.log_loss = log_loss\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/losses/dice/#src.thaw_slump_segmentation.models.losses.dice.DiceLoss.forward","title":"<code>forward(y_pred, y_true)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/losses/dice.py</code> <pre><code>def forward(self, y_pred: torch.Tensor, y_true: torch.Tensor) -&gt; torch.Tensor:\n\n    assert y_true.size(0) == y_pred.size(0)\n\n    if self.from_logits:\n        # Apply activations to get [0..1] class probabilities\n        # Using Log-Exp as this gives more numerically stable result and does not cause vanishing gradient on\n        # extreme values 0 and 1\n        if self.mode == MULTICLASS_MODE:\n            y_pred = y_pred.log_softmax(dim=1).exp()\n        else:\n            y_pred = F.logsigmoid(y_pred).exp()\n\n    bs = y_true.size(0)\n    num_classes = y_pred.size(1)\n    dims = (0, 2)\n\n    if self.mode == BINARY_MODE:\n        y_true = y_true.view(bs, 1, -1)\n        y_pred = y_pred.view(bs, 1, -1)\n\n    if self.mode == MULTICLASS_MODE:\n        y_true = y_true.view(bs, -1)\n        y_pred = y_pred.view(bs, num_classes, -1)\n\n        y_true = F.one_hot(y_true, num_classes)  # N,H*W -&gt; N,H*W, C\n        y_true = y_true.permute(0, 2, 1)  # H, C, H*W\n\n    if self.mode == MULTILABEL_MODE:\n        y_true = y_true.view(bs, num_classes, -1)\n        y_pred = y_pred.view(bs, num_classes, -1)\n\n    scores = soft_dice_score(y_pred, y_true.type_as(y_pred), smooth=self.smooth, eps=self.eps, dims=dims)\n\n    if self.log_loss:\n        loss = -torch.log(scores.clamp_min(self.eps))\n    else:\n        loss = 1.0 - scores\n\n    # Dice loss is undefined for non-empty classes\n    # So we zero contribution of channel that does not have true pixels\n    # NOTE: A better workaround would be to use loss term `mean(y_pred)`\n    # for this case, however it will be a modified jaccard loss\n\n    mask = y_true.sum(dims) &gt; 0\n    loss *= mask.to(loss.dtype)\n\n    if self.classes is not None:\n        loss = loss[self.classes]\n\n    return loss.mean()\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/losses/focal/","title":"Focal","text":""},{"location":"reference/src/thaw_slump_segmentation/models/losses/focal/#src.thaw_slump_segmentation.models.losses.focal.__all__","title":"<code>__all__ = ['FocalLoss']</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/losses/focal/#src.thaw_slump_segmentation.models.losses.focal.FocalLoss","title":"<code>FocalLoss</code>","text":"<p>               Bases: <code>_Loss</code></p> Source code in <code>src/thaw_slump_segmentation/models/losses/focal.py</code> <pre><code>class FocalLoss(_Loss):\n\n    def __init__(\n        self,\n        mode: str,\n        alpha: Optional[float] = None,\n        gamma: Optional[float] = 2.,\n        ignore_index: Optional[int] = None, \n        reduction: Optional[str] = \"mean\",\n        normalized: bool = False,\n        reduced_threshold: Optional[float] = None,\n    ):\n        \"\"\"Compute Focal loss\n\n        Args:\n            mode: Loss mode 'binary', 'multiclass' or 'multilabel'\n            alpha: Prior probability of having positive value in target.\n            gamma: Power factor for dampening weight (focal strength).\n            ignore_index: If not None, targets may contain values to be ignored.\n                Target values equal to ignore_index will be ignored from loss computation.\n            normalized: Compute normalized focal loss (https://arxiv.org/pdf/1909.07829.pdf).\n            reduced_threshold: Switch to reduced focal loss. Note, when using this mode you should use `reduction=\"sum\"`.\n\n        Shape\n             - **y_pred** - torch.Tensor of shape (N, C, H, W)\n             - **y_true** - torch.Tensor of shape (N, H, W) or (N, C, H, W)\n\n        Reference\n            https://github.com/BloodAxe/pytorch-toolbelt\n\n        \"\"\"\n        assert mode in {BINARY_MODE, MULTILABEL_MODE, MULTICLASS_MODE}\n        super().__init__()\n\n        self.mode = mode\n        self.ignore_index = ignore_index\n        self.focal_loss_fn = partial(\n            focal_loss_with_logits,\n            alpha=alpha,\n            gamma=gamma,\n            reduced_threshold=reduced_threshold,\n            reduction=reduction,\n            normalized=normalized,\n        )\n\n    def forward(self, y_pred: torch.Tensor, y_true: torch.Tensor) -&gt; torch.Tensor:\n\n        if self.mode in {BINARY_MODE, MULTILABEL_MODE}:\n            y_true = y_true.view(-1)\n            y_pred = y_pred.view(-1)\n\n            if self.ignore_index is not None:\n                # Filter predictions with ignore label from loss computation\n                not_ignored = y_true != self.ignore_index\n                y_pred = y_pred[not_ignored]\n                y_true = y_true[not_ignored]\n\n            loss = self.focal_loss_fn(y_pred, y_true)\n\n        elif self.mode == MULTICLASS_MODE:\n\n            num_classes = y_pred.size(1)\n            loss = 0\n\n            # Filter anchors with -1 label from loss computation\n            if self.ignore_index is not None:\n                not_ignored = y_true != self.ignore_index\n\n            for cls in range(num_classes):\n                cls_y_true = (y_true == cls).long()\n                cls_y_pred = y_pred[:, cls, ...]\n\n                if self.ignore_index is not None:\n                    cls_y_true = cls_y_true[not_ignored]\n                    cls_y_pred = cls_y_pred[not_ignored]\n\n                loss += self.focal_loss_fn(cls_y_pred, cls_y_true)\n\n        return loss\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/losses/focal/#src.thaw_slump_segmentation.models.losses.focal.FocalLoss.focal_loss_fn","title":"<code>focal_loss_fn = partial(focal_loss_with_logits, alpha=alpha, gamma=gamma, reduced_threshold=reduced_threshold, reduction=reduction, normalized=normalized)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/losses/focal/#src.thaw_slump_segmentation.models.losses.focal.FocalLoss.ignore_index","title":"<code>ignore_index = ignore_index</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/losses/focal/#src.thaw_slump_segmentation.models.losses.focal.FocalLoss.mode","title":"<code>mode = mode</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/losses/focal/#src.thaw_slump_segmentation.models.losses.focal.FocalLoss.__init__","title":"<code>__init__(mode, alpha=None, gamma=2.0, ignore_index=None, reduction='mean', normalized=False, reduced_threshold=None)</code>","text":"<p>Compute Focal loss</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>Loss mode 'binary', 'multiclass' or 'multilabel'</p> required <code>alpha</code> <code>Optional[float]</code> <p>Prior probability of having positive value in target.</p> <code>None</code> <code>gamma</code> <code>Optional[float]</code> <p>Power factor for dampening weight (focal strength).</p> <code>2.0</code> <code>ignore_index</code> <code>Optional[int]</code> <p>If not None, targets may contain values to be ignored. Target values equal to ignore_index will be ignored from loss computation.</p> <code>None</code> <code>normalized</code> <code>bool</code> <p>Compute normalized focal loss (https://arxiv.org/pdf/1909.07829.pdf).</p> <code>False</code> <code>reduced_threshold</code> <code>Optional[float]</code> <p>Switch to reduced focal loss. Note, when using this mode you should use <code>reduction=\"sum\"</code>.</p> <code>None</code> <p>Shape      - y_pred - torch.Tensor of shape (N, C, H, W)      - y_true - torch.Tensor of shape (N, H, W) or (N, C, H, W)</p> <p>Reference     https://github.com/BloodAxe/pytorch-toolbelt</p> Source code in <code>src/thaw_slump_segmentation/models/losses/focal.py</code> <pre><code>def __init__(\n    self,\n    mode: str,\n    alpha: Optional[float] = None,\n    gamma: Optional[float] = 2.,\n    ignore_index: Optional[int] = None, \n    reduction: Optional[str] = \"mean\",\n    normalized: bool = False,\n    reduced_threshold: Optional[float] = None,\n):\n    \"\"\"Compute Focal loss\n\n    Args:\n        mode: Loss mode 'binary', 'multiclass' or 'multilabel'\n        alpha: Prior probability of having positive value in target.\n        gamma: Power factor for dampening weight (focal strength).\n        ignore_index: If not None, targets may contain values to be ignored.\n            Target values equal to ignore_index will be ignored from loss computation.\n        normalized: Compute normalized focal loss (https://arxiv.org/pdf/1909.07829.pdf).\n        reduced_threshold: Switch to reduced focal loss. Note, when using this mode you should use `reduction=\"sum\"`.\n\n    Shape\n         - **y_pred** - torch.Tensor of shape (N, C, H, W)\n         - **y_true** - torch.Tensor of shape (N, H, W) or (N, C, H, W)\n\n    Reference\n        https://github.com/BloodAxe/pytorch-toolbelt\n\n    \"\"\"\n    assert mode in {BINARY_MODE, MULTILABEL_MODE, MULTICLASS_MODE}\n    super().__init__()\n\n    self.mode = mode\n    self.ignore_index = ignore_index\n    self.focal_loss_fn = partial(\n        focal_loss_with_logits,\n        alpha=alpha,\n        gamma=gamma,\n        reduced_threshold=reduced_threshold,\n        reduction=reduction,\n        normalized=normalized,\n    )\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/losses/focal/#src.thaw_slump_segmentation.models.losses.focal.FocalLoss.forward","title":"<code>forward(y_pred, y_true)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/losses/focal.py</code> <pre><code>def forward(self, y_pred: torch.Tensor, y_true: torch.Tensor) -&gt; torch.Tensor:\n\n    if self.mode in {BINARY_MODE, MULTILABEL_MODE}:\n        y_true = y_true.view(-1)\n        y_pred = y_pred.view(-1)\n\n        if self.ignore_index is not None:\n            # Filter predictions with ignore label from loss computation\n            not_ignored = y_true != self.ignore_index\n            y_pred = y_pred[not_ignored]\n            y_true = y_true[not_ignored]\n\n        loss = self.focal_loss_fn(y_pred, y_true)\n\n    elif self.mode == MULTICLASS_MODE:\n\n        num_classes = y_pred.size(1)\n        loss = 0\n\n        # Filter anchors with -1 label from loss computation\n        if self.ignore_index is not None:\n            not_ignored = y_true != self.ignore_index\n\n        for cls in range(num_classes):\n            cls_y_true = (y_true == cls).long()\n            cls_y_pred = y_pred[:, cls, ...]\n\n            if self.ignore_index is not None:\n                cls_y_true = cls_y_true[not_ignored]\n                cls_y_pred = cls_y_pred[not_ignored]\n\n            loss += self.focal_loss_fn(cls_y_pred, cls_y_true)\n\n    return loss\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/losses/jaccard/","title":"Jaccard","text":""},{"location":"reference/src/thaw_slump_segmentation/models/losses/jaccard/#src.thaw_slump_segmentation.models.losses.jaccard.__all__","title":"<code>__all__ = ['JaccardLoss']</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/losses/jaccard/#src.thaw_slump_segmentation.models.losses.jaccard.JaccardLoss","title":"<code>JaccardLoss</code>","text":"<p>               Bases: <code>_Loss</code></p> Source code in <code>src/thaw_slump_segmentation/models/losses/jaccard.py</code> <pre><code>class JaccardLoss(_Loss):\n\n    def __init__(\n        self,\n        mode: str,\n        classes: Optional[List[int]] = None,\n        log_loss: bool = False,\n        from_logits: bool = True,\n        smooth: float = 0.,\n        eps: float = 1e-7,\n    ):\n        \"\"\"Implementation of Jaccard loss for image segmentation task.\n        It supports binary, multiclass and multilabel cases\n\n        Args:\n            mode: Loss mode 'binary', 'multiclass' or 'multilabel'\n            classes:  List of classes that contribute in loss computation. By default, all channels are included.\n            log_loss: If True, loss computed as `- log(jaccard_coeff)`, otherwise `1 - jaccard_coeff`\n            from_logits: If True, assumes input is raw logits\n            smooth: Smoothness constant for dice coefficient\n            ignore_index: Label that indicates ignored pixels (does not contribute to loss)\n            eps: A small epsilon for numerical stability to avoid zero division error \n                (denominator will be always greater or equal to eps)\n\n        Shape\n             - **y_pred** - torch.Tensor of shape (N, C, H, W)\n             - **y_true** - torch.Tensor of shape (N, H, W) or (N, C, H, W)\n\n        Reference\n            https://github.com/BloodAxe/pytorch-toolbelt\n        \"\"\"\n        assert mode in {BINARY_MODE, MULTILABEL_MODE, MULTICLASS_MODE}\n        super(JaccardLoss, self).__init__()\n\n        self.mode = mode\n        if classes is not None:\n            assert mode != BINARY_MODE, \"Masking classes is not supported with mode=binary\"\n            classes = to_tensor(classes, dtype=torch.long)\n\n        self.classes = classes\n        self.from_logits = from_logits\n        self.smooth = smooth\n        self.eps = eps\n        self.log_loss = log_loss\n\n    def forward(self, y_pred: torch.Tensor, y_true: torch.Tensor) -&gt; torch.Tensor:\n\n        assert y_true.size(0) == y_pred.size(0)\n\n        if self.from_logits:\n            # Apply activations to get [0..1] class probabilities\n            # Using Log-Exp as this gives more numerically stable result and does not cause vanishing gradient on\n            # extreme values 0 and 1\n            if self.mode == MULTICLASS_MODE:\n                y_pred = y_pred.log_softmax(dim=1).exp()\n            else:\n                y_pred = F.logsigmoid(y_pred).exp()\n\n        bs = y_true.size(0)\n        num_classes = y_pred.size(1)\n        dims = (0, 2)\n\n        if self.mode == BINARY_MODE:\n            y_true = y_true.view(bs, 1, -1)\n            y_pred = y_pred.view(bs, 1, -1)\n\n        if self.mode == MULTICLASS_MODE:\n            y_true = y_true.view(bs, -1)\n            y_pred = y_pred.view(bs, num_classes, -1)\n\n            y_true = F.one_hot(y_true, num_classes)  # N,H*W -&gt; N,H*W, C\n            y_true = y_true.permute(0, 2, 1)  # H, C, H*W\n\n        if self.mode == MULTILABEL_MODE:\n            y_true = y_true.view(bs, num_classes, -1)\n            y_pred = y_pred.view(bs, num_classes, -1)\n\n        scores = soft_jaccard_score(y_pred, y_true.type(y_pred.dtype), smooth=self.smooth, eps=self.eps, dims=dims)\n\n        if self.log_loss:\n            loss = -torch.log(scores.clamp_min(self.eps))\n        else:\n            loss = 1.0 - scores\n\n        # IoU loss is defined for non-empty classes\n        # So we zero contribution of channel that does not have true pixels\n        # NOTE: A better workaround would be to use loss term `mean(y_pred)`\n        # for this case, however it will be a modified jaccard loss\n\n        mask = y_true.sum(dims) &gt; 0\n        loss *= mask.float()\n\n        if self.classes is not None:\n            loss = loss[self.classes]\n\n        return loss.mean()\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/losses/jaccard/#src.thaw_slump_segmentation.models.losses.jaccard.JaccardLoss.classes","title":"<code>classes = classes</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/losses/jaccard/#src.thaw_slump_segmentation.models.losses.jaccard.JaccardLoss.eps","title":"<code>eps = eps</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/losses/jaccard/#src.thaw_slump_segmentation.models.losses.jaccard.JaccardLoss.from_logits","title":"<code>from_logits = from_logits</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/losses/jaccard/#src.thaw_slump_segmentation.models.losses.jaccard.JaccardLoss.log_loss","title":"<code>log_loss = log_loss</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/losses/jaccard/#src.thaw_slump_segmentation.models.losses.jaccard.JaccardLoss.mode","title":"<code>mode = mode</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/losses/jaccard/#src.thaw_slump_segmentation.models.losses.jaccard.JaccardLoss.smooth","title":"<code>smooth = smooth</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/losses/jaccard/#src.thaw_slump_segmentation.models.losses.jaccard.JaccardLoss.__init__","title":"<code>__init__(mode, classes=None, log_loss=False, from_logits=True, smooth=0.0, eps=1e-07)</code>","text":"<p>Implementation of Jaccard loss for image segmentation task. It supports binary, multiclass and multilabel cases</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>Loss mode 'binary', 'multiclass' or 'multilabel'</p> required <code>classes</code> <code>Optional[List[int]]</code> <p>List of classes that contribute in loss computation. By default, all channels are included.</p> <code>None</code> <code>log_loss</code> <code>bool</code> <p>If True, loss computed as <code>- log(jaccard_coeff)</code>, otherwise <code>1 - jaccard_coeff</code></p> <code>False</code> <code>from_logits</code> <code>bool</code> <p>If True, assumes input is raw logits</p> <code>True</code> <code>smooth</code> <code>float</code> <p>Smoothness constant for dice coefficient</p> <code>0.0</code> <code>ignore_index</code> <p>Label that indicates ignored pixels (does not contribute to loss)</p> required <code>eps</code> <code>float</code> <p>A small epsilon for numerical stability to avoid zero division error  (denominator will be always greater or equal to eps)</p> <code>1e-07</code> <p>Shape      - y_pred - torch.Tensor of shape (N, C, H, W)      - y_true - torch.Tensor of shape (N, H, W) or (N, C, H, W)</p> <p>Reference     https://github.com/BloodAxe/pytorch-toolbelt</p> Source code in <code>src/thaw_slump_segmentation/models/losses/jaccard.py</code> <pre><code>def __init__(\n    self,\n    mode: str,\n    classes: Optional[List[int]] = None,\n    log_loss: bool = False,\n    from_logits: bool = True,\n    smooth: float = 0.,\n    eps: float = 1e-7,\n):\n    \"\"\"Implementation of Jaccard loss for image segmentation task.\n    It supports binary, multiclass and multilabel cases\n\n    Args:\n        mode: Loss mode 'binary', 'multiclass' or 'multilabel'\n        classes:  List of classes that contribute in loss computation. By default, all channels are included.\n        log_loss: If True, loss computed as `- log(jaccard_coeff)`, otherwise `1 - jaccard_coeff`\n        from_logits: If True, assumes input is raw logits\n        smooth: Smoothness constant for dice coefficient\n        ignore_index: Label that indicates ignored pixels (does not contribute to loss)\n        eps: A small epsilon for numerical stability to avoid zero division error \n            (denominator will be always greater or equal to eps)\n\n    Shape\n         - **y_pred** - torch.Tensor of shape (N, C, H, W)\n         - **y_true** - torch.Tensor of shape (N, H, W) or (N, C, H, W)\n\n    Reference\n        https://github.com/BloodAxe/pytorch-toolbelt\n    \"\"\"\n    assert mode in {BINARY_MODE, MULTILABEL_MODE, MULTICLASS_MODE}\n    super(JaccardLoss, self).__init__()\n\n    self.mode = mode\n    if classes is not None:\n        assert mode != BINARY_MODE, \"Masking classes is not supported with mode=binary\"\n        classes = to_tensor(classes, dtype=torch.long)\n\n    self.classes = classes\n    self.from_logits = from_logits\n    self.smooth = smooth\n    self.eps = eps\n    self.log_loss = log_loss\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/losses/jaccard/#src.thaw_slump_segmentation.models.losses.jaccard.JaccardLoss.forward","title":"<code>forward(y_pred, y_true)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/losses/jaccard.py</code> <pre><code>def forward(self, y_pred: torch.Tensor, y_true: torch.Tensor) -&gt; torch.Tensor:\n\n    assert y_true.size(0) == y_pred.size(0)\n\n    if self.from_logits:\n        # Apply activations to get [0..1] class probabilities\n        # Using Log-Exp as this gives more numerically stable result and does not cause vanishing gradient on\n        # extreme values 0 and 1\n        if self.mode == MULTICLASS_MODE:\n            y_pred = y_pred.log_softmax(dim=1).exp()\n        else:\n            y_pred = F.logsigmoid(y_pred).exp()\n\n    bs = y_true.size(0)\n    num_classes = y_pred.size(1)\n    dims = (0, 2)\n\n    if self.mode == BINARY_MODE:\n        y_true = y_true.view(bs, 1, -1)\n        y_pred = y_pred.view(bs, 1, -1)\n\n    if self.mode == MULTICLASS_MODE:\n        y_true = y_true.view(bs, -1)\n        y_pred = y_pred.view(bs, num_classes, -1)\n\n        y_true = F.one_hot(y_true, num_classes)  # N,H*W -&gt; N,H*W, C\n        y_true = y_true.permute(0, 2, 1)  # H, C, H*W\n\n    if self.mode == MULTILABEL_MODE:\n        y_true = y_true.view(bs, num_classes, -1)\n        y_pred = y_pred.view(bs, num_classes, -1)\n\n    scores = soft_jaccard_score(y_pred, y_true.type(y_pred.dtype), smooth=self.smooth, eps=self.eps, dims=dims)\n\n    if self.log_loss:\n        loss = -torch.log(scores.clamp_min(self.eps))\n    else:\n        loss = 1.0 - scores\n\n    # IoU loss is defined for non-empty classes\n    # So we zero contribution of channel that does not have true pixels\n    # NOTE: A better workaround would be to use loss term `mean(y_pred)`\n    # for this case, however it will be a modified jaccard loss\n\n    mask = y_true.sum(dims) &gt; 0\n    loss *= mask.float()\n\n    if self.classes is not None:\n        loss = loss[self.classes]\n\n    return loss.mean()\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/losses/lovasz/","title":"Lovasz","text":"<p>Lovasz-Softmax and Jaccard hinge loss in PyTorch Maxim Berman 2018 ESAT-PSI KU Leuven (MIT License)</p>"},{"location":"reference/src/thaw_slump_segmentation/models/losses/lovasz/#src.thaw_slump_segmentation.models.losses.lovasz.__all__","title":"<code>__all__ = ['LovaszLoss']</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/losses/lovasz/#src.thaw_slump_segmentation.models.losses.lovasz.LovaszLoss","title":"<code>LovaszLoss</code>","text":"<p>               Bases: <code>_Loss</code></p> Source code in <code>src/thaw_slump_segmentation/models/losses/lovasz.py</code> <pre><code>class LovaszLoss(_Loss):\n    def __init__(\n        self,\n        mode: str,\n        per_image: bool = False,\n        ignore_index: Optional[int] = None,\n        from_logits: bool = True,\n    ):\n        \"\"\"Implementation of Lovasz loss for image segmentation task.\n        It supports binary, multiclass and multilabel cases\n\n        Args:\n            mode: Loss mode 'binary', 'multiclass' or 'multilabel'\n            ignore_index: Label that indicates ignored pixels (does not contribute to loss)\n            per_image: If True loss computed per each image and then averaged, else computed per whole batch\n\n        Shape\n             - **y_pred** - torch.Tensor of shape (N, C, H, W)\n             - **y_true** - torch.Tensor of shape (N, H, W) or (N, C, H, W)\n\n        Reference\n            https://github.com/BloodAxe/pytorch-toolbelt\n        \"\"\"\n        assert mode in {BINARY_MODE, MULTILABEL_MODE, MULTICLASS_MODE}\n        super().__init__()\n\n        self.mode = mode\n        self.ignore_index = ignore_index\n        self.per_image = per_image\n\n    def forward(self, y_pred, y_true):\n\n        if self.mode in {BINARY_MODE, MULTILABEL_MODE}:\n            loss = _lovasz_hinge(y_pred, y_true, per_image=self.per_image, ignore=self.ignore_index)\n        elif self.mode == MULTICLASS_MODE:\n            y_pred = y_pred.softmax(dim=1)\n            loss = _lovasz_softmax(y_pred, y_true, per_image=self.per_image, ignore=self.ignore_index)    \n        else:\n            raise ValueError(\"Wrong mode {}.\".format(self.mode))\n        return loss\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/losses/lovasz/#src.thaw_slump_segmentation.models.losses.lovasz.LovaszLoss.ignore_index","title":"<code>ignore_index = ignore_index</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/losses/lovasz/#src.thaw_slump_segmentation.models.losses.lovasz.LovaszLoss.mode","title":"<code>mode = mode</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/losses/lovasz/#src.thaw_slump_segmentation.models.losses.lovasz.LovaszLoss.per_image","title":"<code>per_image = per_image</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/losses/lovasz/#src.thaw_slump_segmentation.models.losses.lovasz.LovaszLoss.__init__","title":"<code>__init__(mode, per_image=False, ignore_index=None, from_logits=True)</code>","text":"<p>Implementation of Lovasz loss for image segmentation task. It supports binary, multiclass and multilabel cases</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>Loss mode 'binary', 'multiclass' or 'multilabel'</p> required <code>ignore_index</code> <code>Optional[int]</code> <p>Label that indicates ignored pixels (does not contribute to loss)</p> <code>None</code> <code>per_image</code> <code>bool</code> <p>If True loss computed per each image and then averaged, else computed per whole batch</p> <code>False</code> <p>Shape      - y_pred - torch.Tensor of shape (N, C, H, W)      - y_true - torch.Tensor of shape (N, H, W) or (N, C, H, W)</p> <p>Reference     https://github.com/BloodAxe/pytorch-toolbelt</p> Source code in <code>src/thaw_slump_segmentation/models/losses/lovasz.py</code> <pre><code>def __init__(\n    self,\n    mode: str,\n    per_image: bool = False,\n    ignore_index: Optional[int] = None,\n    from_logits: bool = True,\n):\n    \"\"\"Implementation of Lovasz loss for image segmentation task.\n    It supports binary, multiclass and multilabel cases\n\n    Args:\n        mode: Loss mode 'binary', 'multiclass' or 'multilabel'\n        ignore_index: Label that indicates ignored pixels (does not contribute to loss)\n        per_image: If True loss computed per each image and then averaged, else computed per whole batch\n\n    Shape\n         - **y_pred** - torch.Tensor of shape (N, C, H, W)\n         - **y_true** - torch.Tensor of shape (N, H, W) or (N, C, H, W)\n\n    Reference\n        https://github.com/BloodAxe/pytorch-toolbelt\n    \"\"\"\n    assert mode in {BINARY_MODE, MULTILABEL_MODE, MULTICLASS_MODE}\n    super().__init__()\n\n    self.mode = mode\n    self.ignore_index = ignore_index\n    self.per_image = per_image\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/losses/lovasz/#src.thaw_slump_segmentation.models.losses.lovasz.LovaszLoss.forward","title":"<code>forward(y_pred, y_true)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/losses/lovasz.py</code> <pre><code>def forward(self, y_pred, y_true):\n\n    if self.mode in {BINARY_MODE, MULTILABEL_MODE}:\n        loss = _lovasz_hinge(y_pred, y_true, per_image=self.per_image, ignore=self.ignore_index)\n    elif self.mode == MULTICLASS_MODE:\n        y_pred = y_pred.softmax(dim=1)\n        loss = _lovasz_softmax(y_pred, y_true, per_image=self.per_image, ignore=self.ignore_index)    \n    else:\n        raise ValueError(\"Wrong mode {}.\".format(self.mode))\n    return loss\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/losses/lovasz/#src.thaw_slump_segmentation.models.losses.lovasz.isnan","title":"<code>isnan(x)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/losses/lovasz.py</code> <pre><code>def isnan(x):\n    return x != x\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/losses/lovasz/#src.thaw_slump_segmentation.models.losses.lovasz.mean","title":"<code>mean(values, ignore_nan=False, empty=0)</code>","text":"<p>Nanmean compatible with generators.</p> Source code in <code>src/thaw_slump_segmentation/models/losses/lovasz.py</code> <pre><code>def mean(values, ignore_nan=False, empty=0):\n    \"\"\"Nanmean compatible with generators.\n    \"\"\"\n    values = iter(values)\n    if ignore_nan:\n        values = ifilterfalse(isnan, values)\n    try:\n        n = 1\n        acc = next(values)\n    except StopIteration:\n        if empty == \"raise\":\n            raise ValueError(\"Empty mean\")\n        return empty\n    for n, v in enumerate(values, 2):\n        acc += v\n    if n == 1:\n        return acc\n    return acc / n\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/losses/soft_bce/","title":"Soft bce","text":""},{"location":"reference/src/thaw_slump_segmentation/models/losses/soft_bce/#src.thaw_slump_segmentation.models.losses.soft_bce.__all__","title":"<code>__all__ = ['SoftBCEWithLogitsLoss']</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/losses/soft_bce/#src.thaw_slump_segmentation.models.losses.soft_bce.SoftBCEWithLogitsLoss","title":"<code>SoftBCEWithLogitsLoss</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/thaw_slump_segmentation/models/losses/soft_bce.py</code> <pre><code>class SoftBCEWithLogitsLoss(nn.Module):\n\n    __constants__ = [\"weight\", \"pos_weight\", \"reduction\", \"ignore_index\", \"smooth_factor\"]\n\n    def __init__(\n        self,\n        mode: str,\n        weight: Optional[torch.Tensor] = None,\n        ignore_index: Optional[int] = -100,\n        reduction: str = \"mean\",\n        smooth_factor: Optional[float] = 0.1,\n        pos_weight: Optional[torch.Tensor] = None,\n    ):\n        \"\"\"Drop-in replacement for torch.nn.BCEWithLogitsLoss with few additions: ignore_index and label_smoothing\n\n        Args:\n            ignore_index: Specifies a target value that is ignored and does not contribute to the input gradient. \n            smooth_factor: Factor to smooth target (e.g. if smooth_factor=0.1 then [1, 0, 1] -&gt; [0.9, 0.1, 0.9])\n\n        Shape\n             - **y_pred** - torch.Tensor of shape NxCxHxW\n             - **y_true** - torch.Tensor of shape NxHxW or Nx1xHxW\n\n        Reference\n            https://github.com/BloodAxe/pytorch-toolbelt\n\n        \"\"\"\n        super().__init__()\n        assert mode == BINARY_MODE\n        self.ignore_index = ignore_index\n        self.reduction = reduction\n        self.smooth_factor = smooth_factor\n        self.register_buffer(\"weight\", weight)\n        self.register_buffer(\"pos_weight\", pos_weight)\n\n    def forward(self, y_pred: torch.Tensor, y_true: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Args:\n            y_pred: torch.Tensor of shape (N, C, H, W)\n            y_true: torch.Tensor of shape (N, H, W)  or (N, 1, H, W)\n\n        Returns:\n            loss: torch.Tensor\n        \"\"\"\n\n        y_true = y_true.float()\n        if self.smooth_factor is not None:\n            soft_targets = (1 - y_true) * self.smooth_factor + y_true * (1 - self.smooth_factor)\n        else:\n            soft_targets = y_true\n\n        loss = F.binary_cross_entropy_with_logits(\n            y_pred, soft_targets, self.weight, pos_weight=self.pos_weight, reduction=\"none\"\n        )\n\n        if self.ignore_index is not None:\n            not_ignored_mask = y_true != self.ignore_index\n            loss *= not_ignored_mask.type_as(loss)\n\n        if self.reduction == \"mean\":\n            loss = loss.mean()\n\n        if self.reduction == \"sum\":\n            loss = loss.sum()\n\n        return loss\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/losses/soft_bce/#src.thaw_slump_segmentation.models.losses.soft_bce.SoftBCEWithLogitsLoss.__constants__","title":"<code>__constants__ = ['weight', 'pos_weight', 'reduction', 'ignore_index', 'smooth_factor']</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/losses/soft_bce/#src.thaw_slump_segmentation.models.losses.soft_bce.SoftBCEWithLogitsLoss.ignore_index","title":"<code>ignore_index = ignore_index</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/losses/soft_bce/#src.thaw_slump_segmentation.models.losses.soft_bce.SoftBCEWithLogitsLoss.reduction","title":"<code>reduction = reduction</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/losses/soft_bce/#src.thaw_slump_segmentation.models.losses.soft_bce.SoftBCEWithLogitsLoss.smooth_factor","title":"<code>smooth_factor = smooth_factor</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/losses/soft_bce/#src.thaw_slump_segmentation.models.losses.soft_bce.SoftBCEWithLogitsLoss.__init__","title":"<code>__init__(mode, weight=None, ignore_index=-100, reduction='mean', smooth_factor=0.1, pos_weight=None)</code>","text":"<p>Drop-in replacement for torch.nn.BCEWithLogitsLoss with few additions: ignore_index and label_smoothing</p> <p>Parameters:</p> Name Type Description Default <code>ignore_index</code> <code>Optional[int]</code> <p>Specifies a target value that is ignored and does not contribute to the input gradient. </p> <code>-100</code> <code>smooth_factor</code> <code>Optional[float]</code> <p>Factor to smooth target (e.g. if smooth_factor=0.1 then [1, 0, 1] -&gt; [0.9, 0.1, 0.9])</p> <code>0.1</code> <p>Shape      - y_pred - torch.Tensor of shape NxCxHxW      - y_true - torch.Tensor of shape NxHxW or Nx1xHxW</p> <p>Reference     https://github.com/BloodAxe/pytorch-toolbelt</p> Source code in <code>src/thaw_slump_segmentation/models/losses/soft_bce.py</code> <pre><code>def __init__(\n    self,\n    mode: str,\n    weight: Optional[torch.Tensor] = None,\n    ignore_index: Optional[int] = -100,\n    reduction: str = \"mean\",\n    smooth_factor: Optional[float] = 0.1,\n    pos_weight: Optional[torch.Tensor] = None,\n):\n    \"\"\"Drop-in replacement for torch.nn.BCEWithLogitsLoss with few additions: ignore_index and label_smoothing\n\n    Args:\n        ignore_index: Specifies a target value that is ignored and does not contribute to the input gradient. \n        smooth_factor: Factor to smooth target (e.g. if smooth_factor=0.1 then [1, 0, 1] -&gt; [0.9, 0.1, 0.9])\n\n    Shape\n         - **y_pred** - torch.Tensor of shape NxCxHxW\n         - **y_true** - torch.Tensor of shape NxHxW or Nx1xHxW\n\n    Reference\n        https://github.com/BloodAxe/pytorch-toolbelt\n\n    \"\"\"\n    super().__init__()\n    assert mode == BINARY_MODE\n    self.ignore_index = ignore_index\n    self.reduction = reduction\n    self.smooth_factor = smooth_factor\n    self.register_buffer(\"weight\", weight)\n    self.register_buffer(\"pos_weight\", pos_weight)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/losses/soft_bce/#src.thaw_slump_segmentation.models.losses.soft_bce.SoftBCEWithLogitsLoss.forward","title":"<code>forward(y_pred, y_true)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>y_pred</code> <code>Tensor</code> <p>torch.Tensor of shape (N, C, H, W)</p> required <code>y_true</code> <code>Tensor</code> <p>torch.Tensor of shape (N, H, W)  or (N, 1, H, W)</p> required <p>Returns:</p> Name Type Description <code>loss</code> <code>Tensor</code> <p>torch.Tensor</p> Source code in <code>src/thaw_slump_segmentation/models/losses/soft_bce.py</code> <pre><code>def forward(self, y_pred: torch.Tensor, y_true: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Args:\n        y_pred: torch.Tensor of shape (N, C, H, W)\n        y_true: torch.Tensor of shape (N, H, W)  or (N, 1, H, W)\n\n    Returns:\n        loss: torch.Tensor\n    \"\"\"\n\n    y_true = y_true.float()\n    if self.smooth_factor is not None:\n        soft_targets = (1 - y_true) * self.smooth_factor + y_true * (1 - self.smooth_factor)\n    else:\n        soft_targets = y_true\n\n    loss = F.binary_cross_entropy_with_logits(\n        y_pred, soft_targets, self.weight, pos_weight=self.pos_weight, reduction=\"none\"\n    )\n\n    if self.ignore_index is not None:\n        not_ignored_mask = y_true != self.ignore_index\n        loss *= not_ignored_mask.type_as(loss)\n\n    if self.reduction == \"mean\":\n        loss = loss.mean()\n\n    if self.reduction == \"sum\":\n        loss = loss.sum()\n\n    return loss\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/losses/soft_ce/","title":"Soft ce","text":""},{"location":"reference/src/thaw_slump_segmentation/models/losses/soft_ce/#src.thaw_slump_segmentation.models.losses.soft_ce.__all__","title":"<code>__all__ = ['SoftCrossEntropyLoss']</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/losses/soft_ce/#src.thaw_slump_segmentation.models.losses.soft_ce.SoftCrossEntropyLoss","title":"<code>SoftCrossEntropyLoss</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/thaw_slump_segmentation/models/losses/soft_ce.py</code> <pre><code>class SoftCrossEntropyLoss(nn.Module):\n\n    __constants__ = [\"reduction\", \"ignore_index\", \"smooth_factor\"]\n\n    def __init__(\n        self,\n        reduction: str = \"mean\",\n        smooth_factor: Optional[float] = None,\n        ignore_index: Optional[int] = -100,\n        dim: int = 1,\n    ):\n        \"\"\"Drop-in replacement for torch.nn.CrossEntropyLoss with label_smoothing\n\n        Args:\n            smooth_factor: Factor to smooth target (e.g. if smooth_factor=0.1 then [1, 0, 0] -&gt; [0.9, 0.05, 0.05])\n\n        Shape\n             - **y_pred** - torch.Tensor of shape (N, C, H, W)\n             - **y_true** - torch.Tensor of shape (N, H, W)\n\n        Reference\n            https://github.com/BloodAxe/pytorch-toolbelt\n        \"\"\"\n        super().__init__()\n        self.smooth_factor = smooth_factor\n        self.ignore_index = ignore_index\n        self.reduction = reduction\n        self.dim = dim\n\n    def forward(self, y_pred: torch.Tensor, y_true: torch.Tensor) -&gt; torch.Tensor:\n        log_prob = F.log_softmax(y_pred, dim=self.dim)\n        return label_smoothed_nll_loss(\n            log_prob,\n            y_true,\n            epsilon=self.smooth_factor,\n            ignore_index=self.ignore_index,\n            reduction=self.reduction,\n            dim=self.dim,\n        )\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/losses/soft_ce/#src.thaw_slump_segmentation.models.losses.soft_ce.SoftCrossEntropyLoss.__constants__","title":"<code>__constants__ = ['reduction', 'ignore_index', 'smooth_factor']</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/losses/soft_ce/#src.thaw_slump_segmentation.models.losses.soft_ce.SoftCrossEntropyLoss.dim","title":"<code>dim = dim</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/losses/soft_ce/#src.thaw_slump_segmentation.models.losses.soft_ce.SoftCrossEntropyLoss.ignore_index","title":"<code>ignore_index = ignore_index</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/losses/soft_ce/#src.thaw_slump_segmentation.models.losses.soft_ce.SoftCrossEntropyLoss.reduction","title":"<code>reduction = reduction</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/losses/soft_ce/#src.thaw_slump_segmentation.models.losses.soft_ce.SoftCrossEntropyLoss.smooth_factor","title":"<code>smooth_factor = smooth_factor</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/losses/soft_ce/#src.thaw_slump_segmentation.models.losses.soft_ce.SoftCrossEntropyLoss.__init__","title":"<code>__init__(reduction='mean', smooth_factor=None, ignore_index=-100, dim=1)</code>","text":"<p>Drop-in replacement for torch.nn.CrossEntropyLoss with label_smoothing</p> <p>Parameters:</p> Name Type Description Default <code>smooth_factor</code> <code>Optional[float]</code> <p>Factor to smooth target (e.g. if smooth_factor=0.1 then [1, 0, 0] -&gt; [0.9, 0.05, 0.05])</p> <code>None</code> <p>Shape      - y_pred - torch.Tensor of shape (N, C, H, W)      - y_true - torch.Tensor of shape (N, H, W)</p> <p>Reference     https://github.com/BloodAxe/pytorch-toolbelt</p> Source code in <code>src/thaw_slump_segmentation/models/losses/soft_ce.py</code> <pre><code>def __init__(\n    self,\n    reduction: str = \"mean\",\n    smooth_factor: Optional[float] = None,\n    ignore_index: Optional[int] = -100,\n    dim: int = 1,\n):\n    \"\"\"Drop-in replacement for torch.nn.CrossEntropyLoss with label_smoothing\n\n    Args:\n        smooth_factor: Factor to smooth target (e.g. if smooth_factor=0.1 then [1, 0, 0] -&gt; [0.9, 0.05, 0.05])\n\n    Shape\n         - **y_pred** - torch.Tensor of shape (N, C, H, W)\n         - **y_true** - torch.Tensor of shape (N, H, W)\n\n    Reference\n        https://github.com/BloodAxe/pytorch-toolbelt\n    \"\"\"\n    super().__init__()\n    self.smooth_factor = smooth_factor\n    self.ignore_index = ignore_index\n    self.reduction = reduction\n    self.dim = dim\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/losses/soft_ce/#src.thaw_slump_segmentation.models.losses.soft_ce.SoftCrossEntropyLoss.forward","title":"<code>forward(y_pred, y_true)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/losses/soft_ce.py</code> <pre><code>def forward(self, y_pred: torch.Tensor, y_true: torch.Tensor) -&gt; torch.Tensor:\n    log_prob = F.log_softmax(y_pred, dim=self.dim)\n    return label_smoothed_nll_loss(\n        log_prob,\n        y_true,\n        epsilon=self.smooth_factor,\n        ignore_index=self.ignore_index,\n        reduction=self.reduction,\n        dim=self.dim,\n    )\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/manet/decoder/","title":"Decoder","text":""},{"location":"reference/src/thaw_slump_segmentation/models/manet/decoder/#src.thaw_slump_segmentation.models.manet.decoder.DecoderBlock","title":"<code>DecoderBlock</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/thaw_slump_segmentation/models/manet/decoder.py</code> <pre><code>class DecoderBlock(nn.Module):\n    def __init__(\n            self,\n            in_channels,\n            skip_channels,\n            out_channels,\n            use_batchnorm=True\n    ):\n        super().__init__()\n        self.conv1 = md.Conv2dReLU(\n            in_channels + skip_channels,\n            out_channels,\n            kernel_size=3,\n            padding=1,\n            use_batchnorm=use_batchnorm,\n        )\n        self.conv2 = md.Conv2dReLU(\n            out_channels,\n            out_channels,\n            kernel_size=3,\n            padding=1,\n            use_batchnorm=use_batchnorm,\n        )\n\n    def forward(self, x, skip=None):\n        x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n        if skip is not None:\n            x = torch.cat([x, skip], dim=1)\n        x = self.conv1(x)\n        x = self.conv2(x)\n        return x\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/manet/decoder/#src.thaw_slump_segmentation.models.manet.decoder.DecoderBlock.conv1","title":"<code>conv1 = md.Conv2dReLU(in_channels + skip_channels, out_channels, kernel_size=3, padding=1, use_batchnorm=use_batchnorm)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/manet/decoder/#src.thaw_slump_segmentation.models.manet.decoder.DecoderBlock.conv2","title":"<code>conv2 = md.Conv2dReLU(out_channels, out_channels, kernel_size=3, padding=1, use_batchnorm=use_batchnorm)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/manet/decoder/#src.thaw_slump_segmentation.models.manet.decoder.DecoderBlock.__init__","title":"<code>__init__(in_channels, skip_channels, out_channels, use_batchnorm=True)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/manet/decoder.py</code> <pre><code>def __init__(\n        self,\n        in_channels,\n        skip_channels,\n        out_channels,\n        use_batchnorm=True\n):\n    super().__init__()\n    self.conv1 = md.Conv2dReLU(\n        in_channels + skip_channels,\n        out_channels,\n        kernel_size=3,\n        padding=1,\n        use_batchnorm=use_batchnorm,\n    )\n    self.conv2 = md.Conv2dReLU(\n        out_channels,\n        out_channels,\n        kernel_size=3,\n        padding=1,\n        use_batchnorm=use_batchnorm,\n    )\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/manet/decoder/#src.thaw_slump_segmentation.models.manet.decoder.DecoderBlock.forward","title":"<code>forward(x, skip=None)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/manet/decoder.py</code> <pre><code>def forward(self, x, skip=None):\n    x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n    if skip is not None:\n        x = torch.cat([x, skip], dim=1)\n    x = self.conv1(x)\n    x = self.conv2(x)\n    return x\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/manet/decoder/#src.thaw_slump_segmentation.models.manet.decoder.MAnetDecoder","title":"<code>MAnetDecoder</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/thaw_slump_segmentation/models/manet/decoder.py</code> <pre><code>class MAnetDecoder(nn.Module):\n    def __init__(\n            self,\n            encoder_channels,\n            decoder_channels,\n            n_blocks=5,\n            reduction=16,\n            use_batchnorm=True,\n            pab_channels=64\n    ):\n        super().__init__()\n\n        if n_blocks != len(decoder_channels):\n            raise ValueError(\n                \"Model depth is {}, but you provide `decoder_channels` for {} blocks.\".format(\n                    n_blocks, len(decoder_channels)\n                )\n            )\n\n        encoder_channels = encoder_channels[1:]  # remove first skip with same spatial resolution\n        encoder_channels = encoder_channels[::-1]  # reverse channels to start from head of encoder\n\n        # computing blocks input and output channels\n        head_channels = encoder_channels[0]\n        in_channels = [head_channels] + list(decoder_channels[:-1])\n        skip_channels = list(encoder_channels[1:]) + [0]\n        out_channels = decoder_channels\n\n        self.center = PAB(head_channels, head_channels, pab_channels=pab_channels)\n\n        # combine decoder keyword arguments\n        kwargs = dict(use_batchnorm=use_batchnorm)  # no attention type here\n        blocks = [\n            MFAB(in_ch, skip_ch, out_ch, reduction=reduction, **kwargs) if skip_ch &gt; 0 else\n            DecoderBlock(in_ch, skip_ch, out_ch, **kwargs)\n            for in_ch, skip_ch, out_ch in zip(in_channels, skip_channels, out_channels)\n        ]\n        # for the last we dont have skip connection -&gt; use simple decoder block\n        self.blocks = nn.ModuleList(blocks)\n\n    def forward(self, *features):\n\n        features = features[1:]    # remove first skip with same spatial resolution\n        features = features[::-1]  # reverse channels to start from head of encoder\n\n        head = features[0]\n        skips = features[1:]\n\n        x = self.center(head)\n        for i, decoder_block in enumerate(self.blocks):\n            skip = skips[i] if i &lt; len(skips) else None\n            x = decoder_block(x, skip)\n\n        return x\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/manet/decoder/#src.thaw_slump_segmentation.models.manet.decoder.MAnetDecoder.blocks","title":"<code>blocks = nn.ModuleList(blocks)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/manet/decoder/#src.thaw_slump_segmentation.models.manet.decoder.MAnetDecoder.center","title":"<code>center = PAB(head_channels, head_channels, pab_channels=pab_channels)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/manet/decoder/#src.thaw_slump_segmentation.models.manet.decoder.MAnetDecoder.__init__","title":"<code>__init__(encoder_channels, decoder_channels, n_blocks=5, reduction=16, use_batchnorm=True, pab_channels=64)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/manet/decoder.py</code> <pre><code>def __init__(\n        self,\n        encoder_channels,\n        decoder_channels,\n        n_blocks=5,\n        reduction=16,\n        use_batchnorm=True,\n        pab_channels=64\n):\n    super().__init__()\n\n    if n_blocks != len(decoder_channels):\n        raise ValueError(\n            \"Model depth is {}, but you provide `decoder_channels` for {} blocks.\".format(\n                n_blocks, len(decoder_channels)\n            )\n        )\n\n    encoder_channels = encoder_channels[1:]  # remove first skip with same spatial resolution\n    encoder_channels = encoder_channels[::-1]  # reverse channels to start from head of encoder\n\n    # computing blocks input and output channels\n    head_channels = encoder_channels[0]\n    in_channels = [head_channels] + list(decoder_channels[:-1])\n    skip_channels = list(encoder_channels[1:]) + [0]\n    out_channels = decoder_channels\n\n    self.center = PAB(head_channels, head_channels, pab_channels=pab_channels)\n\n    # combine decoder keyword arguments\n    kwargs = dict(use_batchnorm=use_batchnorm)  # no attention type here\n    blocks = [\n        MFAB(in_ch, skip_ch, out_ch, reduction=reduction, **kwargs) if skip_ch &gt; 0 else\n        DecoderBlock(in_ch, skip_ch, out_ch, **kwargs)\n        for in_ch, skip_ch, out_ch in zip(in_channels, skip_channels, out_channels)\n    ]\n    # for the last we dont have skip connection -&gt; use simple decoder block\n    self.blocks = nn.ModuleList(blocks)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/manet/decoder/#src.thaw_slump_segmentation.models.manet.decoder.MAnetDecoder.forward","title":"<code>forward(*features)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/manet/decoder.py</code> <pre><code>def forward(self, *features):\n\n    features = features[1:]    # remove first skip with same spatial resolution\n    features = features[::-1]  # reverse channels to start from head of encoder\n\n    head = features[0]\n    skips = features[1:]\n\n    x = self.center(head)\n    for i, decoder_block in enumerate(self.blocks):\n        skip = skips[i] if i &lt; len(skips) else None\n        x = decoder_block(x, skip)\n\n    return x\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/manet/decoder/#src.thaw_slump_segmentation.models.manet.decoder.MFAB","title":"<code>MFAB</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/thaw_slump_segmentation/models/manet/decoder.py</code> <pre><code>class MFAB(nn.Module):\n    def __init__(self, in_channels, skip_channels, out_channels, use_batchnorm=True, reduction=16):\n        # MFAB is just a modified version of SE-blocks, one for skip, one for input\n        super(MFAB, self).__init__()\n        self.hl_conv = nn.Sequential(\n            md.Conv2dReLU(\n                in_channels,\n                in_channels,\n                kernel_size=3,\n                padding=1,\n                use_batchnorm=use_batchnorm,\n            ),\n            md.Conv2dReLU(\n                in_channels,\n                skip_channels,\n                kernel_size=1,\n                use_batchnorm=use_batchnorm,\n            )\n        )\n        self.SE_ll = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(skip_channels, skip_channels // reduction, 1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(skip_channels // reduction, skip_channels, 1),\n            nn.Sigmoid(),\n        )\n        self.SE_hl = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(skip_channels, skip_channels // reduction, 1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(skip_channels // reduction, skip_channels, 1),\n            nn.Sigmoid(),\n        )\n        self.conv1 = md.Conv2dReLU(\n            skip_channels + skip_channels,  # we transform C-prime form high level to C from skip connection\n            out_channels,\n            kernel_size=3,\n            padding=1,\n            use_batchnorm=use_batchnorm,\n        )\n        self.conv2 = md.Conv2dReLU(\n            out_channels,\n            out_channels,\n            kernel_size=3,\n            padding=1,\n            use_batchnorm=use_batchnorm,\n        )\n\n    def forward(self, x, skip=None):\n        x = self.hl_conv(x)\n        x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n        attention_hl = self.SE_hl(x)\n        if skip is not None:\n            attention_ll = self.SE_ll(skip)\n            attention_hl = attention_hl + attention_ll\n            x = x * attention_hl\n            x = torch.cat([x, skip], dim=1)\n        x = self.conv1(x)\n        x = self.conv2(x)\n        return x\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/manet/decoder/#src.thaw_slump_segmentation.models.manet.decoder.MFAB.SE_hl","title":"<code>SE_hl = nn.Sequential(nn.AdaptiveAvgPool2d(1), nn.Conv2d(skip_channels, skip_channels // reduction, 1), nn.ReLU(inplace=True), nn.Conv2d(skip_channels // reduction, skip_channels, 1), nn.Sigmoid())</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/manet/decoder/#src.thaw_slump_segmentation.models.manet.decoder.MFAB.SE_ll","title":"<code>SE_ll = nn.Sequential(nn.AdaptiveAvgPool2d(1), nn.Conv2d(skip_channels, skip_channels // reduction, 1), nn.ReLU(inplace=True), nn.Conv2d(skip_channels // reduction, skip_channels, 1), nn.Sigmoid())</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/manet/decoder/#src.thaw_slump_segmentation.models.manet.decoder.MFAB.conv1","title":"<code>conv1 = md.Conv2dReLU(skip_channels + skip_channels, out_channels, kernel_size=3, padding=1, use_batchnorm=use_batchnorm)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/manet/decoder/#src.thaw_slump_segmentation.models.manet.decoder.MFAB.conv2","title":"<code>conv2 = md.Conv2dReLU(out_channels, out_channels, kernel_size=3, padding=1, use_batchnorm=use_batchnorm)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/manet/decoder/#src.thaw_slump_segmentation.models.manet.decoder.MFAB.hl_conv","title":"<code>hl_conv = nn.Sequential(md.Conv2dReLU(in_channels, in_channels, kernel_size=3, padding=1, use_batchnorm=use_batchnorm), md.Conv2dReLU(in_channels, skip_channels, kernel_size=1, use_batchnorm=use_batchnorm))</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/manet/decoder/#src.thaw_slump_segmentation.models.manet.decoder.MFAB.__init__","title":"<code>__init__(in_channels, skip_channels, out_channels, use_batchnorm=True, reduction=16)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/manet/decoder.py</code> <pre><code>def __init__(self, in_channels, skip_channels, out_channels, use_batchnorm=True, reduction=16):\n    # MFAB is just a modified version of SE-blocks, one for skip, one for input\n    super(MFAB, self).__init__()\n    self.hl_conv = nn.Sequential(\n        md.Conv2dReLU(\n            in_channels,\n            in_channels,\n            kernel_size=3,\n            padding=1,\n            use_batchnorm=use_batchnorm,\n        ),\n        md.Conv2dReLU(\n            in_channels,\n            skip_channels,\n            kernel_size=1,\n            use_batchnorm=use_batchnorm,\n        )\n    )\n    self.SE_ll = nn.Sequential(\n        nn.AdaptiveAvgPool2d(1),\n        nn.Conv2d(skip_channels, skip_channels // reduction, 1),\n        nn.ReLU(inplace=True),\n        nn.Conv2d(skip_channels // reduction, skip_channels, 1),\n        nn.Sigmoid(),\n    )\n    self.SE_hl = nn.Sequential(\n        nn.AdaptiveAvgPool2d(1),\n        nn.Conv2d(skip_channels, skip_channels // reduction, 1),\n        nn.ReLU(inplace=True),\n        nn.Conv2d(skip_channels // reduction, skip_channels, 1),\n        nn.Sigmoid(),\n    )\n    self.conv1 = md.Conv2dReLU(\n        skip_channels + skip_channels,  # we transform C-prime form high level to C from skip connection\n        out_channels,\n        kernel_size=3,\n        padding=1,\n        use_batchnorm=use_batchnorm,\n    )\n    self.conv2 = md.Conv2dReLU(\n        out_channels,\n        out_channels,\n        kernel_size=3,\n        padding=1,\n        use_batchnorm=use_batchnorm,\n    )\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/manet/decoder/#src.thaw_slump_segmentation.models.manet.decoder.MFAB.forward","title":"<code>forward(x, skip=None)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/manet/decoder.py</code> <pre><code>def forward(self, x, skip=None):\n    x = self.hl_conv(x)\n    x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n    attention_hl = self.SE_hl(x)\n    if skip is not None:\n        attention_ll = self.SE_ll(skip)\n        attention_hl = attention_hl + attention_ll\n        x = x * attention_hl\n        x = torch.cat([x, skip], dim=1)\n    x = self.conv1(x)\n    x = self.conv2(x)\n    return x\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/manet/decoder/#src.thaw_slump_segmentation.models.manet.decoder.PAB","title":"<code>PAB</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/thaw_slump_segmentation/models/manet/decoder.py</code> <pre><code>class PAB(nn.Module):\n    def __init__(self, in_channels, out_channels, pab_channels=64):\n        super(PAB, self).__init__()\n        # Series of 1x1 conv to generate attention feature maps\n        self.pab_channels = pab_channels\n        self.in_channels = in_channels\n        self.top_conv = nn.Conv2d(in_channels, pab_channels, kernel_size=1)\n        self.center_conv = nn.Conv2d(in_channels, pab_channels, kernel_size=1)\n        self.bottom_conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)\n        self.map_softmax = nn.Softmax(dim=1)\n        self.out_conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)\n\n    def forward(self, x):\n        bsize = x.size()[0]\n        h = x.size()[2]\n        w = x.size()[3]\n        x_top = self.top_conv(x)\n        x_center = self.center_conv(x)\n        x_bottom = self.bottom_conv(x)\n\n        x_top = x_top.flatten(2)\n        x_center = x_center.flatten(2).transpose(1, 2)\n        x_bottom = x_bottom.flatten(2).transpose(1, 2)\n\n        sp_map = torch.matmul(x_center, x_top)\n        sp_map = self.map_softmax(sp_map.view(bsize, -1)).view(bsize, h*w, h*w)\n        sp_map = torch.matmul(sp_map, x_bottom)\n        sp_map = sp_map.reshape(bsize, self.in_channels, h, w)\n        x = x + sp_map\n        x = self.out_conv(x)\n        return x\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/manet/decoder/#src.thaw_slump_segmentation.models.manet.decoder.PAB.bottom_conv","title":"<code>bottom_conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/manet/decoder/#src.thaw_slump_segmentation.models.manet.decoder.PAB.center_conv","title":"<code>center_conv = nn.Conv2d(in_channels, pab_channels, kernel_size=1)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/manet/decoder/#src.thaw_slump_segmentation.models.manet.decoder.PAB.in_channels","title":"<code>in_channels = in_channels</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/manet/decoder/#src.thaw_slump_segmentation.models.manet.decoder.PAB.map_softmax","title":"<code>map_softmax = nn.Softmax(dim=1)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/manet/decoder/#src.thaw_slump_segmentation.models.manet.decoder.PAB.out_conv","title":"<code>out_conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/manet/decoder/#src.thaw_slump_segmentation.models.manet.decoder.PAB.pab_channels","title":"<code>pab_channels = pab_channels</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/manet/decoder/#src.thaw_slump_segmentation.models.manet.decoder.PAB.top_conv","title":"<code>top_conv = nn.Conv2d(in_channels, pab_channels, kernel_size=1)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/manet/decoder/#src.thaw_slump_segmentation.models.manet.decoder.PAB.__init__","title":"<code>__init__(in_channels, out_channels, pab_channels=64)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/manet/decoder.py</code> <pre><code>def __init__(self, in_channels, out_channels, pab_channels=64):\n    super(PAB, self).__init__()\n    # Series of 1x1 conv to generate attention feature maps\n    self.pab_channels = pab_channels\n    self.in_channels = in_channels\n    self.top_conv = nn.Conv2d(in_channels, pab_channels, kernel_size=1)\n    self.center_conv = nn.Conv2d(in_channels, pab_channels, kernel_size=1)\n    self.bottom_conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)\n    self.map_softmax = nn.Softmax(dim=1)\n    self.out_conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/manet/decoder/#src.thaw_slump_segmentation.models.manet.decoder.PAB.forward","title":"<code>forward(x)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/manet/decoder.py</code> <pre><code>def forward(self, x):\n    bsize = x.size()[0]\n    h = x.size()[2]\n    w = x.size()[3]\n    x_top = self.top_conv(x)\n    x_center = self.center_conv(x)\n    x_bottom = self.bottom_conv(x)\n\n    x_top = x_top.flatten(2)\n    x_center = x_center.flatten(2).transpose(1, 2)\n    x_bottom = x_bottom.flatten(2).transpose(1, 2)\n\n    sp_map = torch.matmul(x_center, x_top)\n    sp_map = self.map_softmax(sp_map.view(bsize, -1)).view(bsize, h*w, h*w)\n    sp_map = torch.matmul(sp_map, x_bottom)\n    sp_map = sp_map.reshape(bsize, self.in_channels, h, w)\n    x = x + sp_map\n    x = self.out_conv(x)\n    return x\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/manet/model/","title":"Model","text":""},{"location":"reference/src/thaw_slump_segmentation/models/manet/model/#src.thaw_slump_segmentation.models.manet.model.MAnet","title":"<code>MAnet</code>","text":"<p>               Bases: <code>SegmentationModel</code></p> <p>MAnet_ :  Multi-scale Attention Net. The MA-Net can capture rich contextual dependencies based on the attention mechanism,  using two blocks:  - Position-wise Attention Block (PAB), which captures the spatial dependencies between pixels in a global view  - Multi-scale Fusion Attention Block (MFAB), which  captures the channel dependencies between any feature map by    multi-scale semantic feature fusion</p> <p>Parameters:</p> Name Type Description Default <code>encoder_name</code> <code>str</code> <p>Name of the classification model that will be used as an encoder (a.k.a backbone) to extract features of different spatial resolution</p> <code>'resnet34'</code> <code>encoder_depth</code> <code>int</code> <p>A number of stages used in encoder in range [3, 5]. Each stage generate features  two times smaller in spatial dimensions than previous one (e.g. for depth 0 we will have features with shapes [(N, C, H, W),], for depth 1 - [(N, C, H, W), (N, C, H // 2, W // 2)] and so on). Default is 5</p> <code>5</code> <code>encoder_weights</code> <code>Optional[str]</code> <p>One of None (random initialization), \"imagenet\" (pre-training on ImageNet) and  other pretrained weights (see table with available weights for each encoder_name)</p> <code>'imagenet'</code> <code>decoder_channels</code> <code>List[int]</code> <p>List of integers which specify in_channels parameter for convolutions used in decoder. Length of the list should be the same as encoder_depth</p> <code>(256, 128, 64, 32, 16)</code> <code>decoder_use_batchnorm</code> <code>bool</code> <p>If True, BatchNorm2d layer between Conv2D and Activation layers is used. If \"inplace\" InplaceABN will be used, allows to decrease memory consumption. Available options are True, False, \"inplace\"</p> <code>True</code> <code>decoder_pab_channels</code> <code>int</code> <p>A number of channels for PAB module in decoder.  Default is 64.</p> <code>64</code> <code>in_channels</code> <code>int</code> <p>A number of input channels for the model, default is 3 (RGB images)</p> <code>3</code> <code>classes</code> <code>int</code> <p>A number of classes for output mask (or you can think as a number of channels of output mask)</p> <code>1</code> <code>activation</code> <code>Optional[Union[str, callable]]</code> <p>An activation function to apply after the final convolution layer. Available options are \"sigmoid\", \"softmax\", \"logsoftmax\", \"tanh\", \"identity\", callable and None. Default is None</p> <code>None</code> <code>aux_params</code> <code>Optional[dict]</code> <p>Dictionary with parameters of the auxiliary output (classification head). Auxiliary output is build on top of encoder if aux_params is not None (default). Supported params:     - classes (int): A number of classes     - pooling (str): One of \"max\", \"avg\". Default is \"avg\"     - dropout (float): Dropout factor in [0, 1)     - activation (str): An activation function to apply \"sigmoid\"/\"softmax\" (could be None to return logits)</p> <code>None</code> <p>Returns:</p> Type Description <p><code>torch.nn.Module</code>: MAnet</p> <p>.. _MAnet:     https://ieeexplore.ieee.org/abstract/document/9201310</p> Source code in <code>src/thaw_slump_segmentation/models/manet/model.py</code> <pre><code>class MAnet(SegmentationModel):\n    \"\"\"MAnet_ :  Multi-scale Attention Net. The MA-Net can capture rich contextual dependencies based on the attention mechanism, \n    using two blocks:\n     - Position-wise Attention Block (PAB), which captures the spatial dependencies between pixels in a global view\n     - Multi-scale Fusion Attention Block (MFAB), which  captures the channel dependencies between any feature map by\n       multi-scale semantic feature fusion\n\n    Args:\n        encoder_name: Name of the classification model that will be used as an encoder (a.k.a backbone)\n            to extract features of different spatial resolution\n        encoder_depth: A number of stages used in encoder in range [3, 5]. Each stage generate features \n            two times smaller in spatial dimensions than previous one (e.g. for depth 0 we will have features\n            with shapes [(N, C, H, W),], for depth 1 - [(N, C, H, W), (N, C, H // 2, W // 2)] and so on).\n            Default is 5\n        encoder_weights: One of **None** (random initialization), **\"imagenet\"** (pre-training on ImageNet) and \n            other pretrained weights (see table with available weights for each encoder_name)\n        decoder_channels: List of integers which specify **in_channels** parameter for convolutions used in decoder.\n            Length of the list should be the same as **encoder_depth**\n        decoder_use_batchnorm: If **True**, BatchNorm2d layer between Conv2D and Activation layers\n            is used. If **\"inplace\"** InplaceABN will be used, allows to decrease memory consumption.\n            Available options are **True, False, \"inplace\"**\n        decoder_pab_channels: A number of channels for PAB module in decoder. \n            Default is 64.\n        in_channels: A number of input channels for the model, default is 3 (RGB images)\n        classes: A number of classes for output mask (or you can think as a number of channels of output mask)\n        activation: An activation function to apply after the final convolution layer.\n            Available options are **\"sigmoid\"**, **\"softmax\"**, **\"logsoftmax\"**, **\"tanh\"**, **\"identity\"**, **callable** and **None**.\n            Default is **None**\n        aux_params: Dictionary with parameters of the auxiliary output (classification head). Auxiliary output is build\n            on top of encoder if **aux_params** is not **None** (default). Supported params:\n                - classes (int): A number of classes\n                - pooling (str): One of \"max\", \"avg\". Default is \"avg\"\n                - dropout (float): Dropout factor in [0, 1)\n                - activation (str): An activation function to apply \"sigmoid\"/\"softmax\" (could be **None** to return logits)\n\n    Returns:\n        ``torch.nn.Module``: **MAnet**\n\n    .. _MAnet:\n        https://ieeexplore.ieee.org/abstract/document/9201310\n\n    \"\"\"\n\n    def __init__(\n        self,\n        encoder_name: str = \"resnet34\",\n        encoder_depth: int = 5,\n        encoder_weights: Optional[str] = \"imagenet\",\n        decoder_use_batchnorm: bool = True,\n        decoder_channels: List[int] = (256, 128, 64, 32, 16),\n        decoder_pab_channels: int = 64,\n        in_channels: int = 3,\n        classes: int = 1,\n        activation: Optional[Union[str, callable]] = None,\n        aux_params: Optional[dict] = None\n    ):\n        super().__init__()\n\n        self.encoder = get_encoder(\n            encoder_name,\n            in_channels=in_channels,\n            depth=encoder_depth,\n            weights=encoder_weights,\n        )\n\n        self.decoder = MAnetDecoder(\n            encoder_channels=self.encoder.out_channels,\n            decoder_channels=decoder_channels,\n            n_blocks=encoder_depth,\n            use_batchnorm=decoder_use_batchnorm,\n            pab_channels=decoder_pab_channels\n        )\n\n        self.segmentation_head = SegmentationHead(\n            in_channels=decoder_channels[-1],\n            out_channels=classes,\n            activation=activation,\n            kernel_size=3,\n        )\n\n        if aux_params is not None:\n            self.classification_head = ClassificationHead(\n                in_channels=self.encoder.out_channels[-1], **aux_params\n            )\n        else:\n            self.classification_head = None\n\n        self.name = \"manet-{}\".format(encoder_name)\n        self.initialize()\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/manet/model/#src.thaw_slump_segmentation.models.manet.model.MAnet.classification_head","title":"<code>classification_head = ClassificationHead(in_channels=self.encoder.out_channels[-1], **aux_params)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/manet/model/#src.thaw_slump_segmentation.models.manet.model.MAnet.decoder","title":"<code>decoder = MAnetDecoder(encoder_channels=self.encoder.out_channels, decoder_channels=decoder_channels, n_blocks=encoder_depth, use_batchnorm=decoder_use_batchnorm, pab_channels=decoder_pab_channels)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/manet/model/#src.thaw_slump_segmentation.models.manet.model.MAnet.encoder","title":"<code>encoder = get_encoder(encoder_name, in_channels=in_channels, depth=encoder_depth, weights=encoder_weights)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/manet/model/#src.thaw_slump_segmentation.models.manet.model.MAnet.name","title":"<code>name = 'manet-{}'.format(encoder_name)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/manet/model/#src.thaw_slump_segmentation.models.manet.model.MAnet.segmentation_head","title":"<code>segmentation_head = SegmentationHead(in_channels=decoder_channels[-1], out_channels=classes, activation=activation, kernel_size=3)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/manet/model/#src.thaw_slump_segmentation.models.manet.model.MAnet.__init__","title":"<code>__init__(encoder_name='resnet34', encoder_depth=5, encoder_weights='imagenet', decoder_use_batchnorm=True, decoder_channels=(256, 128, 64, 32, 16), decoder_pab_channels=64, in_channels=3, classes=1, activation=None, aux_params=None)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/manet/model.py</code> <pre><code>def __init__(\n    self,\n    encoder_name: str = \"resnet34\",\n    encoder_depth: int = 5,\n    encoder_weights: Optional[str] = \"imagenet\",\n    decoder_use_batchnorm: bool = True,\n    decoder_channels: List[int] = (256, 128, 64, 32, 16),\n    decoder_pab_channels: int = 64,\n    in_channels: int = 3,\n    classes: int = 1,\n    activation: Optional[Union[str, callable]] = None,\n    aux_params: Optional[dict] = None\n):\n    super().__init__()\n\n    self.encoder = get_encoder(\n        encoder_name,\n        in_channels=in_channels,\n        depth=encoder_depth,\n        weights=encoder_weights,\n    )\n\n    self.decoder = MAnetDecoder(\n        encoder_channels=self.encoder.out_channels,\n        decoder_channels=decoder_channels,\n        n_blocks=encoder_depth,\n        use_batchnorm=decoder_use_batchnorm,\n        pab_channels=decoder_pab_channels\n    )\n\n    self.segmentation_head = SegmentationHead(\n        in_channels=decoder_channels[-1],\n        out_channels=classes,\n        activation=activation,\n        kernel_size=3,\n    )\n\n    if aux_params is not None:\n        self.classification_head = ClassificationHead(\n            in_channels=self.encoder.out_channels[-1], **aux_params\n        )\n    else:\n        self.classification_head = None\n\n    self.name = \"manet-{}\".format(encoder_name)\n    self.initialize()\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/pan/decoder/","title":"Decoder","text":""},{"location":"reference/src/thaw_slump_segmentation/models/pan/decoder/#src.thaw_slump_segmentation.models.pan.decoder.ConvBnRelu","title":"<code>ConvBnRelu</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/thaw_slump_segmentation/models/pan/decoder.py</code> <pre><code>class ConvBnRelu(nn.Module):\n    def __init__(\n            self,\n            in_channels: int,\n            out_channels: int,\n            kernel_size: int,\n            stride: int = 1,\n            padding: int = 0,\n            dilation: int = 1,\n            groups: int = 1,\n            bias: bool = True,\n            add_relu: bool = True,\n            interpolate: bool = False\n    ):\n        super(ConvBnRelu, self).__init__()\n        self.conv = nn.Conv2d(\n            in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size,\n            stride=stride, padding=padding, dilation=dilation, bias=bias, groups=groups\n        )\n        self.add_relu = add_relu\n        self.interpolate = interpolate\n        self.bn = nn.BatchNorm2d(out_channels)\n        self.activation = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        if self.add_relu:\n            x = self.activation(x)\n        if self.interpolate:\n            x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=True)\n        return x\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/pan/decoder/#src.thaw_slump_segmentation.models.pan.decoder.ConvBnRelu.activation","title":"<code>activation = nn.ReLU(inplace=True)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/pan/decoder/#src.thaw_slump_segmentation.models.pan.decoder.ConvBnRelu.add_relu","title":"<code>add_relu = add_relu</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/pan/decoder/#src.thaw_slump_segmentation.models.pan.decoder.ConvBnRelu.bn","title":"<code>bn = nn.BatchNorm2d(out_channels)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/pan/decoder/#src.thaw_slump_segmentation.models.pan.decoder.ConvBnRelu.conv","title":"<code>conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, bias=bias, groups=groups)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/pan/decoder/#src.thaw_slump_segmentation.models.pan.decoder.ConvBnRelu.interpolate","title":"<code>interpolate = interpolate</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/pan/decoder/#src.thaw_slump_segmentation.models.pan.decoder.ConvBnRelu.__init__","title":"<code>__init__(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, add_relu=True, interpolate=False)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/pan/decoder.py</code> <pre><code>def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size: int,\n        stride: int = 1,\n        padding: int = 0,\n        dilation: int = 1,\n        groups: int = 1,\n        bias: bool = True,\n        add_relu: bool = True,\n        interpolate: bool = False\n):\n    super(ConvBnRelu, self).__init__()\n    self.conv = nn.Conv2d(\n        in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size,\n        stride=stride, padding=padding, dilation=dilation, bias=bias, groups=groups\n    )\n    self.add_relu = add_relu\n    self.interpolate = interpolate\n    self.bn = nn.BatchNorm2d(out_channels)\n    self.activation = nn.ReLU(inplace=True)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/pan/decoder/#src.thaw_slump_segmentation.models.pan.decoder.ConvBnRelu.forward","title":"<code>forward(x)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/pan/decoder.py</code> <pre><code>def forward(self, x):\n    x = self.conv(x)\n    x = self.bn(x)\n    if self.add_relu:\n        x = self.activation(x)\n    if self.interpolate:\n        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=True)\n    return x\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/pan/decoder/#src.thaw_slump_segmentation.models.pan.decoder.FPABlock","title":"<code>FPABlock</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/thaw_slump_segmentation/models/pan/decoder.py</code> <pre><code>class FPABlock(nn.Module):\n    def __init__(\n            self,\n            in_channels,\n            out_channels,\n            upscale_mode='bilinear'\n    ):\n        super(FPABlock, self).__init__()\n\n        self.upscale_mode = upscale_mode\n        if self.upscale_mode == 'bilinear':\n            self.align_corners = True\n        else:\n            self.align_corners = False\n\n        # global pooling branch\n        self.branch1 = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            ConvBnRelu(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=1, padding=0)\n        )\n\n        # midddle branch\n        self.mid = nn.Sequential(\n            ConvBnRelu(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=1, padding=0)\n        )\n        self.down1 = nn.Sequential(\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            ConvBnRelu(in_channels=in_channels, out_channels=1, kernel_size=7, stride=1, padding=3)\n        )\n        self.down2 = nn.Sequential(\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            ConvBnRelu(in_channels=1, out_channels=1, kernel_size=5, stride=1, padding=2)\n        )\n        self.down3 = nn.Sequential(\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            ConvBnRelu(in_channels=1, out_channels=1, kernel_size=3, stride=1, padding=1),\n            ConvBnRelu(in_channels=1, out_channels=1, kernel_size=3, stride=1, padding=1),\n        )\n        self.conv2 = ConvBnRelu(in_channels=1, out_channels=1, kernel_size=5, stride=1, padding=2)\n        self.conv1 = ConvBnRelu(in_channels=1, out_channels=1, kernel_size=7, stride=1, padding=3)\n\n    def forward(self, x):\n        h, w = x.size(2), x.size(3)\n        b1 = self.branch1(x)\n        upscale_parameters = dict(\n            mode=self.upscale_mode,\n            align_corners=self.align_corners\n        )\n        b1 = F.interpolate(b1, size=(h, w), **upscale_parameters)\n\n        mid = self.mid(x)\n        x1 = self.down1(x)\n        x2 = self.down2(x1)\n        x3 = self.down3(x2)\n        x3 = F.interpolate(x3, size=(h // 4, w // 4), **upscale_parameters)\n\n        x2 = self.conv2(x2)\n        x = x2 + x3\n        x = F.interpolate(x, size=(h // 2, w // 2), **upscale_parameters)\n\n        x1 = self.conv1(x1)\n        x = x + x1\n        x = F.interpolate(x, size=(h, w), **upscale_parameters)\n\n        x = torch.mul(x, mid)\n        x = x + b1\n        return x\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/pan/decoder/#src.thaw_slump_segmentation.models.pan.decoder.FPABlock.align_corners","title":"<code>align_corners = True</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/pan/decoder/#src.thaw_slump_segmentation.models.pan.decoder.FPABlock.branch1","title":"<code>branch1 = nn.Sequential(nn.AdaptiveAvgPool2d(1), ConvBnRelu(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=1, padding=0))</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/pan/decoder/#src.thaw_slump_segmentation.models.pan.decoder.FPABlock.conv1","title":"<code>conv1 = ConvBnRelu(in_channels=1, out_channels=1, kernel_size=7, stride=1, padding=3)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/pan/decoder/#src.thaw_slump_segmentation.models.pan.decoder.FPABlock.conv2","title":"<code>conv2 = ConvBnRelu(in_channels=1, out_channels=1, kernel_size=5, stride=1, padding=2)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/pan/decoder/#src.thaw_slump_segmentation.models.pan.decoder.FPABlock.down1","title":"<code>down1 = nn.Sequential(nn.MaxPool2d(kernel_size=2, stride=2), ConvBnRelu(in_channels=in_channels, out_channels=1, kernel_size=7, stride=1, padding=3))</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/pan/decoder/#src.thaw_slump_segmentation.models.pan.decoder.FPABlock.down2","title":"<code>down2 = nn.Sequential(nn.MaxPool2d(kernel_size=2, stride=2), ConvBnRelu(in_channels=1, out_channels=1, kernel_size=5, stride=1, padding=2))</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/pan/decoder/#src.thaw_slump_segmentation.models.pan.decoder.FPABlock.down3","title":"<code>down3 = nn.Sequential(nn.MaxPool2d(kernel_size=2, stride=2), ConvBnRelu(in_channels=1, out_channels=1, kernel_size=3, stride=1, padding=1), ConvBnRelu(in_channels=1, out_channels=1, kernel_size=3, stride=1, padding=1))</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/pan/decoder/#src.thaw_slump_segmentation.models.pan.decoder.FPABlock.mid","title":"<code>mid = nn.Sequential(ConvBnRelu(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=1, padding=0))</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/pan/decoder/#src.thaw_slump_segmentation.models.pan.decoder.FPABlock.upscale_mode","title":"<code>upscale_mode = upscale_mode</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/pan/decoder/#src.thaw_slump_segmentation.models.pan.decoder.FPABlock.__init__","title":"<code>__init__(in_channels, out_channels, upscale_mode='bilinear')</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/pan/decoder.py</code> <pre><code>def __init__(\n        self,\n        in_channels,\n        out_channels,\n        upscale_mode='bilinear'\n):\n    super(FPABlock, self).__init__()\n\n    self.upscale_mode = upscale_mode\n    if self.upscale_mode == 'bilinear':\n        self.align_corners = True\n    else:\n        self.align_corners = False\n\n    # global pooling branch\n    self.branch1 = nn.Sequential(\n        nn.AdaptiveAvgPool2d(1),\n        ConvBnRelu(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=1, padding=0)\n    )\n\n    # midddle branch\n    self.mid = nn.Sequential(\n        ConvBnRelu(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=1, padding=0)\n    )\n    self.down1 = nn.Sequential(\n        nn.MaxPool2d(kernel_size=2, stride=2),\n        ConvBnRelu(in_channels=in_channels, out_channels=1, kernel_size=7, stride=1, padding=3)\n    )\n    self.down2 = nn.Sequential(\n        nn.MaxPool2d(kernel_size=2, stride=2),\n        ConvBnRelu(in_channels=1, out_channels=1, kernel_size=5, stride=1, padding=2)\n    )\n    self.down3 = nn.Sequential(\n        nn.MaxPool2d(kernel_size=2, stride=2),\n        ConvBnRelu(in_channels=1, out_channels=1, kernel_size=3, stride=1, padding=1),\n        ConvBnRelu(in_channels=1, out_channels=1, kernel_size=3, stride=1, padding=1),\n    )\n    self.conv2 = ConvBnRelu(in_channels=1, out_channels=1, kernel_size=5, stride=1, padding=2)\n    self.conv1 = ConvBnRelu(in_channels=1, out_channels=1, kernel_size=7, stride=1, padding=3)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/pan/decoder/#src.thaw_slump_segmentation.models.pan.decoder.FPABlock.forward","title":"<code>forward(x)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/pan/decoder.py</code> <pre><code>def forward(self, x):\n    h, w = x.size(2), x.size(3)\n    b1 = self.branch1(x)\n    upscale_parameters = dict(\n        mode=self.upscale_mode,\n        align_corners=self.align_corners\n    )\n    b1 = F.interpolate(b1, size=(h, w), **upscale_parameters)\n\n    mid = self.mid(x)\n    x1 = self.down1(x)\n    x2 = self.down2(x1)\n    x3 = self.down3(x2)\n    x3 = F.interpolate(x3, size=(h // 4, w // 4), **upscale_parameters)\n\n    x2 = self.conv2(x2)\n    x = x2 + x3\n    x = F.interpolate(x, size=(h // 2, w // 2), **upscale_parameters)\n\n    x1 = self.conv1(x1)\n    x = x + x1\n    x = F.interpolate(x, size=(h, w), **upscale_parameters)\n\n    x = torch.mul(x, mid)\n    x = x + b1\n    return x\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/pan/decoder/#src.thaw_slump_segmentation.models.pan.decoder.GAUBlock","title":"<code>GAUBlock</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/thaw_slump_segmentation/models/pan/decoder.py</code> <pre><code>class GAUBlock(nn.Module):\n    def __init__(\n            self,\n            in_channels: int,\n            out_channels: int,\n            upscale_mode: str = 'bilinear'\n    ):\n        super(GAUBlock, self).__init__()\n\n        self.upscale_mode = upscale_mode\n        self.align_corners = True if upscale_mode == 'bilinear' else None\n\n        self.conv1 = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            ConvBnRelu(in_channels=out_channels, out_channels=out_channels, kernel_size=1, add_relu=False),\n            nn.Sigmoid()\n        )\n        self.conv2 = ConvBnRelu(in_channels=in_channels, out_channels=out_channels, kernel_size=3, padding=1)\n\n    def forward(self, x, y):\n        \"\"\"\n        Args:\n            x: low level feature\n            y: high level feature\n        \"\"\"\n        h, w = x.size(2), x.size(3)\n        y_up = F.interpolate(\n            y, size=(h, w), mode=self.upscale_mode, align_corners=self.align_corners\n        )\n        x = self.conv2(x)\n        y = self.conv1(y)\n        z = torch.mul(x, y)\n        return y_up + z\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/pan/decoder/#src.thaw_slump_segmentation.models.pan.decoder.GAUBlock.align_corners","title":"<code>align_corners = True if upscale_mode == 'bilinear' else None</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/pan/decoder/#src.thaw_slump_segmentation.models.pan.decoder.GAUBlock.conv1","title":"<code>conv1 = nn.Sequential(nn.AdaptiveAvgPool2d(1), ConvBnRelu(in_channels=out_channels, out_channels=out_channels, kernel_size=1, add_relu=False), nn.Sigmoid())</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/pan/decoder/#src.thaw_slump_segmentation.models.pan.decoder.GAUBlock.conv2","title":"<code>conv2 = ConvBnRelu(in_channels=in_channels, out_channels=out_channels, kernel_size=3, padding=1)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/pan/decoder/#src.thaw_slump_segmentation.models.pan.decoder.GAUBlock.upscale_mode","title":"<code>upscale_mode = upscale_mode</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/pan/decoder/#src.thaw_slump_segmentation.models.pan.decoder.GAUBlock.__init__","title":"<code>__init__(in_channels, out_channels, upscale_mode='bilinear')</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/pan/decoder.py</code> <pre><code>def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        upscale_mode: str = 'bilinear'\n):\n    super(GAUBlock, self).__init__()\n\n    self.upscale_mode = upscale_mode\n    self.align_corners = True if upscale_mode == 'bilinear' else None\n\n    self.conv1 = nn.Sequential(\n        nn.AdaptiveAvgPool2d(1),\n        ConvBnRelu(in_channels=out_channels, out_channels=out_channels, kernel_size=1, add_relu=False),\n        nn.Sigmoid()\n    )\n    self.conv2 = ConvBnRelu(in_channels=in_channels, out_channels=out_channels, kernel_size=3, padding=1)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/pan/decoder/#src.thaw_slump_segmentation.models.pan.decoder.GAUBlock.forward","title":"<code>forward(x, y)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x</code> <p>low level feature</p> required <code>y</code> <p>high level feature</p> required Source code in <code>src/thaw_slump_segmentation/models/pan/decoder.py</code> <pre><code>def forward(self, x, y):\n    \"\"\"\n    Args:\n        x: low level feature\n        y: high level feature\n    \"\"\"\n    h, w = x.size(2), x.size(3)\n    y_up = F.interpolate(\n        y, size=(h, w), mode=self.upscale_mode, align_corners=self.align_corners\n    )\n    x = self.conv2(x)\n    y = self.conv1(y)\n    z = torch.mul(x, y)\n    return y_up + z\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/pan/decoder/#src.thaw_slump_segmentation.models.pan.decoder.PANDecoder","title":"<code>PANDecoder</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/thaw_slump_segmentation/models/pan/decoder.py</code> <pre><code>class PANDecoder(nn.Module):\n\n    def __init__(\n            self,\n            encoder_channels,\n            decoder_channels,\n            upscale_mode: str = 'bilinear'\n    ):\n        super().__init__()\n\n        self.fpa = FPABlock(in_channels=encoder_channels[-1], out_channels=decoder_channels)\n        self.gau3 = GAUBlock(in_channels=encoder_channels[-2], out_channels=decoder_channels, upscale_mode=upscale_mode)\n        self.gau2 = GAUBlock(in_channels=encoder_channels[-3], out_channels=decoder_channels, upscale_mode=upscale_mode)\n        self.gau1 = GAUBlock(in_channels=encoder_channels[-4], out_channels=decoder_channels, upscale_mode=upscale_mode)\n\n    def forward(self, *features):\n        bottleneck = features[-1]\n        x5 = self.fpa(bottleneck)         # 1/32\n        x4 = self.gau3(features[-2], x5)  # 1/16\n        x3 = self.gau2(features[-3], x4)  # 1/8\n        x2 = self.gau1(features[-4], x3)  # 1/4\n\n        return x2\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/pan/decoder/#src.thaw_slump_segmentation.models.pan.decoder.PANDecoder.fpa","title":"<code>fpa = FPABlock(in_channels=encoder_channels[-1], out_channels=decoder_channels)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/pan/decoder/#src.thaw_slump_segmentation.models.pan.decoder.PANDecoder.gau1","title":"<code>gau1 = GAUBlock(in_channels=encoder_channels[-4], out_channels=decoder_channels, upscale_mode=upscale_mode)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/pan/decoder/#src.thaw_slump_segmentation.models.pan.decoder.PANDecoder.gau2","title":"<code>gau2 = GAUBlock(in_channels=encoder_channels[-3], out_channels=decoder_channels, upscale_mode=upscale_mode)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/pan/decoder/#src.thaw_slump_segmentation.models.pan.decoder.PANDecoder.gau3","title":"<code>gau3 = GAUBlock(in_channels=encoder_channels[-2], out_channels=decoder_channels, upscale_mode=upscale_mode)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/pan/decoder/#src.thaw_slump_segmentation.models.pan.decoder.PANDecoder.__init__","title":"<code>__init__(encoder_channels, decoder_channels, upscale_mode='bilinear')</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/pan/decoder.py</code> <pre><code>def __init__(\n        self,\n        encoder_channels,\n        decoder_channels,\n        upscale_mode: str = 'bilinear'\n):\n    super().__init__()\n\n    self.fpa = FPABlock(in_channels=encoder_channels[-1], out_channels=decoder_channels)\n    self.gau3 = GAUBlock(in_channels=encoder_channels[-2], out_channels=decoder_channels, upscale_mode=upscale_mode)\n    self.gau2 = GAUBlock(in_channels=encoder_channels[-3], out_channels=decoder_channels, upscale_mode=upscale_mode)\n    self.gau1 = GAUBlock(in_channels=encoder_channels[-4], out_channels=decoder_channels, upscale_mode=upscale_mode)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/pan/decoder/#src.thaw_slump_segmentation.models.pan.decoder.PANDecoder.forward","title":"<code>forward(*features)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/pan/decoder.py</code> <pre><code>def forward(self, *features):\n    bottleneck = features[-1]\n    x5 = self.fpa(bottleneck)         # 1/32\n    x4 = self.gau3(features[-2], x5)  # 1/16\n    x3 = self.gau2(features[-3], x4)  # 1/8\n    x2 = self.gau1(features[-4], x3)  # 1/4\n\n    return x2\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/pan/model/","title":"Model","text":""},{"location":"reference/src/thaw_slump_segmentation/models/pan/model/#src.thaw_slump_segmentation.models.pan.model.PAN","title":"<code>PAN</code>","text":"<p>               Bases: <code>SegmentationModel</code></p> <p>Implementation of PAN_ (Pyramid Attention Network).</p> Note <p>Currently works with shape of input tensor &gt;= [B x C x 128 x 128] for pytorch &lt;= 1.1.0 and with shape of input tensor &gt;= [B x C x 256 x 256] for pytorch == 1.3.1</p> <p>Parameters:</p> Name Type Description Default <code>encoder_name</code> <code>str</code> <p>Name of the classification model that will be used as an encoder (a.k.a backbone) to extract features of different spatial resolution</p> <code>'resnet34'</code> <code>encoder_weights</code> <code>Optional[str]</code> <p>One of None (random initialization), \"imagenet\" (pre-training on ImageNet) and  other pretrained weights (see table with available weights for each encoder_name)</p> <code>'imagenet'</code> <code>encoder_dilation</code> <code>bool</code> <p>Flag to use dilation in encoder last layer. Doesn't work with ception, vgg,  densenet` backbones, default is True**</p> <code>True</code> <code>decoder_channels</code> <code>int</code> <p>A number of convolution layer filters in decoder blocks</p> <code>32</code> <code>in_channels</code> <code>int</code> <p>A number of input channels for the model, default is 3 (RGB images)</p> <code>3</code> <code>classes</code> <code>int</code> <p>A number of classes for output mask (or you can think as a number of channels of output mask)</p> <code>1</code> <code>activation</code> <code>Optional[Union[str, callable]]</code> <p>An activation function to apply after the final convolution layer. Available options are \"sigmoid\", \"softmax\", \"logsoftmax\", \"tanh\", \"identity\", callable and None. Default is None</p> <code>None</code> <code>upsampling</code> <code>int</code> <p>Final upsampling factor. Default is 4 to preserve input-output spatial shape identity</p> <code>4</code> <code>aux_params</code> <code>Optional[dict]</code> <p>Dictionary with parameters of the auxiliary output (classification head). Auxiliary output is build  on top of encoder if aux_params is not None (default). Supported params:     - classes (int): A number of classes     - pooling (str): One of \"max\", \"avg\". Default is \"avg\"     - dropout (float): Dropout factor in [0, 1)     - activation (str): An activation function to apply \"sigmoid\"/\"softmax\" (could be None to return logits)</p> <code>None</code> <p>Returns:</p> Type Description <p><code>torch.nn.Module</code>: PAN</p> <p>.. _PAN:     https://arxiv.org/abs/1805.10180</p> Source code in <code>src/thaw_slump_segmentation/models/pan/model.py</code> <pre><code>class PAN(SegmentationModel):\n    \"\"\" Implementation of PAN_ (Pyramid Attention Network).\n\n    Note:\n        Currently works with shape of input tensor &gt;= [B x C x 128 x 128] for pytorch &lt;= 1.1.0\n        and with shape of input tensor &gt;= [B x C x 256 x 256] for pytorch == 1.3.1\n\n    Args:\n        encoder_name: Name of the classification model that will be used as an encoder (a.k.a backbone)\n            to extract features of different spatial resolution\n        encoder_weights: One of **None** (random initialization), **\"imagenet\"** (pre-training on ImageNet) and \n            other pretrained weights (see table with available weights for each encoder_name)\n        encoder_dilation: Flag to use dilation in encoder last layer. Doesn't work with ***ception***, **vgg***, \n            **densenet*`** backbones, default is **True**\n        decoder_channels: A number of convolution layer filters in decoder blocks\n        in_channels: A number of input channels for the model, default is 3 (RGB images)\n        classes: A number of classes for output mask (or you can think as a number of channels of output mask)\n        activation: An activation function to apply after the final convolution layer.\n            Available options are **\"sigmoid\"**, **\"softmax\"**, **\"logsoftmax\"**, **\"tanh\"**, **\"identity\"**, **callable** and **None**.\n            Default is **None**\n        upsampling: Final upsampling factor. Default is 4 to preserve input-output spatial shape identity\n        aux_params: Dictionary with parameters of the auxiliary output (classification head). Auxiliary output is build \n            on top of encoder if **aux_params** is not **None** (default). Supported params:\n                - classes (int): A number of classes\n                - pooling (str): One of \"max\", \"avg\". Default is \"avg\"\n                - dropout (float): Dropout factor in [0, 1)\n                - activation (str): An activation function to apply \"sigmoid\"/\"softmax\" (could be **None** to return logits)\n\n    Returns:\n        ``torch.nn.Module``: **PAN**\n\n    .. _PAN:\n        https://arxiv.org/abs/1805.10180\n\n    \"\"\"\n\n    def __init__(\n            self,\n            encoder_name: str = \"resnet34\",\n            encoder_weights: Optional[str] = \"imagenet\",\n            encoder_dilation: bool = True,\n            decoder_channels: int = 32,\n            in_channels: int = 3,\n            classes: int = 1,\n            activation: Optional[Union[str, callable]] = None,\n            upsampling: int = 4,\n            aux_params: Optional[dict] = None\n    ):\n        super().__init__()\n\n        self.encoder = get_encoder(\n            encoder_name,\n            in_channels=in_channels,\n            depth=5,\n            weights=encoder_weights,\n        )\n\n        if encoder_dilation:\n            self.encoder.make_dilated(\n                stage_list=[5],\n                dilation_list=[2]\n            )\n\n        self.decoder = PANDecoder(\n            encoder_channels=self.encoder.out_channels,\n            decoder_channels=decoder_channels,\n        )\n\n        self.segmentation_head = SegmentationHead(\n            in_channels=decoder_channels,\n            out_channels=classes,\n            activation=activation,\n            kernel_size=3,\n            upsampling=upsampling\n        )\n\n        if aux_params is not None:\n            self.classification_head = ClassificationHead(\n                in_channels=self.encoder.out_channels[-1], **aux_params\n            )\n        else:\n            self.classification_head = None\n\n        self.name = \"pan-{}\".format(encoder_name)\n        self.initialize()\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/pan/model/#src.thaw_slump_segmentation.models.pan.model.PAN.classification_head","title":"<code>classification_head = ClassificationHead(in_channels=self.encoder.out_channels[-1], **aux_params)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/pan/model/#src.thaw_slump_segmentation.models.pan.model.PAN.decoder","title":"<code>decoder = PANDecoder(encoder_channels=self.encoder.out_channels, decoder_channels=decoder_channels)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/pan/model/#src.thaw_slump_segmentation.models.pan.model.PAN.encoder","title":"<code>encoder = get_encoder(encoder_name, in_channels=in_channels, depth=5, weights=encoder_weights)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/pan/model/#src.thaw_slump_segmentation.models.pan.model.PAN.name","title":"<code>name = 'pan-{}'.format(encoder_name)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/pan/model/#src.thaw_slump_segmentation.models.pan.model.PAN.segmentation_head","title":"<code>segmentation_head = SegmentationHead(in_channels=decoder_channels, out_channels=classes, activation=activation, kernel_size=3, upsampling=upsampling)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/pan/model/#src.thaw_slump_segmentation.models.pan.model.PAN.__init__","title":"<code>__init__(encoder_name='resnet34', encoder_weights='imagenet', encoder_dilation=True, decoder_channels=32, in_channels=3, classes=1, activation=None, upsampling=4, aux_params=None)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/pan/model.py</code> <pre><code>def __init__(\n        self,\n        encoder_name: str = \"resnet34\",\n        encoder_weights: Optional[str] = \"imagenet\",\n        encoder_dilation: bool = True,\n        decoder_channels: int = 32,\n        in_channels: int = 3,\n        classes: int = 1,\n        activation: Optional[Union[str, callable]] = None,\n        upsampling: int = 4,\n        aux_params: Optional[dict] = None\n):\n    super().__init__()\n\n    self.encoder = get_encoder(\n        encoder_name,\n        in_channels=in_channels,\n        depth=5,\n        weights=encoder_weights,\n    )\n\n    if encoder_dilation:\n        self.encoder.make_dilated(\n            stage_list=[5],\n            dilation_list=[2]\n        )\n\n    self.decoder = PANDecoder(\n        encoder_channels=self.encoder.out_channels,\n        decoder_channels=decoder_channels,\n    )\n\n    self.segmentation_head = SegmentationHead(\n        in_channels=decoder_channels,\n        out_channels=classes,\n        activation=activation,\n        kernel_size=3,\n        upsampling=upsampling\n    )\n\n    if aux_params is not None:\n        self.classification_head = ClassificationHead(\n            in_channels=self.encoder.out_channels[-1], **aux_params\n        )\n    else:\n        self.classification_head = None\n\n    self.name = \"pan-{}\".format(encoder_name)\n    self.initialize()\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/pspnet/decoder/","title":"Decoder","text":""},{"location":"reference/src/thaw_slump_segmentation/models/pspnet/decoder/#src.thaw_slump_segmentation.models.pspnet.decoder.PSPBlock","title":"<code>PSPBlock</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/thaw_slump_segmentation/models/pspnet/decoder.py</code> <pre><code>class PSPBlock(nn.Module):\n\n    def __init__(self, in_channels, out_channels, pool_size, use_bathcnorm=True):\n        super().__init__()\n        if pool_size == 1:\n            use_bathcnorm = False  # PyTorch does not support BatchNorm for 1x1 shape\n        self.pool = nn.Sequential(\n            nn.AdaptiveAvgPool2d(output_size=(pool_size, pool_size)),\n            modules.Conv2dReLU(in_channels, out_channels, (1, 1), use_batchnorm=use_bathcnorm)\n        )\n\n    def forward(self, x):\n        h, w = x.size(2), x.size(3)\n        x = self.pool(x)\n        x = F.interpolate(x, size=(h, w), mode='bilinear', align_corners=True)\n        return x\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/pspnet/decoder/#src.thaw_slump_segmentation.models.pspnet.decoder.PSPBlock.pool","title":"<code>pool = nn.Sequential(nn.AdaptiveAvgPool2d(output_size=(pool_size, pool_size)), modules.Conv2dReLU(in_channels, out_channels, (1, 1), use_batchnorm=use_bathcnorm))</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/pspnet/decoder/#src.thaw_slump_segmentation.models.pspnet.decoder.PSPBlock.__init__","title":"<code>__init__(in_channels, out_channels, pool_size, use_bathcnorm=True)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/pspnet/decoder.py</code> <pre><code>def __init__(self, in_channels, out_channels, pool_size, use_bathcnorm=True):\n    super().__init__()\n    if pool_size == 1:\n        use_bathcnorm = False  # PyTorch does not support BatchNorm for 1x1 shape\n    self.pool = nn.Sequential(\n        nn.AdaptiveAvgPool2d(output_size=(pool_size, pool_size)),\n        modules.Conv2dReLU(in_channels, out_channels, (1, 1), use_batchnorm=use_bathcnorm)\n    )\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/pspnet/decoder/#src.thaw_slump_segmentation.models.pspnet.decoder.PSPBlock.forward","title":"<code>forward(x)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/pspnet/decoder.py</code> <pre><code>def forward(self, x):\n    h, w = x.size(2), x.size(3)\n    x = self.pool(x)\n    x = F.interpolate(x, size=(h, w), mode='bilinear', align_corners=True)\n    return x\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/pspnet/decoder/#src.thaw_slump_segmentation.models.pspnet.decoder.PSPDecoder","title":"<code>PSPDecoder</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/thaw_slump_segmentation/models/pspnet/decoder.py</code> <pre><code>class PSPDecoder(nn.Module):\n\n    def __init__(\n            self,\n            encoder_channels,\n            use_batchnorm=True,\n            out_channels=512,\n            dropout=0.2,\n    ):\n        super().__init__()\n\n        self.psp = PSPModule(\n            in_channels=encoder_channels[-1],\n            sizes=(1, 2, 3, 6),\n            use_bathcnorm=use_batchnorm,\n        )\n\n        self.conv = modules.Conv2dReLU(\n            in_channels=encoder_channels[-1] * 2,\n            out_channels=out_channels,\n            kernel_size=1,\n            use_batchnorm=use_batchnorm,\n        )\n\n        self.dropout = nn.Dropout2d(p=dropout)\n\n    def forward(self, *features):\n        x = features[-1]\n        x = self.psp(x)\n        x = self.conv(x)\n        x = self.dropout(x)\n\n        return x\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/pspnet/decoder/#src.thaw_slump_segmentation.models.pspnet.decoder.PSPDecoder.conv","title":"<code>conv = modules.Conv2dReLU(in_channels=encoder_channels[-1] * 2, out_channels=out_channels, kernel_size=1, use_batchnorm=use_batchnorm)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/pspnet/decoder/#src.thaw_slump_segmentation.models.pspnet.decoder.PSPDecoder.dropout","title":"<code>dropout = nn.Dropout2d(p=dropout)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/pspnet/decoder/#src.thaw_slump_segmentation.models.pspnet.decoder.PSPDecoder.psp","title":"<code>psp = PSPModule(in_channels=encoder_channels[-1], sizes=(1, 2, 3, 6), use_bathcnorm=use_batchnorm)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/pspnet/decoder/#src.thaw_slump_segmentation.models.pspnet.decoder.PSPDecoder.__init__","title":"<code>__init__(encoder_channels, use_batchnorm=True, out_channels=512, dropout=0.2)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/pspnet/decoder.py</code> <pre><code>def __init__(\n        self,\n        encoder_channels,\n        use_batchnorm=True,\n        out_channels=512,\n        dropout=0.2,\n):\n    super().__init__()\n\n    self.psp = PSPModule(\n        in_channels=encoder_channels[-1],\n        sizes=(1, 2, 3, 6),\n        use_bathcnorm=use_batchnorm,\n    )\n\n    self.conv = modules.Conv2dReLU(\n        in_channels=encoder_channels[-1] * 2,\n        out_channels=out_channels,\n        kernel_size=1,\n        use_batchnorm=use_batchnorm,\n    )\n\n    self.dropout = nn.Dropout2d(p=dropout)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/pspnet/decoder/#src.thaw_slump_segmentation.models.pspnet.decoder.PSPDecoder.forward","title":"<code>forward(*features)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/pspnet/decoder.py</code> <pre><code>def forward(self, *features):\n    x = features[-1]\n    x = self.psp(x)\n    x = self.conv(x)\n    x = self.dropout(x)\n\n    return x\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/pspnet/decoder/#src.thaw_slump_segmentation.models.pspnet.decoder.PSPModule","title":"<code>PSPModule</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/thaw_slump_segmentation/models/pspnet/decoder.py</code> <pre><code>class PSPModule(nn.Module):\n    def __init__(self, in_channels, sizes=(1, 2, 3, 6), use_bathcnorm=True):\n        super().__init__()\n\n        self.blocks = nn.ModuleList([\n            PSPBlock(in_channels, in_channels // len(sizes), size, use_bathcnorm=use_bathcnorm) for size in sizes\n        ])\n\n    def forward(self, x):\n        xs = [block(x) for block in self.blocks] + [x]\n        x = torch.cat(xs, dim=1)\n        return x\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/pspnet/decoder/#src.thaw_slump_segmentation.models.pspnet.decoder.PSPModule.blocks","title":"<code>blocks = nn.ModuleList([PSPBlock(in_channels, in_channels // len(sizes), size, use_bathcnorm=use_bathcnorm) for size in sizes])</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/pspnet/decoder/#src.thaw_slump_segmentation.models.pspnet.decoder.PSPModule.__init__","title":"<code>__init__(in_channels, sizes=(1, 2, 3, 6), use_bathcnorm=True)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/pspnet/decoder.py</code> <pre><code>def __init__(self, in_channels, sizes=(1, 2, 3, 6), use_bathcnorm=True):\n    super().__init__()\n\n    self.blocks = nn.ModuleList([\n        PSPBlock(in_channels, in_channels // len(sizes), size, use_bathcnorm=use_bathcnorm) for size in sizes\n    ])\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/pspnet/decoder/#src.thaw_slump_segmentation.models.pspnet.decoder.PSPModule.forward","title":"<code>forward(x)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/pspnet/decoder.py</code> <pre><code>def forward(self, x):\n    xs = [block(x) for block in self.blocks] + [x]\n    x = torch.cat(xs, dim=1)\n    return x\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/pspnet/model/","title":"Model","text":""},{"location":"reference/src/thaw_slump_segmentation/models/pspnet/model/#src.thaw_slump_segmentation.models.pspnet.model.PSPNet","title":"<code>PSPNet</code>","text":"<p>               Bases: <code>SegmentationModel</code></p> <p>PSPNet_ is a fully convolution neural network for image semantic segmentation. Consist of  encoder and Spatial Pyramid (decoder). Spatial Pyramid build on top of encoder and does not  use \"fine-features\" (features of high spatial resolution). PSPNet can be used for multiclass segmentation of high resolution images, however it is not good for detecting small objects and producing accurate, pixel-level mask. </p> <p>Parameters:</p> Name Type Description Default <code>encoder_name</code> <code>str</code> <p>Name of the classification model that will be used as an encoder (a.k.a backbone) to extract features of different spatial resolution</p> <code>'resnet34'</code> <code>encoder_depth</code> <code>int</code> <p>A number of stages used in encoder in range [3, 5]. Each stage generate features  two times smaller in spatial dimensions than previous one (e.g. for depth 0 we will have features with shapes [(N, C, H, W),], for depth 1 - [(N, C, H, W), (N, C, H // 2, W // 2)] and so on). Default is 5</p> <code>3</code> <code>encoder_weights</code> <code>Optional[str]</code> <p>One of None (random initialization), \"imagenet\" (pre-training on ImageNet) and  other pretrained weights (see table with available weights for each encoder_name)</p> <code>'imagenet'</code> <code>psp_out_channels</code> <code>int</code> <p>A number of filters in Spatial Pyramid</p> <code>512</code> <code>psp_use_batchnorm</code> <code>bool</code> <p>If True, BatchNorm2d layer between Conv2D and Activation layers is used. If \"inplace\" InplaceABN will be used, allows to decrease memory consumption. Available options are True, False, \"inplace\"</p> <code>True</code> <code>psp_dropout</code> <code>float</code> <p>Spatial dropout rate in [0, 1) used in Spatial Pyramid</p> <code>0.2</code> <code>in_channels</code> <code>int</code> <p>A number of input channels for the model, default is 3 (RGB images)</p> <code>3</code> <code>classes</code> <code>int</code> <p>A number of classes for output mask (or you can think as a number of channels of output mask)</p> <code>1</code> <code>activation</code> <code>Optional[Union[str, callable]]</code> <p>An activation function to apply after the final convolution layer. Available options are \"sigmoid\", \"softmax\", \"logsoftmax\", \"tanh\", \"identity\", callable and None. Default is None</p> <code>None</code> <code>upsampling</code> <code>int</code> <p>Final upsampling factor. Default is 8 to preserve input-output spatial shape identity</p> <code>8</code> <code>aux_params</code> <code>Optional[dict]</code> <p>Dictionary with parameters of the auxiliary output (classification head). Auxiliary output is build  on top of encoder if aux_params is not None (default). Supported params:     - classes (int): A number of classes     - pooling (str): One of \"max\", \"avg\". Default is \"avg\"     - dropout (float): Dropout factor in [0, 1)     - activation (str): An activation function to apply \"sigmoid\"/\"softmax\" (could be None to return logits)</p> <code>None</code> <p>Returns:</p> Type Description <p><code>torch.nn.Module</code>: PSPNet</p> <p>.. _PSPNet:     https://arxiv.org/abs/1612.01105</p> Source code in <code>src/thaw_slump_segmentation/models/pspnet/model.py</code> <pre><code>class PSPNet(SegmentationModel):\n    \"\"\"PSPNet_ is a fully convolution neural network for image semantic segmentation. Consist of \n    *encoder* and *Spatial Pyramid* (decoder). Spatial Pyramid build on top of encoder and does not \n    use \"fine-features\" (features of high spatial resolution). PSPNet can be used for multiclass segmentation\n    of high resolution images, however it is not good for detecting small objects and producing accurate, pixel-level mask. \n\n    Args:\n        encoder_name: Name of the classification model that will be used as an encoder (a.k.a backbone)\n            to extract features of different spatial resolution\n        encoder_depth: A number of stages used in encoder in range [3, 5]. Each stage generate features \n            two times smaller in spatial dimensions than previous one (e.g. for depth 0 we will have features\n            with shapes [(N, C, H, W),], for depth 1 - [(N, C, H, W), (N, C, H // 2, W // 2)] and so on).\n            Default is 5\n        encoder_weights: One of **None** (random initialization), **\"imagenet\"** (pre-training on ImageNet) and \n            other pretrained weights (see table with available weights for each encoder_name)\n        psp_out_channels: A number of filters in Spatial Pyramid\n        psp_use_batchnorm: If **True**, BatchNorm2d layer between Conv2D and Activation layers\n            is used. If **\"inplace\"** InplaceABN will be used, allows to decrease memory consumption.\n            Available options are **True, False, \"inplace\"**\n        psp_dropout: Spatial dropout rate in [0, 1) used in Spatial Pyramid\n        in_channels: A number of input channels for the model, default is 3 (RGB images)\n        classes: A number of classes for output mask (or you can think as a number of channels of output mask)\n        activation: An activation function to apply after the final convolution layer.\n            Available options are **\"sigmoid\"**, **\"softmax\"**, **\"logsoftmax\"**, **\"tanh\"**, **\"identity\"**, **callable** and **None**.\n            Default is **None**\n        upsampling: Final upsampling factor. Default is 8 to preserve input-output spatial shape identity\n        aux_params: Dictionary with parameters of the auxiliary output (classification head). Auxiliary output is build \n            on top of encoder if **aux_params** is not **None** (default). Supported params:\n                - classes (int): A number of classes\n                - pooling (str): One of \"max\", \"avg\". Default is \"avg\"\n                - dropout (float): Dropout factor in [0, 1)\n                - activation (str): An activation function to apply \"sigmoid\"/\"softmax\" (could be **None** to return logits)\n\n    Returns:\n        ``torch.nn.Module``: **PSPNet**\n\n    .. _PSPNet:\n        https://arxiv.org/abs/1612.01105\n    \"\"\"\n\n    def __init__(\n        self,\n        encoder_name: str = \"resnet34\",\n        encoder_weights: Optional[str] = \"imagenet\",\n        encoder_depth: int = 3,\n        psp_out_channels: int = 512,\n        psp_use_batchnorm: bool = True,\n        psp_dropout: float = 0.2,\n        in_channels: int = 3,\n        classes: int = 1,\n        activation: Optional[Union[str, callable]] = None,\n        upsampling: int = 8,\n        aux_params: Optional[dict] = None,\n    ):\n        super().__init__()\n\n        self.encoder = get_encoder(\n            encoder_name,\n            in_channels=in_channels,\n            depth=encoder_depth,\n            weights=encoder_weights,\n        )\n\n        self.decoder = PSPDecoder(\n            encoder_channels=self.encoder.out_channels,\n            use_batchnorm=psp_use_batchnorm,\n            out_channels=psp_out_channels,\n            dropout=psp_dropout,\n        )\n\n        self.segmentation_head = SegmentationHead(\n            in_channels=psp_out_channels,\n            out_channels=classes,\n            kernel_size=3,\n            activation=activation,\n            upsampling=upsampling,\n        )\n\n        if aux_params:\n            self.classification_head = ClassificationHead(\n                in_channels=self.encoder.out_channels[-1], **aux_params\n            )\n        else:\n            self.classification_head = None\n\n        self.name = \"psp-{}\".format(encoder_name)\n        self.initialize()\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/pspnet/model/#src.thaw_slump_segmentation.models.pspnet.model.PSPNet.classification_head","title":"<code>classification_head = ClassificationHead(in_channels=self.encoder.out_channels[-1], **aux_params)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/pspnet/model/#src.thaw_slump_segmentation.models.pspnet.model.PSPNet.decoder","title":"<code>decoder = PSPDecoder(encoder_channels=self.encoder.out_channels, use_batchnorm=psp_use_batchnorm, out_channels=psp_out_channels, dropout=psp_dropout)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/pspnet/model/#src.thaw_slump_segmentation.models.pspnet.model.PSPNet.encoder","title":"<code>encoder = get_encoder(encoder_name, in_channels=in_channels, depth=encoder_depth, weights=encoder_weights)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/pspnet/model/#src.thaw_slump_segmentation.models.pspnet.model.PSPNet.name","title":"<code>name = 'psp-{}'.format(encoder_name)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/pspnet/model/#src.thaw_slump_segmentation.models.pspnet.model.PSPNet.segmentation_head","title":"<code>segmentation_head = SegmentationHead(in_channels=psp_out_channels, out_channels=classes, kernel_size=3, activation=activation, upsampling=upsampling)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/pspnet/model/#src.thaw_slump_segmentation.models.pspnet.model.PSPNet.__init__","title":"<code>__init__(encoder_name='resnet34', encoder_weights='imagenet', encoder_depth=3, psp_out_channels=512, psp_use_batchnorm=True, psp_dropout=0.2, in_channels=3, classes=1, activation=None, upsampling=8, aux_params=None)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/pspnet/model.py</code> <pre><code>def __init__(\n    self,\n    encoder_name: str = \"resnet34\",\n    encoder_weights: Optional[str] = \"imagenet\",\n    encoder_depth: int = 3,\n    psp_out_channels: int = 512,\n    psp_use_batchnorm: bool = True,\n    psp_dropout: float = 0.2,\n    in_channels: int = 3,\n    classes: int = 1,\n    activation: Optional[Union[str, callable]] = None,\n    upsampling: int = 8,\n    aux_params: Optional[dict] = None,\n):\n    super().__init__()\n\n    self.encoder = get_encoder(\n        encoder_name,\n        in_channels=in_channels,\n        depth=encoder_depth,\n        weights=encoder_weights,\n    )\n\n    self.decoder = PSPDecoder(\n        encoder_channels=self.encoder.out_channels,\n        use_batchnorm=psp_use_batchnorm,\n        out_channels=psp_out_channels,\n        dropout=psp_dropout,\n    )\n\n    self.segmentation_head = SegmentationHead(\n        in_channels=psp_out_channels,\n        out_channels=classes,\n        kernel_size=3,\n        activation=activation,\n        upsampling=upsampling,\n    )\n\n    if aux_params:\n        self.classification_head = ClassificationHead(\n            in_channels=self.encoder.out_channels[-1], **aux_params\n        )\n    else:\n        self.classification_head = None\n\n    self.name = \"psp-{}\".format(encoder_name)\n    self.initialize()\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/unet/decoder/","title":"Decoder","text":""},{"location":"reference/src/thaw_slump_segmentation/models/unet/decoder/#src.thaw_slump_segmentation.models.unet.decoder.CenterBlock","title":"<code>CenterBlock</code>","text":"<p>               Bases: <code>Sequential</code></p> Source code in <code>src/thaw_slump_segmentation/models/unet/decoder.py</code> <pre><code>class CenterBlock(nn.Sequential):\n    def __init__(self, in_channels, out_channels, use_batchnorm=True):\n        conv1 = md.Conv2dReLU(\n            in_channels,\n            out_channels,\n            kernel_size=3,\n            padding=1,\n            use_batchnorm=use_batchnorm,\n        )\n        conv2 = md.Conv2dReLU(\n            out_channels,\n            out_channels,\n            kernel_size=3,\n            padding=1,\n            use_batchnorm=use_batchnorm,\n        )\n        super().__init__(conv1, conv2)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/unet/decoder/#src.thaw_slump_segmentation.models.unet.decoder.CenterBlock.__init__","title":"<code>__init__(in_channels, out_channels, use_batchnorm=True)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/unet/decoder.py</code> <pre><code>def __init__(self, in_channels, out_channels, use_batchnorm=True):\n    conv1 = md.Conv2dReLU(\n        in_channels,\n        out_channels,\n        kernel_size=3,\n        padding=1,\n        use_batchnorm=use_batchnorm,\n    )\n    conv2 = md.Conv2dReLU(\n        out_channels,\n        out_channels,\n        kernel_size=3,\n        padding=1,\n        use_batchnorm=use_batchnorm,\n    )\n    super().__init__(conv1, conv2)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/unet/decoder/#src.thaw_slump_segmentation.models.unet.decoder.DecoderBlock","title":"<code>DecoderBlock</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/thaw_slump_segmentation/models/unet/decoder.py</code> <pre><code>class DecoderBlock(nn.Module):\n    def __init__(\n            self,\n            in_channels,\n            skip_channels,\n            out_channels,\n            use_batchnorm=True,\n            attention_type=None,\n    ):\n        super().__init__()\n        self.conv1 = md.Conv2dReLU(\n            in_channels + skip_channels,\n            out_channels,\n            kernel_size=3,\n            padding=1,\n            use_batchnorm=use_batchnorm,\n        )\n        self.attention1 = md.Attention(attention_type, in_channels=in_channels + skip_channels)\n        self.conv2 = md.Conv2dReLU(\n            out_channels,\n            out_channels,\n            kernel_size=3,\n            padding=1,\n            use_batchnorm=use_batchnorm,\n        )\n        self.attention2 = md.Attention(attention_type, in_channels=out_channels)\n\n    def forward(self, x, skip=None):\n        x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n        if skip is not None:\n            x = torch.cat([x, skip], dim=1)\n            x = self.attention1(x)\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.attention2(x)\n        return x\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/unet/decoder/#src.thaw_slump_segmentation.models.unet.decoder.DecoderBlock.attention1","title":"<code>attention1 = md.Attention(attention_type, in_channels=in_channels + skip_channels)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/unet/decoder/#src.thaw_slump_segmentation.models.unet.decoder.DecoderBlock.attention2","title":"<code>attention2 = md.Attention(attention_type, in_channels=out_channels)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/unet/decoder/#src.thaw_slump_segmentation.models.unet.decoder.DecoderBlock.conv1","title":"<code>conv1 = md.Conv2dReLU(in_channels + skip_channels, out_channels, kernel_size=3, padding=1, use_batchnorm=use_batchnorm)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/unet/decoder/#src.thaw_slump_segmentation.models.unet.decoder.DecoderBlock.conv2","title":"<code>conv2 = md.Conv2dReLU(out_channels, out_channels, kernel_size=3, padding=1, use_batchnorm=use_batchnorm)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/unet/decoder/#src.thaw_slump_segmentation.models.unet.decoder.DecoderBlock.__init__","title":"<code>__init__(in_channels, skip_channels, out_channels, use_batchnorm=True, attention_type=None)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/unet/decoder.py</code> <pre><code>def __init__(\n        self,\n        in_channels,\n        skip_channels,\n        out_channels,\n        use_batchnorm=True,\n        attention_type=None,\n):\n    super().__init__()\n    self.conv1 = md.Conv2dReLU(\n        in_channels + skip_channels,\n        out_channels,\n        kernel_size=3,\n        padding=1,\n        use_batchnorm=use_batchnorm,\n    )\n    self.attention1 = md.Attention(attention_type, in_channels=in_channels + skip_channels)\n    self.conv2 = md.Conv2dReLU(\n        out_channels,\n        out_channels,\n        kernel_size=3,\n        padding=1,\n        use_batchnorm=use_batchnorm,\n    )\n    self.attention2 = md.Attention(attention_type, in_channels=out_channels)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/unet/decoder/#src.thaw_slump_segmentation.models.unet.decoder.DecoderBlock.forward","title":"<code>forward(x, skip=None)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/unet/decoder.py</code> <pre><code>def forward(self, x, skip=None):\n    x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n    if skip is not None:\n        x = torch.cat([x, skip], dim=1)\n        x = self.attention1(x)\n    x = self.conv1(x)\n    x = self.conv2(x)\n    x = self.attention2(x)\n    return x\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/unet/decoder/#src.thaw_slump_segmentation.models.unet.decoder.UnetDecoder","title":"<code>UnetDecoder</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/thaw_slump_segmentation/models/unet/decoder.py</code> <pre><code>class UnetDecoder(nn.Module):\n    def __init__(\n            self,\n            encoder_channels,\n            decoder_channels,\n            n_blocks=5,\n            use_batchnorm=True,\n            attention_type=None,\n            center=False,\n    ):\n        super().__init__()\n\n        if n_blocks != len(decoder_channels):\n            raise ValueError(\n                \"Model depth is {}, but you provide `decoder_channels` for {} blocks.\".format(\n                    n_blocks, len(decoder_channels)\n                )\n            )\n\n        encoder_channels = encoder_channels[1:]  # remove first skip with same spatial resolution\n        encoder_channels = encoder_channels[::-1]  # reverse channels to start from head of encoder\n\n        # computing blocks input and output channels\n        head_channels = encoder_channels[0]\n        in_channels = [head_channels] + list(decoder_channels[:-1])\n        skip_channels = list(encoder_channels[1:]) + [0]\n        out_channels = decoder_channels\n\n        if center:\n            self.center = CenterBlock(\n                head_channels, head_channels, use_batchnorm=use_batchnorm\n            )\n        else:\n            self.center = nn.Identity()\n\n        # combine decoder keyword arguments\n        kwargs = dict(use_batchnorm=use_batchnorm, attention_type=attention_type)\n        blocks = [\n            DecoderBlock(in_ch, skip_ch, out_ch, **kwargs)\n            for in_ch, skip_ch, out_ch in zip(in_channels, skip_channels, out_channels)\n        ]\n        self.blocks = nn.ModuleList(blocks)\n\n    def forward(self, *features):\n\n        features = features[1:]    # remove first skip with same spatial resolution\n        features = features[::-1]  # reverse channels to start from head of encoder\n\n        head = features[0]\n        skips = features[1:]\n\n        x = self.center(head)\n        for i, decoder_block in enumerate(self.blocks):\n            skip = skips[i] if i &lt; len(skips) else None\n            x = decoder_block(x, skip)\n\n        return x\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/unet/decoder/#src.thaw_slump_segmentation.models.unet.decoder.UnetDecoder.blocks","title":"<code>blocks = nn.ModuleList(blocks)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/unet/decoder/#src.thaw_slump_segmentation.models.unet.decoder.UnetDecoder.center","title":"<code>center = CenterBlock(head_channels, head_channels, use_batchnorm=use_batchnorm)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/unet/decoder/#src.thaw_slump_segmentation.models.unet.decoder.UnetDecoder.__init__","title":"<code>__init__(encoder_channels, decoder_channels, n_blocks=5, use_batchnorm=True, attention_type=None, center=False)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/unet/decoder.py</code> <pre><code>def __init__(\n        self,\n        encoder_channels,\n        decoder_channels,\n        n_blocks=5,\n        use_batchnorm=True,\n        attention_type=None,\n        center=False,\n):\n    super().__init__()\n\n    if n_blocks != len(decoder_channels):\n        raise ValueError(\n            \"Model depth is {}, but you provide `decoder_channels` for {} blocks.\".format(\n                n_blocks, len(decoder_channels)\n            )\n        )\n\n    encoder_channels = encoder_channels[1:]  # remove first skip with same spatial resolution\n    encoder_channels = encoder_channels[::-1]  # reverse channels to start from head of encoder\n\n    # computing blocks input and output channels\n    head_channels = encoder_channels[0]\n    in_channels = [head_channels] + list(decoder_channels[:-1])\n    skip_channels = list(encoder_channels[1:]) + [0]\n    out_channels = decoder_channels\n\n    if center:\n        self.center = CenterBlock(\n            head_channels, head_channels, use_batchnorm=use_batchnorm\n        )\n    else:\n        self.center = nn.Identity()\n\n    # combine decoder keyword arguments\n    kwargs = dict(use_batchnorm=use_batchnorm, attention_type=attention_type)\n    blocks = [\n        DecoderBlock(in_ch, skip_ch, out_ch, **kwargs)\n        for in_ch, skip_ch, out_ch in zip(in_channels, skip_channels, out_channels)\n    ]\n    self.blocks = nn.ModuleList(blocks)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/unet/decoder/#src.thaw_slump_segmentation.models.unet.decoder.UnetDecoder.forward","title":"<code>forward(*features)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/unet/decoder.py</code> <pre><code>def forward(self, *features):\n\n    features = features[1:]    # remove first skip with same spatial resolution\n    features = features[::-1]  # reverse channels to start from head of encoder\n\n    head = features[0]\n    skips = features[1:]\n\n    x = self.center(head)\n    for i, decoder_block in enumerate(self.blocks):\n        skip = skips[i] if i &lt; len(skips) else None\n        x = decoder_block(x, skip)\n\n    return x\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/unet/model/","title":"Model","text":""},{"location":"reference/src/thaw_slump_segmentation/models/unet/model/#src.thaw_slump_segmentation.models.unet.model.Unet","title":"<code>Unet</code>","text":"<p>               Bases: <code>SegmentationModel</code></p> <p>Unet_ is a fully convolution neural network for image semantic segmentation. Consist of encoder  and decoder parts connected with skip connections. Encoder extract features of different spatial  resolution (skip connections) which are used by decoder to define accurate segmentation mask. Use concatenation for fusing decoder blocks with skip connections.</p> <p>Parameters:</p> Name Type Description Default <code>encoder_name</code> <code>str</code> <p>Name of the classification model that will be used as an encoder (a.k.a backbone) to extract features of different spatial resolution</p> <code>'resnet34'</code> <code>encoder_depth</code> <code>int</code> <p>A number of stages used in encoder in range [3, 5]. Each stage generate features  two times smaller in spatial dimensions than previous one (e.g. for depth 0 we will have features with shapes [(N, C, H, W),], for depth 1 - [(N, C, H, W), (N, C, H // 2, W // 2)] and so on). Default is 5</p> <code>5</code> <code>encoder_weights</code> <code>Optional[str]</code> <p>One of None (random initialization), \"imagenet\" (pre-training on ImageNet) and  other pretrained weights (see table with available weights for each encoder_name)</p> <code>'imagenet'</code> <code>decoder_channels</code> <code>List[int]</code> <p>List of integers which specify in_channels parameter for convolutions used in decoder. Length of the list should be the same as encoder_depth</p> <code>(256, 128, 64, 32, 16)</code> <code>decoder_use_batchnorm</code> <code>bool</code> <p>If True, BatchNorm2d layer between Conv2D and Activation layers is used. If \"inplace\" InplaceABN will be used, allows to decrease memory consumption. Available options are True, False, \"inplace\"</p> <code>True</code> <code>decoder_attention_type</code> <code>Optional[str]</code> <p>Attention module used in decoder of the model. Available options are None and scse. SCSE paper - https://arxiv.org/abs/1808.08127</p> <code>None</code> <code>in_channels</code> <code>int</code> <p>A number of input channels for the model, default is 3 (RGB images)</p> <code>3</code> <code>classes</code> <code>int</code> <p>A number of classes for output mask (or you can think as a number of channels of output mask)</p> <code>1</code> <code>activation</code> <code>Optional[Union[str, callable]]</code> <p>An activation function to apply after the final convolution layer. Available options are \"sigmoid\", \"softmax\", \"logsoftmax\", \"tanh\", \"identity\", callable and None. Default is None</p> <code>None</code> <code>aux_params</code> <code>Optional[dict]</code> <p>Dictionary with parameters of the auxiliary output (classification head). Auxiliary output is build  on top of encoder if aux_params is not None (default). Supported params:     - classes (int): A number of classes     - pooling (str): One of \"max\", \"avg\". Default is \"avg\"     - dropout (float): Dropout factor in [0, 1)     - activation (str): An activation function to apply \"sigmoid\"/\"softmax\" (could be None to return logits)</p> <code>None</code> <p>Returns:</p> Type Description <p><code>torch.nn.Module</code>: Unet</p> <p>.. _Unet:     https://arxiv.org/abs/1505.04597</p> Source code in <code>src/thaw_slump_segmentation/models/unet/model.py</code> <pre><code>class Unet(SegmentationModel):\n    \"\"\"Unet_ is a fully convolution neural network for image semantic segmentation. Consist of *encoder* \n    and *decoder* parts connected with *skip connections*. Encoder extract features of different spatial \n    resolution (skip connections) which are used by decoder to define accurate segmentation mask. Use *concatenation*\n    for fusing decoder blocks with skip connections.\n\n    Args:\n        encoder_name: Name of the classification model that will be used as an encoder (a.k.a backbone)\n            to extract features of different spatial resolution\n        encoder_depth: A number of stages used in encoder in range [3, 5]. Each stage generate features \n            two times smaller in spatial dimensions than previous one (e.g. for depth 0 we will have features\n            with shapes [(N, C, H, W),], for depth 1 - [(N, C, H, W), (N, C, H // 2, W // 2)] and so on).\n            Default is 5\n        encoder_weights: One of **None** (random initialization), **\"imagenet\"** (pre-training on ImageNet) and \n            other pretrained weights (see table with available weights for each encoder_name)\n        decoder_channels: List of integers which specify **in_channels** parameter for convolutions used in decoder.\n            Length of the list should be the same as **encoder_depth**\n        decoder_use_batchnorm: If **True**, BatchNorm2d layer between Conv2D and Activation layers\n            is used. If **\"inplace\"** InplaceABN will be used, allows to decrease memory consumption.\n            Available options are **True, False, \"inplace\"**\n        decoder_attention_type: Attention module used in decoder of the model. Available options are **None** and **scse**.\n            SCSE paper - https://arxiv.org/abs/1808.08127\n        in_channels: A number of input channels for the model, default is 3 (RGB images)\n        classes: A number of classes for output mask (or you can think as a number of channels of output mask)\n        activation: An activation function to apply after the final convolution layer.\n            Available options are **\"sigmoid\"**, **\"softmax\"**, **\"logsoftmax\"**, **\"tanh\"**, **\"identity\"**, **callable** and **None**.\n            Default is **None**\n        aux_params: Dictionary with parameters of the auxiliary output (classification head). Auxiliary output is build \n            on top of encoder if **aux_params** is not **None** (default). Supported params:\n                - classes (int): A number of classes\n                - pooling (str): One of \"max\", \"avg\". Default is \"avg\"\n                - dropout (float): Dropout factor in [0, 1)\n                - activation (str): An activation function to apply \"sigmoid\"/\"softmax\" (could be **None** to return logits)\n\n    Returns:\n        ``torch.nn.Module``: Unet\n\n    .. _Unet:\n        https://arxiv.org/abs/1505.04597\n\n    \"\"\"\n\n    def __init__(\n        self,\n        encoder_name: str = \"resnet34\",\n        encoder_depth: int = 5,\n        encoder_weights: Optional[str] = \"imagenet\",\n        decoder_use_batchnorm: bool = True,\n        decoder_channels: List[int] = (256, 128, 64, 32, 16),\n        decoder_attention_type: Optional[str] = None,\n        in_channels: int = 3,\n        classes: int = 1,\n        activation: Optional[Union[str, callable]] = None,\n        aux_params: Optional[dict] = None,\n    ):\n        super().__init__()\n\n        self.encoder = get_encoder(\n            encoder_name,\n            in_channels=in_channels,\n            depth=encoder_depth,\n            weights=encoder_weights,\n        )\n\n        self.decoder = UnetDecoder(\n            encoder_channels=self.encoder.out_channels,\n            decoder_channels=decoder_channels,\n            n_blocks=encoder_depth,\n            use_batchnorm=decoder_use_batchnorm,\n            center=True if encoder_name.startswith(\"vgg\") else False,\n            attention_type=decoder_attention_type,\n        )\n\n        self.segmentation_head = SegmentationHead(\n            in_channels=decoder_channels[-1],\n            out_channels=classes,\n            activation=activation,\n            kernel_size=3,\n        )\n\n        if aux_params is not None:\n            self.classification_head = ClassificationHead(\n                in_channels=self.encoder.out_channels[-1], **aux_params\n            )\n        else:\n            self.classification_head = None\n\n        self.name = \"u-{}\".format(encoder_name)\n        self.initialize()\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/unet/model/#src.thaw_slump_segmentation.models.unet.model.Unet.classification_head","title":"<code>classification_head = ClassificationHead(in_channels=self.encoder.out_channels[-1], **aux_params)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/unet/model/#src.thaw_slump_segmentation.models.unet.model.Unet.decoder","title":"<code>decoder = UnetDecoder(encoder_channels=self.encoder.out_channels, decoder_channels=decoder_channels, n_blocks=encoder_depth, use_batchnorm=decoder_use_batchnorm, center=True if encoder_name.startswith('vgg') else False, attention_type=decoder_attention_type)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/unet/model/#src.thaw_slump_segmentation.models.unet.model.Unet.encoder","title":"<code>encoder = get_encoder(encoder_name, in_channels=in_channels, depth=encoder_depth, weights=encoder_weights)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/unet/model/#src.thaw_slump_segmentation.models.unet.model.Unet.name","title":"<code>name = 'u-{}'.format(encoder_name)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/unet/model/#src.thaw_slump_segmentation.models.unet.model.Unet.segmentation_head","title":"<code>segmentation_head = SegmentationHead(in_channels=decoder_channels[-1], out_channels=classes, activation=activation, kernel_size=3)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/unet/model/#src.thaw_slump_segmentation.models.unet.model.Unet.__init__","title":"<code>__init__(encoder_name='resnet34', encoder_depth=5, encoder_weights='imagenet', decoder_use_batchnorm=True, decoder_channels=(256, 128, 64, 32, 16), decoder_attention_type=None, in_channels=3, classes=1, activation=None, aux_params=None)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/unet/model.py</code> <pre><code>def __init__(\n    self,\n    encoder_name: str = \"resnet34\",\n    encoder_depth: int = 5,\n    encoder_weights: Optional[str] = \"imagenet\",\n    decoder_use_batchnorm: bool = True,\n    decoder_channels: List[int] = (256, 128, 64, 32, 16),\n    decoder_attention_type: Optional[str] = None,\n    in_channels: int = 3,\n    classes: int = 1,\n    activation: Optional[Union[str, callable]] = None,\n    aux_params: Optional[dict] = None,\n):\n    super().__init__()\n\n    self.encoder = get_encoder(\n        encoder_name,\n        in_channels=in_channels,\n        depth=encoder_depth,\n        weights=encoder_weights,\n    )\n\n    self.decoder = UnetDecoder(\n        encoder_channels=self.encoder.out_channels,\n        decoder_channels=decoder_channels,\n        n_blocks=encoder_depth,\n        use_batchnorm=decoder_use_batchnorm,\n        center=True if encoder_name.startswith(\"vgg\") else False,\n        attention_type=decoder_attention_type,\n    )\n\n    self.segmentation_head = SegmentationHead(\n        in_channels=decoder_channels[-1],\n        out_channels=classes,\n        activation=activation,\n        kernel_size=3,\n    )\n\n    if aux_params is not None:\n        self.classification_head = ClassificationHead(\n            in_channels=self.encoder.out_channels[-1], **aux_params\n        )\n    else:\n        self.classification_head = None\n\n    self.name = \"u-{}\".format(encoder_name)\n    self.initialize()\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/unetplusplus/decoder/","title":"Decoder","text":""},{"location":"reference/src/thaw_slump_segmentation/models/unetplusplus/decoder/#src.thaw_slump_segmentation.models.unetplusplus.decoder.CenterBlock","title":"<code>CenterBlock</code>","text":"<p>               Bases: <code>Sequential</code></p> Source code in <code>src/thaw_slump_segmentation/models/unetplusplus/decoder.py</code> <pre><code>class CenterBlock(nn.Sequential):\n    def __init__(self, in_channels, out_channels, use_batchnorm=True):\n        conv1 = md.Conv2dReLU(\n            in_channels,\n            out_channels,\n            kernel_size=3,\n            padding=1,\n            use_batchnorm=use_batchnorm,\n        )\n        conv2 = md.Conv2dReLU(\n            out_channels,\n            out_channels,\n            kernel_size=3,\n            padding=1,\n            use_batchnorm=use_batchnorm,\n        )\n        super().__init__(conv1, conv2)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/unetplusplus/decoder/#src.thaw_slump_segmentation.models.unetplusplus.decoder.CenterBlock.__init__","title":"<code>__init__(in_channels, out_channels, use_batchnorm=True)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/unetplusplus/decoder.py</code> <pre><code>def __init__(self, in_channels, out_channels, use_batchnorm=True):\n    conv1 = md.Conv2dReLU(\n        in_channels,\n        out_channels,\n        kernel_size=3,\n        padding=1,\n        use_batchnorm=use_batchnorm,\n    )\n    conv2 = md.Conv2dReLU(\n        out_channels,\n        out_channels,\n        kernel_size=3,\n        padding=1,\n        use_batchnorm=use_batchnorm,\n    )\n    super().__init__(conv1, conv2)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/unetplusplus/decoder/#src.thaw_slump_segmentation.models.unetplusplus.decoder.DecoderBlock","title":"<code>DecoderBlock</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/thaw_slump_segmentation/models/unetplusplus/decoder.py</code> <pre><code>class DecoderBlock(nn.Module):\n    def __init__(\n            self,\n            in_channels,\n            skip_channels,\n            out_channels,\n            use_batchnorm=True,\n            attention_type=None,\n    ):\n        super().__init__()\n        self.conv1 = md.Conv2dReLU(\n            in_channels + skip_channels,\n            out_channels,\n            kernel_size=3,\n            padding=1,\n            use_batchnorm=use_batchnorm,\n        )\n        self.attention1 = md.Attention(attention_type, in_channels=in_channels + skip_channels)\n        self.conv2 = md.Conv2dReLU(\n            out_channels,\n            out_channels,\n            kernel_size=3,\n            padding=1,\n            use_batchnorm=use_batchnorm,\n        )\n        self.attention2 = md.Attention(attention_type, in_channels=out_channels)\n\n    def forward(self, x, skip=None):\n        x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n        if skip is not None:\n            x = torch.cat([x, skip], dim=1)\n            x = self.attention1(x)\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.attention2(x)\n        return x\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/unetplusplus/decoder/#src.thaw_slump_segmentation.models.unetplusplus.decoder.DecoderBlock.attention1","title":"<code>attention1 = md.Attention(attention_type, in_channels=in_channels + skip_channels)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/unetplusplus/decoder/#src.thaw_slump_segmentation.models.unetplusplus.decoder.DecoderBlock.attention2","title":"<code>attention2 = md.Attention(attention_type, in_channels=out_channels)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/unetplusplus/decoder/#src.thaw_slump_segmentation.models.unetplusplus.decoder.DecoderBlock.conv1","title":"<code>conv1 = md.Conv2dReLU(in_channels + skip_channels, out_channels, kernel_size=3, padding=1, use_batchnorm=use_batchnorm)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/unetplusplus/decoder/#src.thaw_slump_segmentation.models.unetplusplus.decoder.DecoderBlock.conv2","title":"<code>conv2 = md.Conv2dReLU(out_channels, out_channels, kernel_size=3, padding=1, use_batchnorm=use_batchnorm)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/unetplusplus/decoder/#src.thaw_slump_segmentation.models.unetplusplus.decoder.DecoderBlock.__init__","title":"<code>__init__(in_channels, skip_channels, out_channels, use_batchnorm=True, attention_type=None)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/unetplusplus/decoder.py</code> <pre><code>def __init__(\n        self,\n        in_channels,\n        skip_channels,\n        out_channels,\n        use_batchnorm=True,\n        attention_type=None,\n):\n    super().__init__()\n    self.conv1 = md.Conv2dReLU(\n        in_channels + skip_channels,\n        out_channels,\n        kernel_size=3,\n        padding=1,\n        use_batchnorm=use_batchnorm,\n    )\n    self.attention1 = md.Attention(attention_type, in_channels=in_channels + skip_channels)\n    self.conv2 = md.Conv2dReLU(\n        out_channels,\n        out_channels,\n        kernel_size=3,\n        padding=1,\n        use_batchnorm=use_batchnorm,\n    )\n    self.attention2 = md.Attention(attention_type, in_channels=out_channels)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/unetplusplus/decoder/#src.thaw_slump_segmentation.models.unetplusplus.decoder.DecoderBlock.forward","title":"<code>forward(x, skip=None)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/unetplusplus/decoder.py</code> <pre><code>def forward(self, x, skip=None):\n    x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n    if skip is not None:\n        x = torch.cat([x, skip], dim=1)\n        x = self.attention1(x)\n    x = self.conv1(x)\n    x = self.conv2(x)\n    x = self.attention2(x)\n    return x\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/unetplusplus/decoder/#src.thaw_slump_segmentation.models.unetplusplus.decoder.UnetPlusPlusDecoder","title":"<code>UnetPlusPlusDecoder</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/thaw_slump_segmentation/models/unetplusplus/decoder.py</code> <pre><code>class UnetPlusPlusDecoder(nn.Module):\n    def __init__(\n            self,\n            encoder_channels,\n            decoder_channels,\n            n_blocks=5,\n            use_batchnorm=True,\n            attention_type=None,\n            center=False,\n    ):\n        super().__init__()\n\n        if n_blocks != len(decoder_channels):\n            raise ValueError(\n                \"Model depth is {}, but you provide `decoder_channels` for {} blocks.\".format(\n                    n_blocks, len(decoder_channels)\n                )\n            )\n\n        encoder_channels = encoder_channels[1:]  # remove first skip with same spatial resolution\n        encoder_channels = encoder_channels[::-1]  # reverse channels to start from head of encoder\n        # computing blocks input and output channels\n        head_channels = encoder_channels[0]\n        self.in_channels = [head_channels] + list(decoder_channels[:-1])\n        self.skip_channels = list(encoder_channels[1:]) + [0]\n        self.out_channels = decoder_channels\n        if center:\n            self.center = CenterBlock(\n                head_channels, head_channels, use_batchnorm=use_batchnorm\n            )\n        else:\n            self.center = nn.Identity()\n\n        # combine decoder keyword arguments\n        kwargs = dict(use_batchnorm=use_batchnorm, attention_type=attention_type)\n\n        blocks = {}\n        for layer_idx in range(len(self.in_channels) - 1):\n            for depth_idx in range(layer_idx+1):\n                if depth_idx == 0:\n                    in_ch = self.in_channels[layer_idx]\n                    skip_ch = self.skip_channels[layer_idx] * (layer_idx+1)\n                    out_ch = self.out_channels[layer_idx]\n                else:\n                    out_ch = self.skip_channels[layer_idx]\n                    skip_ch = self.skip_channels[layer_idx] * (layer_idx+1-depth_idx)\n                    in_ch = self.skip_channels[layer_idx - 1]\n                blocks[f'x_{depth_idx}_{layer_idx}'] = DecoderBlock(in_ch, skip_ch, out_ch, **kwargs)\n        blocks[f'x_{0}_{len(self.in_channels)-1}'] =\\\n            DecoderBlock(self.in_channels[-1], 0, self.out_channels[-1], **kwargs)\n        self.blocks = nn.ModuleDict(blocks)\n        self.depth = len(self.in_channels) - 1\n\n    def forward(self, *features):\n\n        features = features[1:]    # remove first skip with same spatial resolution\n        features = features[::-1]  # reverse channels to start from head of encoder\n        # start building dense connections\n        dense_x = {}\n        for layer_idx in range(len(self.in_channels)-1):\n            for depth_idx in range(self.depth-layer_idx):\n                if layer_idx == 0:\n                    output = self.blocks[f'x_{depth_idx}_{depth_idx}'](features[depth_idx], features[depth_idx+1])\n                    dense_x[f'x_{depth_idx}_{depth_idx}'] = output\n                else:\n                    dense_l_i = depth_idx + layer_idx\n                    cat_features = [dense_x[f'x_{idx}_{dense_l_i}'] for idx in range(depth_idx+1, dense_l_i+1)]\n                    cat_features = torch.cat(cat_features + [features[dense_l_i+1]], dim=1)\n                    dense_x[f'x_{depth_idx}_{dense_l_i}'] =\\\n                        self.blocks[f'x_{depth_idx}_{dense_l_i}'](dense_x[f'x_{depth_idx}_{dense_l_i-1}'], cat_features)\n        dense_x[f'x_{0}_{self.depth}'] = self.blocks[f'x_{0}_{self.depth}'](dense_x[f'x_{0}_{self.depth-1}'])\n        return dense_x[f'x_{0}_{self.depth}']\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/unetplusplus/decoder/#src.thaw_slump_segmentation.models.unetplusplus.decoder.UnetPlusPlusDecoder.blocks","title":"<code>blocks = nn.ModuleDict(blocks)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/unetplusplus/decoder/#src.thaw_slump_segmentation.models.unetplusplus.decoder.UnetPlusPlusDecoder.center","title":"<code>center = CenterBlock(head_channels, head_channels, use_batchnorm=use_batchnorm)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/unetplusplus/decoder/#src.thaw_slump_segmentation.models.unetplusplus.decoder.UnetPlusPlusDecoder.depth","title":"<code>depth = len(self.in_channels) - 1</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/unetplusplus/decoder/#src.thaw_slump_segmentation.models.unetplusplus.decoder.UnetPlusPlusDecoder.in_channels","title":"<code>in_channels = [head_channels] + list(decoder_channels[:-1])</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/unetplusplus/decoder/#src.thaw_slump_segmentation.models.unetplusplus.decoder.UnetPlusPlusDecoder.out_channels","title":"<code>out_channels = decoder_channels</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/unetplusplus/decoder/#src.thaw_slump_segmentation.models.unetplusplus.decoder.UnetPlusPlusDecoder.skip_channels","title":"<code>skip_channels = list(encoder_channels[1:]) + [0]</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/unetplusplus/decoder/#src.thaw_slump_segmentation.models.unetplusplus.decoder.UnetPlusPlusDecoder.__init__","title":"<code>__init__(encoder_channels, decoder_channels, n_blocks=5, use_batchnorm=True, attention_type=None, center=False)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/unetplusplus/decoder.py</code> <pre><code>def __init__(\n        self,\n        encoder_channels,\n        decoder_channels,\n        n_blocks=5,\n        use_batchnorm=True,\n        attention_type=None,\n        center=False,\n):\n    super().__init__()\n\n    if n_blocks != len(decoder_channels):\n        raise ValueError(\n            \"Model depth is {}, but you provide `decoder_channels` for {} blocks.\".format(\n                n_blocks, len(decoder_channels)\n            )\n        )\n\n    encoder_channels = encoder_channels[1:]  # remove first skip with same spatial resolution\n    encoder_channels = encoder_channels[::-1]  # reverse channels to start from head of encoder\n    # computing blocks input and output channels\n    head_channels = encoder_channels[0]\n    self.in_channels = [head_channels] + list(decoder_channels[:-1])\n    self.skip_channels = list(encoder_channels[1:]) + [0]\n    self.out_channels = decoder_channels\n    if center:\n        self.center = CenterBlock(\n            head_channels, head_channels, use_batchnorm=use_batchnorm\n        )\n    else:\n        self.center = nn.Identity()\n\n    # combine decoder keyword arguments\n    kwargs = dict(use_batchnorm=use_batchnorm, attention_type=attention_type)\n\n    blocks = {}\n    for layer_idx in range(len(self.in_channels) - 1):\n        for depth_idx in range(layer_idx+1):\n            if depth_idx == 0:\n                in_ch = self.in_channels[layer_idx]\n                skip_ch = self.skip_channels[layer_idx] * (layer_idx+1)\n                out_ch = self.out_channels[layer_idx]\n            else:\n                out_ch = self.skip_channels[layer_idx]\n                skip_ch = self.skip_channels[layer_idx] * (layer_idx+1-depth_idx)\n                in_ch = self.skip_channels[layer_idx - 1]\n            blocks[f'x_{depth_idx}_{layer_idx}'] = DecoderBlock(in_ch, skip_ch, out_ch, **kwargs)\n    blocks[f'x_{0}_{len(self.in_channels)-1}'] =\\\n        DecoderBlock(self.in_channels[-1], 0, self.out_channels[-1], **kwargs)\n    self.blocks = nn.ModuleDict(blocks)\n    self.depth = len(self.in_channels) - 1\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/unetplusplus/decoder/#src.thaw_slump_segmentation.models.unetplusplus.decoder.UnetPlusPlusDecoder.forward","title":"<code>forward(*features)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/unetplusplus/decoder.py</code> <pre><code>def forward(self, *features):\n\n    features = features[1:]    # remove first skip with same spatial resolution\n    features = features[::-1]  # reverse channels to start from head of encoder\n    # start building dense connections\n    dense_x = {}\n    for layer_idx in range(len(self.in_channels)-1):\n        for depth_idx in range(self.depth-layer_idx):\n            if layer_idx == 0:\n                output = self.blocks[f'x_{depth_idx}_{depth_idx}'](features[depth_idx], features[depth_idx+1])\n                dense_x[f'x_{depth_idx}_{depth_idx}'] = output\n            else:\n                dense_l_i = depth_idx + layer_idx\n                cat_features = [dense_x[f'x_{idx}_{dense_l_i}'] for idx in range(depth_idx+1, dense_l_i+1)]\n                cat_features = torch.cat(cat_features + [features[dense_l_i+1]], dim=1)\n                dense_x[f'x_{depth_idx}_{dense_l_i}'] =\\\n                    self.blocks[f'x_{depth_idx}_{dense_l_i}'](dense_x[f'x_{depth_idx}_{dense_l_i-1}'], cat_features)\n    dense_x[f'x_{0}_{self.depth}'] = self.blocks[f'x_{0}_{self.depth}'](dense_x[f'x_{0}_{self.depth-1}'])\n    return dense_x[f'x_{0}_{self.depth}']\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/unetplusplus/model/","title":"Model","text":""},{"location":"reference/src/thaw_slump_segmentation/models/unetplusplus/model/#src.thaw_slump_segmentation.models.unetplusplus.model.UnetPlusPlus","title":"<code>UnetPlusPlus</code>","text":"<p>               Bases: <code>SegmentationModel</code></p> <p>Unet++ is a fully convolution neural network for image semantic segmentation. Consist of encoder  and decoder parts connected with skip connections. Encoder extract features of different spatial  resolution (skip connections) which are used by decoder to define accurate segmentation mask. Decoder of Unet++ is more complex than in usual Unet.</p> <p>Parameters:</p> Name Type Description Default <code>encoder_name</code> <code>str</code> <p>Name of the classification model that will be used as an encoder (a.k.a backbone) to extract features of different spatial resolution</p> <code>'resnet34'</code> <code>encoder_depth</code> <code>int</code> <p>A number of stages used in encoder in range [3, 5]. Each stage generate features  two times smaller in spatial dimensions than previous one (e.g. for depth 0 we will have features with shapes [(N, C, H, W),], for depth 1 - [(N, C, H, W), (N, C, H // 2, W // 2)] and so on). Default is 5</p> <code>5</code> <code>encoder_weights</code> <code>Optional[str]</code> <p>One of None (random initialization), \"imagenet\" (pre-training on ImageNet) and  other pretrained weights (see table with available weights for each encoder_name)</p> <code>'imagenet'</code> <code>decoder_channels</code> <code>List[int]</code> <p>List of integers which specify in_channels parameter for convolutions used in decoder. Length of the list should be the same as encoder_depth</p> <code>(256, 128, 64, 32, 16)</code> <code>decoder_use_batchnorm</code> <code>bool</code> <p>If True, BatchNorm2d layer between Conv2D and Activation layers is used. If \"inplace\" InplaceABN will be used, allows to decrease memory consumption. Available options are True, False, \"inplace\"</p> <code>True</code> <code>decoder_attention_type</code> <code>Optional[str]</code> <p>Attention module used in decoder of the model. Available options are None and scse. SCSE paper - https://arxiv.org/abs/1808.08127</p> <code>None</code> <code>in_channels</code> <code>int</code> <p>A number of input channels for the model, default is 3 (RGB images)</p> <code>3</code> <code>classes</code> <code>int</code> <p>A number of classes for output mask (or you can think as a number of channels of output mask)</p> <code>1</code> <code>activation</code> <code>Optional[Union[str, callable]]</code> <p>An activation function to apply after the final convolution layer. Available options are \"sigmoid\", \"softmax\", \"logsoftmax\", \"tanh\", \"identity\", callable and None. Default is None</p> <code>None</code> <code>aux_params</code> <code>Optional[dict]</code> <p>Dictionary with parameters of the auxiliary output (classification head). Auxiliary output is build  on top of encoder if aux_params is not None (default). Supported params:     - classes (int): A number of classes     - pooling (str): One of \"max\", \"avg\". Default is \"avg\"     - dropout (float): Dropout factor in [0, 1)     - activation (str): An activation function to apply \"sigmoid\"/\"softmax\" (could be None to return logits)</p> <code>None</code> <p>Returns:</p> Type Description <p><code>torch.nn.Module</code>: Unet++</p> Reference <p>https://arxiv.org/abs/1807.10165</p> Source code in <code>src/thaw_slump_segmentation/models/unetplusplus/model.py</code> <pre><code>class UnetPlusPlus(SegmentationModel):\n    \"\"\"Unet++ is a fully convolution neural network for image semantic segmentation. Consist of *encoder* \n    and *decoder* parts connected with *skip connections*. Encoder extract features of different spatial \n    resolution (skip connections) which are used by decoder to define accurate segmentation mask. Decoder of\n    Unet++ is more complex than in usual Unet.\n\n    Args:\n        encoder_name: Name of the classification model that will be used as an encoder (a.k.a backbone)\n            to extract features of different spatial resolution\n        encoder_depth: A number of stages used in encoder in range [3, 5]. Each stage generate features \n            two times smaller in spatial dimensions than previous one (e.g. for depth 0 we will have features\n            with shapes [(N, C, H, W),], for depth 1 - [(N, C, H, W), (N, C, H // 2, W // 2)] and so on).\n            Default is 5\n        encoder_weights: One of **None** (random initialization), **\"imagenet\"** (pre-training on ImageNet) and \n            other pretrained weights (see table with available weights for each encoder_name)\n        decoder_channels: List of integers which specify **in_channels** parameter for convolutions used in decoder.\n            Length of the list should be the same as **encoder_depth**\n        decoder_use_batchnorm: If **True**, BatchNorm2d layer between Conv2D and Activation layers\n            is used. If **\"inplace\"** InplaceABN will be used, allows to decrease memory consumption.\n            Available options are **True, False, \"inplace\"**\n        decoder_attention_type: Attention module used in decoder of the model. Available options are **None** and **scse**.\n            SCSE paper - https://arxiv.org/abs/1808.08127\n        in_channels: A number of input channels for the model, default is 3 (RGB images)\n        classes: A number of classes for output mask (or you can think as a number of channels of output mask)\n        activation: An activation function to apply after the final convolution layer.\n            Available options are **\"sigmoid\"**, **\"softmax\"**, **\"logsoftmax\"**, **\"tanh\"**, **\"identity\"**, **callable** and **None**.\n            Default is **None**\n        aux_params: Dictionary with parameters of the auxiliary output (classification head). Auxiliary output is build \n            on top of encoder if **aux_params** is not **None** (default). Supported params:\n                - classes (int): A number of classes\n                - pooling (str): One of \"max\", \"avg\". Default is \"avg\"\n                - dropout (float): Dropout factor in [0, 1)\n                - activation (str): An activation function to apply \"sigmoid\"/\"softmax\" (could be **None** to return logits)\n\n    Returns:\n        ``torch.nn.Module``: **Unet++**\n\n    Reference:\n        https://arxiv.org/abs/1807.10165\n\n    \"\"\"\n\n    def __init__(\n        self,\n        encoder_name: str = \"resnet34\",\n        encoder_depth: int = 5,\n        encoder_weights: Optional[str] = \"imagenet\",\n        decoder_use_batchnorm: bool = True,\n        decoder_channels: List[int] = (256, 128, 64, 32, 16),\n        decoder_attention_type: Optional[str] = None,\n        in_channels: int = 3,\n        classes: int = 1,\n        activation: Optional[Union[str, callable]] = None,\n        aux_params: Optional[dict] = None,\n    ):\n        super().__init__()\n\n        self.encoder = get_encoder(\n            encoder_name,\n            in_channels=in_channels,\n            depth=encoder_depth,\n            weights=encoder_weights,\n        )\n\n        self.decoder = UnetPlusPlusDecoder(\n            encoder_channels=self.encoder.out_channels,\n            decoder_channels=decoder_channels,\n            n_blocks=encoder_depth,\n            use_batchnorm=decoder_use_batchnorm,\n            center=True if encoder_name.startswith(\"vgg\") else False,\n            attention_type=decoder_attention_type,\n        )\n\n        self.segmentation_head = SegmentationHead(\n            in_channels=decoder_channels[-1],\n            out_channels=classes,\n            activation=activation,\n            kernel_size=3,\n        )\n\n        if aux_params is not None:\n            self.classification_head = ClassificationHead(\n                in_channels=self.encoder.out_channels[-1], **aux_params\n            )\n        else:\n            self.classification_head = None\n\n        self.name = \"unetplusplus-{}\".format(encoder_name)\n        self.initialize()\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/unetplusplus/model/#src.thaw_slump_segmentation.models.unetplusplus.model.UnetPlusPlus.classification_head","title":"<code>classification_head = ClassificationHead(in_channels=self.encoder.out_channels[-1], **aux_params)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/unetplusplus/model/#src.thaw_slump_segmentation.models.unetplusplus.model.UnetPlusPlus.decoder","title":"<code>decoder = UnetPlusPlusDecoder(encoder_channels=self.encoder.out_channels, decoder_channels=decoder_channels, n_blocks=encoder_depth, use_batchnorm=decoder_use_batchnorm, center=True if encoder_name.startswith('vgg') else False, attention_type=decoder_attention_type)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/unetplusplus/model/#src.thaw_slump_segmentation.models.unetplusplus.model.UnetPlusPlus.encoder","title":"<code>encoder = get_encoder(encoder_name, in_channels=in_channels, depth=encoder_depth, weights=encoder_weights)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/unetplusplus/model/#src.thaw_slump_segmentation.models.unetplusplus.model.UnetPlusPlus.name","title":"<code>name = 'unetplusplus-{}'.format(encoder_name)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/unetplusplus/model/#src.thaw_slump_segmentation.models.unetplusplus.model.UnetPlusPlus.segmentation_head","title":"<code>segmentation_head = SegmentationHead(in_channels=decoder_channels[-1], out_channels=classes, activation=activation, kernel_size=3)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/unetplusplus/model/#src.thaw_slump_segmentation.models.unetplusplus.model.UnetPlusPlus.__init__","title":"<code>__init__(encoder_name='resnet34', encoder_depth=5, encoder_weights='imagenet', decoder_use_batchnorm=True, decoder_channels=(256, 128, 64, 32, 16), decoder_attention_type=None, in_channels=3, classes=1, activation=None, aux_params=None)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/unetplusplus/model.py</code> <pre><code>def __init__(\n    self,\n    encoder_name: str = \"resnet34\",\n    encoder_depth: int = 5,\n    encoder_weights: Optional[str] = \"imagenet\",\n    decoder_use_batchnorm: bool = True,\n    decoder_channels: List[int] = (256, 128, 64, 32, 16),\n    decoder_attention_type: Optional[str] = None,\n    in_channels: int = 3,\n    classes: int = 1,\n    activation: Optional[Union[str, callable]] = None,\n    aux_params: Optional[dict] = None,\n):\n    super().__init__()\n\n    self.encoder = get_encoder(\n        encoder_name,\n        in_channels=in_channels,\n        depth=encoder_depth,\n        weights=encoder_weights,\n    )\n\n    self.decoder = UnetPlusPlusDecoder(\n        encoder_channels=self.encoder.out_channels,\n        decoder_channels=decoder_channels,\n        n_blocks=encoder_depth,\n        use_batchnorm=decoder_use_batchnorm,\n        center=True if encoder_name.startswith(\"vgg\") else False,\n        attention_type=decoder_attention_type,\n    )\n\n    self.segmentation_head = SegmentationHead(\n        in_channels=decoder_channels[-1],\n        out_channels=classes,\n        activation=activation,\n        kernel_size=3,\n    )\n\n    if aux_params is not None:\n        self.classification_head = ClassificationHead(\n            in_channels=self.encoder.out_channels[-1], **aux_params\n        )\n    else:\n        self.classification_head = None\n\n    self.name = \"unetplusplus-{}\".format(encoder_name)\n    self.initialize()\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/base/","title":"Base","text":""},{"location":"reference/src/thaw_slump_segmentation/models/utils/base/#src.thaw_slump_segmentation.models.utils.base.BaseObject","title":"<code>BaseObject</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/thaw_slump_segmentation/models/utils/base.py</code> <pre><code>class BaseObject(nn.Module):\n\n    def __init__(self, name=None):\n        super().__init__()\n        self._name = name\n\n    @property\n    def __name__(self):\n        if self._name is None:\n            name = self.__class__.__name__\n            s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n            return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()\n        else:\n            return self._name\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/base/#src.thaw_slump_segmentation.models.utils.base.BaseObject.__name__","title":"<code>__name__</code>  <code>property</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/utils/base/#src.thaw_slump_segmentation.models.utils.base.BaseObject.__init__","title":"<code>__init__(name=None)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/utils/base.py</code> <pre><code>def __init__(self, name=None):\n    super().__init__()\n    self._name = name\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/base/#src.thaw_slump_segmentation.models.utils.base.Loss","title":"<code>Loss</code>","text":"<p>               Bases: <code>BaseObject</code></p> Source code in <code>src/thaw_slump_segmentation/models/utils/base.py</code> <pre><code>class Loss(BaseObject):\n\n    def __add__(self, other):\n        if isinstance(other, Loss):\n            return SumOfLosses(self, other)\n        else:\n            raise ValueError('Loss should be inherited from `Loss` class')\n\n    def __radd__(self, other):\n        return self.__add__(other)\n\n    def __mul__(self, value):\n        if isinstance(value, (int, float)):\n            return MultipliedLoss(self, value)\n        else:\n            raise ValueError('Loss should be inherited from `BaseLoss` class')\n\n    def __rmul__(self, other):\n        return self.__mul__(other)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/base/#src.thaw_slump_segmentation.models.utils.base.Loss.__add__","title":"<code>__add__(other)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/utils/base.py</code> <pre><code>def __add__(self, other):\n    if isinstance(other, Loss):\n        return SumOfLosses(self, other)\n    else:\n        raise ValueError('Loss should be inherited from `Loss` class')\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/base/#src.thaw_slump_segmentation.models.utils.base.Loss.__mul__","title":"<code>__mul__(value)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/utils/base.py</code> <pre><code>def __mul__(self, value):\n    if isinstance(value, (int, float)):\n        return MultipliedLoss(self, value)\n    else:\n        raise ValueError('Loss should be inherited from `BaseLoss` class')\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/base/#src.thaw_slump_segmentation.models.utils.base.Loss.__radd__","title":"<code>__radd__(other)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/utils/base.py</code> <pre><code>def __radd__(self, other):\n    return self.__add__(other)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/base/#src.thaw_slump_segmentation.models.utils.base.Loss.__rmul__","title":"<code>__rmul__(other)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/utils/base.py</code> <pre><code>def __rmul__(self, other):\n    return self.__mul__(other)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/base/#src.thaw_slump_segmentation.models.utils.base.Metric","title":"<code>Metric</code>","text":"<p>               Bases: <code>BaseObject</code></p> Source code in <code>src/thaw_slump_segmentation/models/utils/base.py</code> <pre><code>class Metric(BaseObject):\n    pass\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/base/#src.thaw_slump_segmentation.models.utils.base.MultipliedLoss","title":"<code>MultipliedLoss</code>","text":"<p>               Bases: <code>Loss</code></p> Source code in <code>src/thaw_slump_segmentation/models/utils/base.py</code> <pre><code>class MultipliedLoss(Loss):\n\n    def __init__(self, loss, multiplier):\n\n        # resolve name\n        if len(loss.__name__.split('+')) &gt; 1:\n            name = '{} * ({})'.format(multiplier, loss.__name__)\n        else:\n            name = '{} * {}'.format(multiplier, loss.__name__)\n        super().__init__(name=name)\n        self.loss = loss\n        self.multiplier = multiplier\n\n    def __call__(self, *inputs):\n        return self.multiplier * self.loss.forward(*inputs)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/base/#src.thaw_slump_segmentation.models.utils.base.MultipliedLoss.loss","title":"<code>loss = loss</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/utils/base/#src.thaw_slump_segmentation.models.utils.base.MultipliedLoss.multiplier","title":"<code>multiplier = multiplier</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/utils/base/#src.thaw_slump_segmentation.models.utils.base.MultipliedLoss.__call__","title":"<code>__call__(*inputs)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/utils/base.py</code> <pre><code>def __call__(self, *inputs):\n    return self.multiplier * self.loss.forward(*inputs)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/base/#src.thaw_slump_segmentation.models.utils.base.MultipliedLoss.__init__","title":"<code>__init__(loss, multiplier)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/utils/base.py</code> <pre><code>def __init__(self, loss, multiplier):\n\n    # resolve name\n    if len(loss.__name__.split('+')) &gt; 1:\n        name = '{} * ({})'.format(multiplier, loss.__name__)\n    else:\n        name = '{} * {}'.format(multiplier, loss.__name__)\n    super().__init__(name=name)\n    self.loss = loss\n    self.multiplier = multiplier\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/base/#src.thaw_slump_segmentation.models.utils.base.SumOfLosses","title":"<code>SumOfLosses</code>","text":"<p>               Bases: <code>Loss</code></p> Source code in <code>src/thaw_slump_segmentation/models/utils/base.py</code> <pre><code>class SumOfLosses(Loss):\n\n    def __init__(self, l1, l2):\n        name = '{} + {}'.format(l1.__name__, l2.__name__)\n        super().__init__(name=name)\n        self.l1 = l1\n        self.l2 = l2\n\n    def __call__(self, *inputs):\n        return self.l1.forward(*inputs) + self.l2.forward(*inputs)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/base/#src.thaw_slump_segmentation.models.utils.base.SumOfLosses.l1","title":"<code>l1 = l1</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/utils/base/#src.thaw_slump_segmentation.models.utils.base.SumOfLosses.l2","title":"<code>l2 = l2</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/utils/base/#src.thaw_slump_segmentation.models.utils.base.SumOfLosses.__call__","title":"<code>__call__(*inputs)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/utils/base.py</code> <pre><code>def __call__(self, *inputs):\n    return self.l1.forward(*inputs) + self.l2.forward(*inputs)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/base/#src.thaw_slump_segmentation.models.utils.base.SumOfLosses.__init__","title":"<code>__init__(l1, l2)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/utils/base.py</code> <pre><code>def __init__(self, l1, l2):\n    name = '{} + {}'.format(l1.__name__, l2.__name__)\n    super().__init__(name=name)\n    self.l1 = l1\n    self.l2 = l2\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/functional/","title":"Functional","text":""},{"location":"reference/src/thaw_slump_segmentation/models/utils/functional/#src.thaw_slump_segmentation.models.utils.functional.jaccard","title":"<code>jaccard = iou</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/utils/functional/#src.thaw_slump_segmentation.models.utils.functional.accuracy","title":"<code>accuracy(pr, gt, threshold=0.5, ignore_channels=None)</code>","text":"<p>Calculate accuracy score between ground truth and prediction Args:     pr (torch.Tensor): predicted tensor     gt (torch.Tensor):  ground truth tensor     eps (float): epsilon to avoid zero division     threshold: threshold for outputs binarization Returns:     float: precision score</p> Source code in <code>src/thaw_slump_segmentation/models/utils/functional.py</code> <pre><code>def accuracy(pr, gt, threshold=0.5, ignore_channels=None):\n    \"\"\"Calculate accuracy score between ground truth and prediction\n    Args:\n        pr (torch.Tensor): predicted tensor\n        gt (torch.Tensor):  ground truth tensor\n        eps (float): epsilon to avoid zero division\n        threshold: threshold for outputs binarization\n    Returns:\n        float: precision score\n    \"\"\"\n    pr = _threshold(pr, threshold=threshold)\n    pr, gt = _take_channels(pr, gt, ignore_channels=ignore_channels)\n\n    tp = torch.sum(gt == pr, dtype=pr.dtype)\n    score = tp / gt.view(-1).shape[0]\n    return score\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/functional/#src.thaw_slump_segmentation.models.utils.functional.f_score","title":"<code>f_score(pr, gt, beta=1, eps=1e-07, threshold=None, ignore_channels=None)</code>","text":"<p>Calculate F-score between ground truth and prediction Args:     pr (torch.Tensor): predicted tensor     gt (torch.Tensor):  ground truth tensor     beta (float): positive constant     eps (float): epsilon to avoid zero division     threshold: threshold for outputs binarization Returns:     float: F score</p> Source code in <code>src/thaw_slump_segmentation/models/utils/functional.py</code> <pre><code>def f_score(pr, gt, beta=1, eps=1e-7, threshold=None, ignore_channels=None):\n    \"\"\"Calculate F-score between ground truth and prediction\n    Args:\n        pr (torch.Tensor): predicted tensor\n        gt (torch.Tensor):  ground truth tensor\n        beta (float): positive constant\n        eps (float): epsilon to avoid zero division\n        threshold: threshold for outputs binarization\n    Returns:\n        float: F score\n    \"\"\"\n\n    pr = _threshold(pr, threshold=threshold)\n    pr, gt = _take_channels(pr, gt, ignore_channels=ignore_channels)\n\n    tp = torch.sum(gt * pr)\n    fp = torch.sum(pr) - tp\n    fn = torch.sum(gt) - tp\n\n    score = ((1 + beta ** 2) * tp + eps) \\\n            / ((1 + beta ** 2) * tp + beta ** 2 * fn + fp + eps)\n\n    return score\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/functional/#src.thaw_slump_segmentation.models.utils.functional.iou","title":"<code>iou(pr, gt, eps=1e-07, threshold=None, ignore_channels=None)</code>","text":"<p>Calculate Intersection over Union between ground truth and prediction Args:     pr (torch.Tensor): predicted tensor     gt (torch.Tensor):  ground truth tensor     eps (float): epsilon to avoid zero division     threshold: threshold for outputs binarization Returns:     float: IoU (Jaccard) score</p> Source code in <code>src/thaw_slump_segmentation/models/utils/functional.py</code> <pre><code>def iou(pr, gt, eps=1e-7, threshold=None, ignore_channels=None):\n    \"\"\"Calculate Intersection over Union between ground truth and prediction\n    Args:\n        pr (torch.Tensor): predicted tensor\n        gt (torch.Tensor):  ground truth tensor\n        eps (float): epsilon to avoid zero division\n        threshold: threshold for outputs binarization\n    Returns:\n        float: IoU (Jaccard) score\n    \"\"\"\n\n    pr = _threshold(pr, threshold=threshold)\n    pr, gt = _take_channels(pr, gt, ignore_channels=ignore_channels)\n\n    intersection = torch.sum(gt * pr)\n    union = torch.sum(gt) + torch.sum(pr) - intersection + eps\n    return (intersection + eps) / union\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/functional/#src.thaw_slump_segmentation.models.utils.functional.precision","title":"<code>precision(pr, gt, eps=1e-07, threshold=None, ignore_channels=None)</code>","text":"<p>Calculate precision score between ground truth and prediction Args:     pr (torch.Tensor): predicted tensor     gt (torch.Tensor):  ground truth tensor     eps (float): epsilon to avoid zero division     threshold: threshold for outputs binarization Returns:     float: precision score</p> Source code in <code>src/thaw_slump_segmentation/models/utils/functional.py</code> <pre><code>def precision(pr, gt, eps=1e-7, threshold=None, ignore_channels=None):\n    \"\"\"Calculate precision score between ground truth and prediction\n    Args:\n        pr (torch.Tensor): predicted tensor\n        gt (torch.Tensor):  ground truth tensor\n        eps (float): epsilon to avoid zero division\n        threshold: threshold for outputs binarization\n    Returns:\n        float: precision score\n    \"\"\"\n\n    pr = _threshold(pr, threshold=threshold)\n    pr, gt = _take_channels(pr, gt, ignore_channels=ignore_channels)\n\n    tp = torch.sum(gt * pr)\n    fp = torch.sum(pr) - tp\n\n    score = (tp + eps) / (tp + fp + eps)\n\n    return score\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/functional/#src.thaw_slump_segmentation.models.utils.functional.recall","title":"<code>recall(pr, gt, eps=1e-07, threshold=None, ignore_channels=None)</code>","text":"<p>Calculate Recall between ground truth and prediction Args:     pr (torch.Tensor): A list of predicted elements     gt (torch.Tensor):  A list of elements that are to be predicted     eps (float): epsilon to avoid zero division     threshold: threshold for outputs binarization Returns:     float: recall score</p> Source code in <code>src/thaw_slump_segmentation/models/utils/functional.py</code> <pre><code>def recall(pr, gt, eps=1e-7, threshold=None, ignore_channels=None):\n    \"\"\"Calculate Recall between ground truth and prediction\n    Args:\n        pr (torch.Tensor): A list of predicted elements\n        gt (torch.Tensor):  A list of elements that are to be predicted\n        eps (float): epsilon to avoid zero division\n        threshold: threshold for outputs binarization\n    Returns:\n        float: recall score\n    \"\"\"\n\n    pr = _threshold(pr, threshold=threshold)\n    pr, gt = _take_channels(pr, gt, ignore_channels=ignore_channels)\n\n    tp = torch.sum(gt * pr)\n    fn = torch.sum(gt) - tp\n\n    score = (tp + eps) / (tp + fn + eps)\n\n    return score\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/losses/","title":"Losses","text":""},{"location":"reference/src/thaw_slump_segmentation/models/utils/losses/#src.thaw_slump_segmentation.models.utils.losses.BCELoss","title":"<code>BCELoss</code>","text":"<p>               Bases: <code>BCELoss</code>, <code>Loss</code></p> Source code in <code>src/thaw_slump_segmentation/models/utils/losses.py</code> <pre><code>class BCELoss(nn.BCELoss, base.Loss):\n    pass\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/losses/#src.thaw_slump_segmentation.models.utils.losses.BCEWithLogitsLoss","title":"<code>BCEWithLogitsLoss</code>","text":"<p>               Bases: <code>BCEWithLogitsLoss</code>, <code>Loss</code></p> Source code in <code>src/thaw_slump_segmentation/models/utils/losses.py</code> <pre><code>class BCEWithLogitsLoss(nn.BCEWithLogitsLoss, base.Loss):\n    pass\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/losses/#src.thaw_slump_segmentation.models.utils.losses.CrossEntropyLoss","title":"<code>CrossEntropyLoss</code>","text":"<p>               Bases: <code>CrossEntropyLoss</code>, <code>Loss</code></p> Source code in <code>src/thaw_slump_segmentation/models/utils/losses.py</code> <pre><code>class CrossEntropyLoss(nn.CrossEntropyLoss, base.Loss):\n    pass\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/losses/#src.thaw_slump_segmentation.models.utils.losses.DiceLoss","title":"<code>DiceLoss</code>","text":"<p>               Bases: <code>Loss</code></p> Source code in <code>src/thaw_slump_segmentation/models/utils/losses.py</code> <pre><code>class DiceLoss(base.Loss):\n    def __init__(self, eps=1., beta=1., activation=None, ignore_channels=None, **kwargs):\n        super().__init__(**kwargs)\n        self.eps = eps\n        self.beta = beta\n        self.activation = Activation(activation)\n        self.ignore_channels = ignore_channels\n\n    def forward(self, y_pr, y_gt):\n        y_pr = self.activation(y_pr)\n        return 1 - F.f_score(\n            y_pr, y_gt,\n            beta=self.beta,\n            eps=self.eps,\n            threshold=None,\n            ignore_channels=self.ignore_channels,\n        )\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/losses/#src.thaw_slump_segmentation.models.utils.losses.DiceLoss.activation","title":"<code>activation = Activation(activation)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/utils/losses/#src.thaw_slump_segmentation.models.utils.losses.DiceLoss.beta","title":"<code>beta = beta</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/utils/losses/#src.thaw_slump_segmentation.models.utils.losses.DiceLoss.eps","title":"<code>eps = eps</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/utils/losses/#src.thaw_slump_segmentation.models.utils.losses.DiceLoss.ignore_channels","title":"<code>ignore_channels = ignore_channels</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/utils/losses/#src.thaw_slump_segmentation.models.utils.losses.DiceLoss.__init__","title":"<code>__init__(eps=1.0, beta=1.0, activation=None, ignore_channels=None, **kwargs)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/utils/losses.py</code> <pre><code>def __init__(self, eps=1., beta=1., activation=None, ignore_channels=None, **kwargs):\n    super().__init__(**kwargs)\n    self.eps = eps\n    self.beta = beta\n    self.activation = Activation(activation)\n    self.ignore_channels = ignore_channels\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/losses/#src.thaw_slump_segmentation.models.utils.losses.DiceLoss.forward","title":"<code>forward(y_pr, y_gt)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/utils/losses.py</code> <pre><code>def forward(self, y_pr, y_gt):\n    y_pr = self.activation(y_pr)\n    return 1 - F.f_score(\n        y_pr, y_gt,\n        beta=self.beta,\n        eps=self.eps,\n        threshold=None,\n        ignore_channels=self.ignore_channels,\n    )\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/losses/#src.thaw_slump_segmentation.models.utils.losses.JaccardLoss","title":"<code>JaccardLoss</code>","text":"<p>               Bases: <code>Loss</code></p> Source code in <code>src/thaw_slump_segmentation/models/utils/losses.py</code> <pre><code>class JaccardLoss(base.Loss):\n    def __init__(self, eps=1., activation=None, ignore_channels=None, **kwargs):\n        super().__init__(**kwargs)\n        self.eps = eps\n        self.activation = Activation(activation)\n        self.ignore_channels = ignore_channels\n\n    def forward(self, y_pr, y_gt):\n        y_pr = self.activation(y_pr)\n        return 1 - F.jaccard(\n            y_pr, y_gt,\n            eps=self.eps,\n            threshold=None,\n            ignore_channels=self.ignore_channels,\n        )\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/losses/#src.thaw_slump_segmentation.models.utils.losses.JaccardLoss.activation","title":"<code>activation = Activation(activation)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/utils/losses/#src.thaw_slump_segmentation.models.utils.losses.JaccardLoss.eps","title":"<code>eps = eps</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/utils/losses/#src.thaw_slump_segmentation.models.utils.losses.JaccardLoss.ignore_channels","title":"<code>ignore_channels = ignore_channels</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/utils/losses/#src.thaw_slump_segmentation.models.utils.losses.JaccardLoss.__init__","title":"<code>__init__(eps=1.0, activation=None, ignore_channels=None, **kwargs)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/utils/losses.py</code> <pre><code>def __init__(self, eps=1., activation=None, ignore_channels=None, **kwargs):\n    super().__init__(**kwargs)\n    self.eps = eps\n    self.activation = Activation(activation)\n    self.ignore_channels = ignore_channels\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/losses/#src.thaw_slump_segmentation.models.utils.losses.JaccardLoss.forward","title":"<code>forward(y_pr, y_gt)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/utils/losses.py</code> <pre><code>def forward(self, y_pr, y_gt):\n    y_pr = self.activation(y_pr)\n    return 1 - F.jaccard(\n        y_pr, y_gt,\n        eps=self.eps,\n        threshold=None,\n        ignore_channels=self.ignore_channels,\n    )\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/losses/#src.thaw_slump_segmentation.models.utils.losses.L1Loss","title":"<code>L1Loss</code>","text":"<p>               Bases: <code>L1Loss</code>, <code>Loss</code></p> Source code in <code>src/thaw_slump_segmentation/models/utils/losses.py</code> <pre><code>class L1Loss(nn.L1Loss, base.Loss):\n    pass\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/losses/#src.thaw_slump_segmentation.models.utils.losses.MSELoss","title":"<code>MSELoss</code>","text":"<p>               Bases: <code>MSELoss</code>, <code>Loss</code></p> Source code in <code>src/thaw_slump_segmentation/models/utils/losses.py</code> <pre><code>class MSELoss(nn.MSELoss, base.Loss):\n    pass\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/losses/#src.thaw_slump_segmentation.models.utils.losses.NLLLoss","title":"<code>NLLLoss</code>","text":"<p>               Bases: <code>NLLLoss</code>, <code>Loss</code></p> Source code in <code>src/thaw_slump_segmentation/models/utils/losses.py</code> <pre><code>class NLLLoss(nn.NLLLoss, base.Loss):\n    pass\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/meter/","title":"Meter","text":""},{"location":"reference/src/thaw_slump_segmentation/models/utils/meter/#src.thaw_slump_segmentation.models.utils.meter.AverageValueMeter","title":"<code>AverageValueMeter</code>","text":"<p>               Bases: <code>Meter</code></p> Source code in <code>src/thaw_slump_segmentation/models/utils/meter.py</code> <pre><code>class AverageValueMeter(Meter):\n    def __init__(self):\n        super(AverageValueMeter, self).__init__()\n        self.reset()\n        self.val = 0\n\n    def add(self, value, n=1):\n        self.val = value\n        self.sum += value\n        self.var += value * value\n        self.n += n\n\n        if self.n == 0:\n            self.mean, self.std = np.nan, np.nan\n        elif self.n == 1:\n            self.mean = 0.0 + self.sum  # This is to force a copy in torch/numpy\n            self.std = np.inf\n            self.mean_old = self.mean\n            self.m_s = 0.0\n        else:\n            self.mean = self.mean_old + (value - n * self.mean_old) / float(self.n)\n            self.m_s += (value - self.mean_old) * (value - self.mean)\n            self.mean_old = self.mean\n            self.std = np.sqrt(self.m_s / (self.n - 1.0))\n\n    def value(self):\n        return self.mean, self.std\n\n    def reset(self):\n        self.n = 0\n        self.sum = 0.0\n        self.var = 0.0\n        self.val = 0.0\n        self.mean = np.nan\n        self.mean_old = 0.0\n        self.m_s = 0.0\n        self.std = np.nan\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/meter/#src.thaw_slump_segmentation.models.utils.meter.AverageValueMeter.val","title":"<code>val = 0</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/utils/meter/#src.thaw_slump_segmentation.models.utils.meter.AverageValueMeter.__init__","title":"<code>__init__()</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/utils/meter.py</code> <pre><code>def __init__(self):\n    super(AverageValueMeter, self).__init__()\n    self.reset()\n    self.val = 0\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/meter/#src.thaw_slump_segmentation.models.utils.meter.AverageValueMeter.add","title":"<code>add(value, n=1)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/utils/meter.py</code> <pre><code>def add(self, value, n=1):\n    self.val = value\n    self.sum += value\n    self.var += value * value\n    self.n += n\n\n    if self.n == 0:\n        self.mean, self.std = np.nan, np.nan\n    elif self.n == 1:\n        self.mean = 0.0 + self.sum  # This is to force a copy in torch/numpy\n        self.std = np.inf\n        self.mean_old = self.mean\n        self.m_s = 0.0\n    else:\n        self.mean = self.mean_old + (value - n * self.mean_old) / float(self.n)\n        self.m_s += (value - self.mean_old) * (value - self.mean)\n        self.mean_old = self.mean\n        self.std = np.sqrt(self.m_s / (self.n - 1.0))\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/meter/#src.thaw_slump_segmentation.models.utils.meter.AverageValueMeter.reset","title":"<code>reset()</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/utils/meter.py</code> <pre><code>def reset(self):\n    self.n = 0\n    self.sum = 0.0\n    self.var = 0.0\n    self.val = 0.0\n    self.mean = np.nan\n    self.mean_old = 0.0\n    self.m_s = 0.0\n    self.std = np.nan\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/meter/#src.thaw_slump_segmentation.models.utils.meter.AverageValueMeter.value","title":"<code>value()</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/utils/meter.py</code> <pre><code>def value(self):\n    return self.mean, self.std\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/meter/#src.thaw_slump_segmentation.models.utils.meter.Meter","title":"<code>Meter</code>","text":"<p>               Bases: <code>object</code></p> <p>Meters provide a way to keep track of important statistics in an online manner. This class is abstract, but provides a standard interface for all meters to follow.</p> Source code in <code>src/thaw_slump_segmentation/models/utils/meter.py</code> <pre><code>class Meter(object):\n    '''Meters provide a way to keep track of important statistics in an online manner.\n    This class is abstract, but provides a standard interface for all meters to follow.\n    '''\n\n    def reset(self):\n        '''Resets the meter to default settings.'''\n        pass\n\n    def add(self, value):\n        '''Log a new value to the meter\n        Args:\n            value: Next result to include.\n        '''\n        pass\n\n    def value(self):\n        '''Get the value of the meter in the current state.'''\n        pass\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/meter/#src.thaw_slump_segmentation.models.utils.meter.Meter.add","title":"<code>add(value)</code>","text":"<p>Log a new value to the meter Args:     value: Next result to include.</p> Source code in <code>src/thaw_slump_segmentation/models/utils/meter.py</code> <pre><code>def add(self, value):\n    '''Log a new value to the meter\n    Args:\n        value: Next result to include.\n    '''\n    pass\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/meter/#src.thaw_slump_segmentation.models.utils.meter.Meter.reset","title":"<code>reset()</code>","text":"<p>Resets the meter to default settings.</p> Source code in <code>src/thaw_slump_segmentation/models/utils/meter.py</code> <pre><code>def reset(self):\n    '''Resets the meter to default settings.'''\n    pass\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/meter/#src.thaw_slump_segmentation.models.utils.meter.Meter.value","title":"<code>value()</code>","text":"<p>Get the value of the meter in the current state.</p> Source code in <code>src/thaw_slump_segmentation/models/utils/meter.py</code> <pre><code>def value(self):\n    '''Get the value of the meter in the current state.'''\n    pass\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/metrics/","title":"Metrics","text":""},{"location":"reference/src/thaw_slump_segmentation/models/utils/metrics/#src.thaw_slump_segmentation.models.utils.metrics.Accuracy","title":"<code>Accuracy</code>","text":"<p>               Bases: <code>Metric</code></p> Source code in <code>src/thaw_slump_segmentation/models/utils/metrics.py</code> <pre><code>class Accuracy(base.Metric):\n\n    def __init__(self, threshold=0.5, activation=None, ignore_channels=None, **kwargs):\n        super().__init__(**kwargs)\n        self.threshold = threshold\n        self.activation = Activation(activation)\n        self.ignore_channels = ignore_channels\n\n    def forward(self, y_pr, y_gt):\n        y_pr = self.activation(y_pr)\n        return F.accuracy(\n            y_pr, y_gt,\n            threshold=self.threshold,\n            ignore_channels=self.ignore_channels,\n        )\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/metrics/#src.thaw_slump_segmentation.models.utils.metrics.Accuracy.activation","title":"<code>activation = Activation(activation)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/utils/metrics/#src.thaw_slump_segmentation.models.utils.metrics.Accuracy.ignore_channels","title":"<code>ignore_channels = ignore_channels</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/utils/metrics/#src.thaw_slump_segmentation.models.utils.metrics.Accuracy.threshold","title":"<code>threshold = threshold</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/utils/metrics/#src.thaw_slump_segmentation.models.utils.metrics.Accuracy.__init__","title":"<code>__init__(threshold=0.5, activation=None, ignore_channels=None, **kwargs)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/utils/metrics.py</code> <pre><code>def __init__(self, threshold=0.5, activation=None, ignore_channels=None, **kwargs):\n    super().__init__(**kwargs)\n    self.threshold = threshold\n    self.activation = Activation(activation)\n    self.ignore_channels = ignore_channels\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/metrics/#src.thaw_slump_segmentation.models.utils.metrics.Accuracy.forward","title":"<code>forward(y_pr, y_gt)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/utils/metrics.py</code> <pre><code>def forward(self, y_pr, y_gt):\n    y_pr = self.activation(y_pr)\n    return F.accuracy(\n        y_pr, y_gt,\n        threshold=self.threshold,\n        ignore_channels=self.ignore_channels,\n    )\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/metrics/#src.thaw_slump_segmentation.models.utils.metrics.Fscore","title":"<code>Fscore</code>","text":"<p>               Bases: <code>Metric</code></p> Source code in <code>src/thaw_slump_segmentation/models/utils/metrics.py</code> <pre><code>class Fscore(base.Metric):\n\n    def __init__(self, beta=1, eps=1e-7, threshold=0.5, activation=None, ignore_channels=None, **kwargs):\n        super().__init__(**kwargs)\n        self.eps = eps\n        self.beta = beta\n        self.threshold = threshold\n        self.activation = Activation(activation)\n        self.ignore_channels = ignore_channels\n\n    def forward(self, y_pr, y_gt):\n        y_pr = self.activation(y_pr)\n        return F.f_score(\n            y_pr, y_gt,\n            eps=self.eps,\n            beta=self.beta,\n            threshold=self.threshold,\n            ignore_channels=self.ignore_channels,\n        )\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/metrics/#src.thaw_slump_segmentation.models.utils.metrics.Fscore.activation","title":"<code>activation = Activation(activation)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/utils/metrics/#src.thaw_slump_segmentation.models.utils.metrics.Fscore.beta","title":"<code>beta = beta</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/utils/metrics/#src.thaw_slump_segmentation.models.utils.metrics.Fscore.eps","title":"<code>eps = eps</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/utils/metrics/#src.thaw_slump_segmentation.models.utils.metrics.Fscore.ignore_channels","title":"<code>ignore_channels = ignore_channels</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/utils/metrics/#src.thaw_slump_segmentation.models.utils.metrics.Fscore.threshold","title":"<code>threshold = threshold</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/utils/metrics/#src.thaw_slump_segmentation.models.utils.metrics.Fscore.__init__","title":"<code>__init__(beta=1, eps=1e-07, threshold=0.5, activation=None, ignore_channels=None, **kwargs)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/utils/metrics.py</code> <pre><code>def __init__(self, beta=1, eps=1e-7, threshold=0.5, activation=None, ignore_channels=None, **kwargs):\n    super().__init__(**kwargs)\n    self.eps = eps\n    self.beta = beta\n    self.threshold = threshold\n    self.activation = Activation(activation)\n    self.ignore_channels = ignore_channels\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/metrics/#src.thaw_slump_segmentation.models.utils.metrics.Fscore.forward","title":"<code>forward(y_pr, y_gt)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/utils/metrics.py</code> <pre><code>def forward(self, y_pr, y_gt):\n    y_pr = self.activation(y_pr)\n    return F.f_score(\n        y_pr, y_gt,\n        eps=self.eps,\n        beta=self.beta,\n        threshold=self.threshold,\n        ignore_channels=self.ignore_channels,\n    )\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/metrics/#src.thaw_slump_segmentation.models.utils.metrics.IoU","title":"<code>IoU</code>","text":"<p>               Bases: <code>Metric</code></p> Source code in <code>src/thaw_slump_segmentation/models/utils/metrics.py</code> <pre><code>class IoU(base.Metric):\n    __name__ = 'iou_score'\n\n    def __init__(self, eps=1e-7, threshold=0.5, activation=None, ignore_channels=None, **kwargs):\n        super().__init__(**kwargs)\n        self.eps = eps\n        self.threshold = threshold\n        self.activation = Activation(activation)\n        self.ignore_channels = ignore_channels\n\n    def forward(self, y_pr, y_gt):\n        y_pr = self.activation(y_pr)\n        return F.iou(\n            y_pr, y_gt,\n            eps=self.eps,\n            threshold=self.threshold,\n            ignore_channels=self.ignore_channels,\n        )\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/metrics/#src.thaw_slump_segmentation.models.utils.metrics.IoU.__name__","title":"<code>__name__ = 'iou_score'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/utils/metrics/#src.thaw_slump_segmentation.models.utils.metrics.IoU.activation","title":"<code>activation = Activation(activation)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/utils/metrics/#src.thaw_slump_segmentation.models.utils.metrics.IoU.eps","title":"<code>eps = eps</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/utils/metrics/#src.thaw_slump_segmentation.models.utils.metrics.IoU.ignore_channels","title":"<code>ignore_channels = ignore_channels</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/utils/metrics/#src.thaw_slump_segmentation.models.utils.metrics.IoU.threshold","title":"<code>threshold = threshold</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/utils/metrics/#src.thaw_slump_segmentation.models.utils.metrics.IoU.__init__","title":"<code>__init__(eps=1e-07, threshold=0.5, activation=None, ignore_channels=None, **kwargs)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/utils/metrics.py</code> <pre><code>def __init__(self, eps=1e-7, threshold=0.5, activation=None, ignore_channels=None, **kwargs):\n    super().__init__(**kwargs)\n    self.eps = eps\n    self.threshold = threshold\n    self.activation = Activation(activation)\n    self.ignore_channels = ignore_channels\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/metrics/#src.thaw_slump_segmentation.models.utils.metrics.IoU.forward","title":"<code>forward(y_pr, y_gt)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/utils/metrics.py</code> <pre><code>def forward(self, y_pr, y_gt):\n    y_pr = self.activation(y_pr)\n    return F.iou(\n        y_pr, y_gt,\n        eps=self.eps,\n        threshold=self.threshold,\n        ignore_channels=self.ignore_channels,\n    )\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/metrics/#src.thaw_slump_segmentation.models.utils.metrics.Precision","title":"<code>Precision</code>","text":"<p>               Bases: <code>Metric</code></p> Source code in <code>src/thaw_slump_segmentation/models/utils/metrics.py</code> <pre><code>class Precision(base.Metric):\n\n    def __init__(self, eps=1e-7, threshold=0.5, activation=None, ignore_channels=None, **kwargs):\n        super().__init__(**kwargs)\n        self.eps = eps\n        self.threshold = threshold\n        self.activation = Activation(activation)\n        self.ignore_channels = ignore_channels\n\n    def forward(self, y_pr, y_gt):\n        y_pr = self.activation(y_pr)\n        return F.precision(\n            y_pr, y_gt,\n            eps=self.eps,\n            threshold=self.threshold,\n            ignore_channels=self.ignore_channels,\n        )\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/metrics/#src.thaw_slump_segmentation.models.utils.metrics.Precision.activation","title":"<code>activation = Activation(activation)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/utils/metrics/#src.thaw_slump_segmentation.models.utils.metrics.Precision.eps","title":"<code>eps = eps</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/utils/metrics/#src.thaw_slump_segmentation.models.utils.metrics.Precision.ignore_channels","title":"<code>ignore_channels = ignore_channels</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/utils/metrics/#src.thaw_slump_segmentation.models.utils.metrics.Precision.threshold","title":"<code>threshold = threshold</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/utils/metrics/#src.thaw_slump_segmentation.models.utils.metrics.Precision.__init__","title":"<code>__init__(eps=1e-07, threshold=0.5, activation=None, ignore_channels=None, **kwargs)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/utils/metrics.py</code> <pre><code>def __init__(self, eps=1e-7, threshold=0.5, activation=None, ignore_channels=None, **kwargs):\n    super().__init__(**kwargs)\n    self.eps = eps\n    self.threshold = threshold\n    self.activation = Activation(activation)\n    self.ignore_channels = ignore_channels\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/metrics/#src.thaw_slump_segmentation.models.utils.metrics.Precision.forward","title":"<code>forward(y_pr, y_gt)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/utils/metrics.py</code> <pre><code>def forward(self, y_pr, y_gt):\n    y_pr = self.activation(y_pr)\n    return F.precision(\n        y_pr, y_gt,\n        eps=self.eps,\n        threshold=self.threshold,\n        ignore_channels=self.ignore_channels,\n    )\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/metrics/#src.thaw_slump_segmentation.models.utils.metrics.Recall","title":"<code>Recall</code>","text":"<p>               Bases: <code>Metric</code></p> Source code in <code>src/thaw_slump_segmentation/models/utils/metrics.py</code> <pre><code>class Recall(base.Metric):\n\n    def __init__(self, eps=1e-7, threshold=0.5, activation=None, ignore_channels=None, **kwargs):\n        super().__init__(**kwargs)\n        self.eps = eps\n        self.threshold = threshold\n        self.activation = Activation(activation)\n        self.ignore_channels = ignore_channels\n\n    def forward(self, y_pr, y_gt):\n        y_pr = self.activation(y_pr)\n        return F.recall(\n            y_pr, y_gt,\n            eps=self.eps,\n            threshold=self.threshold,\n            ignore_channels=self.ignore_channels,\n        )\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/metrics/#src.thaw_slump_segmentation.models.utils.metrics.Recall.activation","title":"<code>activation = Activation(activation)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/utils/metrics/#src.thaw_slump_segmentation.models.utils.metrics.Recall.eps","title":"<code>eps = eps</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/utils/metrics/#src.thaw_slump_segmentation.models.utils.metrics.Recall.ignore_channels","title":"<code>ignore_channels = ignore_channels</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/utils/metrics/#src.thaw_slump_segmentation.models.utils.metrics.Recall.threshold","title":"<code>threshold = threshold</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/utils/metrics/#src.thaw_slump_segmentation.models.utils.metrics.Recall.__init__","title":"<code>__init__(eps=1e-07, threshold=0.5, activation=None, ignore_channels=None, **kwargs)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/utils/metrics.py</code> <pre><code>def __init__(self, eps=1e-7, threshold=0.5, activation=None, ignore_channels=None, **kwargs):\n    super().__init__(**kwargs)\n    self.eps = eps\n    self.threshold = threshold\n    self.activation = Activation(activation)\n    self.ignore_channels = ignore_channels\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/metrics/#src.thaw_slump_segmentation.models.utils.metrics.Recall.forward","title":"<code>forward(y_pr, y_gt)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/utils/metrics.py</code> <pre><code>def forward(self, y_pr, y_gt):\n    y_pr = self.activation(y_pr)\n    return F.recall(\n        y_pr, y_gt,\n        eps=self.eps,\n        threshold=self.threshold,\n        ignore_channels=self.ignore_channels,\n    )\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/train/","title":"Train","text":""},{"location":"reference/src/thaw_slump_segmentation/models/utils/train/#src.thaw_slump_segmentation.models.utils.train.Epoch","title":"<code>Epoch</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/utils/train.py</code> <pre><code>class Epoch:\n\n    def __init__(self, model, loss, metrics, stage_name, device='cpu', verbose=True):\n        self.model = model\n        self.loss = loss\n        self.metrics = metrics\n        self.stage_name = stage_name\n        self.verbose = verbose\n        self.device = device\n\n        self._to_device()\n\n    def _to_device(self):\n        self.model.to(self.device)\n        self.loss.to(self.device)\n        for metric in self.metrics:\n            metric.to(self.device)\n\n    def _format_logs(self, logs):\n        str_logs = ['{} - {:.4}'.format(k, v) for k, v in logs.items()]\n        s = ', '.join(str_logs)\n        return s\n\n    def batch_update(self, x, y):\n        raise NotImplementedError\n\n    def on_epoch_start(self):\n        pass\n\n    def run(self, dataloader):\n\n        self.on_epoch_start()\n\n        logs = {}\n        loss_meter = AverageValueMeter()\n        metrics_meters = {metric.__name__: AverageValueMeter() for metric in self.metrics}\n\n        with tqdm(dataloader, desc=self.stage_name, file=sys.stdout, disable=not (self.verbose)) as iterator:\n            for x, y in iterator:\n                x, y = x.to(self.device), y.to(self.device)\n                loss, y_pred = self.batch_update(x, y)\n\n                # update loss logs\n                loss_value = loss.cpu().detach().numpy()\n                loss_meter.add(loss_value)\n                loss_logs = {'loss': loss_meter.mean}\n                logs.update(loss_logs)\n\n                # update metrics logs\n                for metric_fn in self.metrics:\n                    metric_value = metric_fn(y_pred, y).cpu().detach().numpy()\n                    metrics_meters[metric_fn.__name__].add(metric_value)\n                metrics_logs = {k: v.mean for k, v in metrics_meters.items()}\n                logs.update(metrics_logs)\n\n                if self.verbose:\n                    s = self._format_logs(logs)\n                    iterator.set_postfix_str(s)\n\n        return logs\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/train/#src.thaw_slump_segmentation.models.utils.train.Epoch.device","title":"<code>device = device</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/utils/train/#src.thaw_slump_segmentation.models.utils.train.Epoch.loss","title":"<code>loss = loss</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/utils/train/#src.thaw_slump_segmentation.models.utils.train.Epoch.metrics","title":"<code>metrics = metrics</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/utils/train/#src.thaw_slump_segmentation.models.utils.train.Epoch.model","title":"<code>model = model</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/utils/train/#src.thaw_slump_segmentation.models.utils.train.Epoch.stage_name","title":"<code>stage_name = stage_name</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/utils/train/#src.thaw_slump_segmentation.models.utils.train.Epoch.verbose","title":"<code>verbose = verbose</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/utils/train/#src.thaw_slump_segmentation.models.utils.train.Epoch.__init__","title":"<code>__init__(model, loss, metrics, stage_name, device='cpu', verbose=True)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/utils/train.py</code> <pre><code>def __init__(self, model, loss, metrics, stage_name, device='cpu', verbose=True):\n    self.model = model\n    self.loss = loss\n    self.metrics = metrics\n    self.stage_name = stage_name\n    self.verbose = verbose\n    self.device = device\n\n    self._to_device()\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/train/#src.thaw_slump_segmentation.models.utils.train.Epoch.batch_update","title":"<code>batch_update(x, y)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/utils/train.py</code> <pre><code>def batch_update(self, x, y):\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/train/#src.thaw_slump_segmentation.models.utils.train.Epoch.on_epoch_start","title":"<code>on_epoch_start()</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/utils/train.py</code> <pre><code>def on_epoch_start(self):\n    pass\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/train/#src.thaw_slump_segmentation.models.utils.train.Epoch.run","title":"<code>run(dataloader)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/utils/train.py</code> <pre><code>def run(self, dataloader):\n\n    self.on_epoch_start()\n\n    logs = {}\n    loss_meter = AverageValueMeter()\n    metrics_meters = {metric.__name__: AverageValueMeter() for metric in self.metrics}\n\n    with tqdm(dataloader, desc=self.stage_name, file=sys.stdout, disable=not (self.verbose)) as iterator:\n        for x, y in iterator:\n            x, y = x.to(self.device), y.to(self.device)\n            loss, y_pred = self.batch_update(x, y)\n\n            # update loss logs\n            loss_value = loss.cpu().detach().numpy()\n            loss_meter.add(loss_value)\n            loss_logs = {'loss': loss_meter.mean}\n            logs.update(loss_logs)\n\n            # update metrics logs\n            for metric_fn in self.metrics:\n                metric_value = metric_fn(y_pred, y).cpu().detach().numpy()\n                metrics_meters[metric_fn.__name__].add(metric_value)\n            metrics_logs = {k: v.mean for k, v in metrics_meters.items()}\n            logs.update(metrics_logs)\n\n            if self.verbose:\n                s = self._format_logs(logs)\n                iterator.set_postfix_str(s)\n\n    return logs\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/train/#src.thaw_slump_segmentation.models.utils.train.TrainEpoch","title":"<code>TrainEpoch</code>","text":"<p>               Bases: <code>Epoch</code></p> Source code in <code>src/thaw_slump_segmentation/models/utils/train.py</code> <pre><code>class TrainEpoch(Epoch):\n\n    def __init__(self, model, loss, metrics, optimizer, device='cpu', verbose=True):\n        super().__init__(\n            model=model,\n            loss=loss,\n            metrics=metrics,\n            stage_name='train',\n            device=device,\n            verbose=verbose,\n        )\n        self.optimizer = optimizer\n\n    def on_epoch_start(self):\n        self.model.train()\n\n    def batch_update(self, x, y):\n        self.optimizer.zero_grad()\n        prediction = self.model.forward(x)\n        loss = self.loss(prediction, y)\n        loss.backward()\n        self.optimizer.step()\n        return loss, prediction\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/train/#src.thaw_slump_segmentation.models.utils.train.TrainEpoch.optimizer","title":"<code>optimizer = optimizer</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/models/utils/train/#src.thaw_slump_segmentation.models.utils.train.TrainEpoch.__init__","title":"<code>__init__(model, loss, metrics, optimizer, device='cpu', verbose=True)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/utils/train.py</code> <pre><code>def __init__(self, model, loss, metrics, optimizer, device='cpu', verbose=True):\n    super().__init__(\n        model=model,\n        loss=loss,\n        metrics=metrics,\n        stage_name='train',\n        device=device,\n        verbose=verbose,\n    )\n    self.optimizer = optimizer\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/train/#src.thaw_slump_segmentation.models.utils.train.TrainEpoch.batch_update","title":"<code>batch_update(x, y)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/utils/train.py</code> <pre><code>def batch_update(self, x, y):\n    self.optimizer.zero_grad()\n    prediction = self.model.forward(x)\n    loss = self.loss(prediction, y)\n    loss.backward()\n    self.optimizer.step()\n    return loss, prediction\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/train/#src.thaw_slump_segmentation.models.utils.train.TrainEpoch.on_epoch_start","title":"<code>on_epoch_start()</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/utils/train.py</code> <pre><code>def on_epoch_start(self):\n    self.model.train()\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/train/#src.thaw_slump_segmentation.models.utils.train.ValidEpoch","title":"<code>ValidEpoch</code>","text":"<p>               Bases: <code>Epoch</code></p> Source code in <code>src/thaw_slump_segmentation/models/utils/train.py</code> <pre><code>class ValidEpoch(Epoch):\n\n    def __init__(self, model, loss, metrics, device='cpu', verbose=True):\n        super().__init__(\n            model=model,\n            loss=loss,\n            metrics=metrics,\n            stage_name='valid',\n            device=device,\n            verbose=verbose,\n        )\n\n    def on_epoch_start(self):\n        self.model.eval()\n\n    def batch_update(self, x, y):\n        with torch.no_grad():\n            prediction = self.model.forward(x)\n            loss = self.loss(prediction, y)\n        return loss, prediction\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/train/#src.thaw_slump_segmentation.models.utils.train.ValidEpoch.__init__","title":"<code>__init__(model, loss, metrics, device='cpu', verbose=True)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/utils/train.py</code> <pre><code>def __init__(self, model, loss, metrics, device='cpu', verbose=True):\n    super().__init__(\n        model=model,\n        loss=loss,\n        metrics=metrics,\n        stage_name='valid',\n        device=device,\n        verbose=verbose,\n    )\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/train/#src.thaw_slump_segmentation.models.utils.train.ValidEpoch.batch_update","title":"<code>batch_update(x, y)</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/utils/train.py</code> <pre><code>def batch_update(self, x, y):\n    with torch.no_grad():\n        prediction = self.model.forward(x)\n        loss = self.loss(prediction, y)\n    return loss, prediction\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/models/utils/train/#src.thaw_slump_segmentation.models.utils.train.ValidEpoch.on_epoch_start","title":"<code>on_epoch_start()</code>","text":"Source code in <code>src/thaw_slump_segmentation/models/utils/train.py</code> <pre><code>def on_epoch_start(self):\n    self.model.eval()\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/scripts/inference/","title":"Inference","text":"<p>Usecase 2 Inference Script</p>"},{"location":"reference/src/thaw_slump_segmentation/scripts/inference/#src.thaw_slump_segmentation.scripts.inference.FIGSIZE_MAX","title":"<code>FIGSIZE_MAX = 20</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/scripts/inference/#src.thaw_slump_segmentation.scripts.inference.cmap_dem","title":"<code>cmap_dem = flatui_cmap('Alizarin', 'Clouds', 'Peter River')</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/scripts/inference/#src.thaw_slump_segmentation.scripts.inference.cmap_ndvi","title":"<code>cmap_ndvi = 'RdYlGn'</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/scripts/inference/#src.thaw_slump_segmentation.scripts.inference.cmap_prob","title":"<code>cmap_prob = flatui_cmap('Midnight Blue', 'Alizarin')</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/scripts/inference/#src.thaw_slump_segmentation.scripts.inference.cmap_slope","title":"<code>cmap_slope = flatui_cmap('Clouds', 'Midnight Blue')</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/scripts/inference/#src.thaw_slump_segmentation.scripts.inference.do_inference","title":"<code>do_inference(tilename, sources, model, dev, logger, name, data_dir, inference_dir, patch_size, margin_size, log_path=None)</code>","text":"Source code in <code>src/thaw_slump_segmentation/scripts/inference.py</code> <pre><code>def do_inference(\n    tilename, sources, model, dev, logger, name, data_dir, inference_dir, patch_size, margin_size, log_path=None\n):\n    tile_logger = get_logger(f'inference.{tilename}')\n    # ===== PREPARE THE DATA =====\n    DATA_ROOT = data_dir\n    INFERENCE_ROOT = inference_dir\n    data_directory = DATA_ROOT / 'tiles' / tilename\n    if not data_directory.exists():\n        logger.info(f'Preprocessing directory {tilename}')\n        raw_directory = DATA_ROOT / 'input' / tilename\n        if not raw_directory.exists():\n            logger.error(\n                f\"Couldn't find tile '{tilename}' in {DATA_ROOT}/tiles or {DATA_ROOT}/input. Skipping this tile\"\n            )\n            return\n        # TODO: The arguments don't match the function signature -&gt; Invest how to resolve\n        preprocess_directory(raw_directory, log_path, label_required=False)\n        # After this, data_directory should contain all the stuff that we need.\n\n    if name:\n        output_directory = INFERENCE_ROOT / name / tilename\n    else:\n        output_directory = INFERENCE_ROOT / tilename\n\n    output_directory.mkdir(exist_ok=True, parents=True)\n\n    planet_imagery_path = next(data_directory.glob('*_SR.tif'))\n\n    data = []\n    for source in sources:\n        tile_logger.debug(f'loading {source.name}')\n        if source.name == 'planet':\n            tif_path = planet_imagery_path\n        else:\n            tif_path = data_directory / f'{source.name}.tif'\n\n        data_part = rio.open(tif_path).read().astype(np.float32)\n\n        if source.name == 'tcvis':\n            data_part = data_part[:3]\n        data_part = np.nan_to_num(data_part, nan=0.0)\n\n        data_part = data_part / np.array(source.normalization_factors, dtype=np.float32).reshape(-1, 1, 1)\n        data.append(data_part)\n\n    def make_img(filename, source, colorbar=False, mask=None, **kwargs):\n        idx = sources.index(source)\n        h, w = data[idx].shape[1:]\n        if mask is None:\n            mask = np.zeros((h, w), dtype=np.bool)\n        if h &gt; w:\n            figsize = (FIGSIZE_MAX * w / h, FIGSIZE_MAX)\n        else:\n            figsize = (FIGSIZE_MAX, FIGSIZE_MAX * h / w)\n        fig, ax = plt.subplots(figsize=figsize)\n        ax.axis('off')\n\n        if source.channels &gt;= 3:\n            rgb = np.stack([np.ma.masked_where(mask, data[idx][i])[::10, ::10] for i in range(3)], axis=-1)\n            rgb = np.clip(rgb, 0, 1)\n            ax.imshow(rgb, aspect='equal')\n        elif source.channels == 1:\n            image = ax.imshow(np.ma.masked_where(mask, data[idx][0]), aspect='equal', **kwargs)\n            if colorbar:\n                plt.colorbar(image)\n        plt.savefig(output_directory / filename, bbox_inches='tight', pad_inches=0)\n        plt.close()\n\n    def plot_results(image, outfile):\n        fig, ax = plt.subplots(figsize=figsize)\n        ax.axis('off')\n        mappable = ax.imshow(image, vmin=0, vmax=1, cmap=cmap_prob, aspect='equal')\n        plt.colorbar(mappable)\n        plt.savefig(outfile, bbox_inches='tight', pad_inches=0)\n        plt.close()\n\n    full_data = np.concatenate(data, axis=0)\n    nodata = np.all(full_data == 0, axis=0, keepdims=True)\n    full_data = torch.from_numpy(full_data)\n    full_data = full_data.unsqueeze(0)  # Pretend this is a batch of size 1\n\n    res = predict(model, full_data, patch_size, margin_size, dev).numpy()\n    del full_data\n\n    res[nodata] = np.nan\n    binarized = np.ones_like(res, dtype=np.uint8) * 255\n    binarized[~nodata] = (res[~nodata] &gt; 0.5).astype(np.uint8)\n\n    # define output file paths\n    out_path_proba = output_directory / 'pred_probability.tif'\n    out_path_label = output_directory / 'pred_binarized.tif'\n    out_path_pre_poly = output_directory / 'pred_binarized_tmp.tif'\n    out_path_shp = output_directory / 'pred_binarized.shp'\n    out_path_gpkg = output_directory / 'pred_binarized.gpkg'\n\n    # Get the input profile\n    with rio.open(planet_imagery_path) as input_raster:\n        profile = input_raster.profile\n        profile.update(\n            dtype=rio.float32,\n            count=1,\n            compress='lzw',\n            driver='GTiff',\n            # tiled=True\n        )\n\n    with rio.open(out_path_proba, 'w', **profile) as output_raster:\n        output_raster.write(res.astype(np.float32))\n    flush_rio(out_path_proba)\n\n    profile.update(dtype=rio.uint8, nodata=255)\n    with rio.open(out_path_label, 'w', **profile) as output_raster:\n        output_raster.write(binarized)\n    flush_rio(out_path_label)\n\n    with rio.open(out_path_pre_poly, 'w', **profile) as output_raster:\n        output_raster.write((binarized == 1).astype(np.uint8))\n    flush_rio(out_path_pre_poly)\n\n    # create vectors\n    log_run(\n        f'{gdal.polygonize} {out_path_pre_poly} -q -mask {out_path_pre_poly} -f \"ESRI Shapefile\" {out_path_shp}',\n        tile_logger,\n    )\n    log_run(\n        f'{gdal.polygonize} {out_path_pre_poly} -q -mask {out_path_pre_poly} -f \"GPKG\" {out_path_gpkg}', tile_logger\n    )\n    # log_run(f'python {gdal.polygonize} {out_path_pre_poly} -q -mask {out_path_pre_poly} -f \"ESRI Shapefile\" {out_path_shp}', tile_logger)\n    out_path_pre_poly.unlink()\n\n    h, w = res.shape[1:]\n    if h &gt; w:\n        figsize = (FIGSIZE_MAX * w / h, FIGSIZE_MAX)\n    else:\n        figsize = (FIGSIZE_MAX, FIGSIZE_MAX * h / w)\n\n    for src in sources:\n        kwargs = dict()\n        if src.name == 'ndvi':\n            kwargs = dict(colorbar=True, cmap=cmap_ndvi, vmin=0, vmax=1)\n        elif src.name == 'relative_elevation':\n            kwargs = dict(colorbar=True, cmap=cmap_dem, vmin=0, vmax=1)\n        elif src.name == 'slope':\n            kwargs = dict(colorbar=True, cmap=cmap_slope, vmin=0, vmax=0.5)\n        make_img(f'{src.name}.jpg', src, mask=nodata[0], **kwargs)\n\n    outpath = output_directory / 'pred_probability.jpg'\n    plot_results(np.ma.masked_where(nodata[0], res[0]), outpath)\n\n    outpath = output_directory / 'pred_binarized.jpg'\n    plot_results(np.ma.masked_where(nodata[0], binarized[0]), outpath)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/scripts/inference/#src.thaw_slump_segmentation.scripts.inference.flush_rio","title":"<code>flush_rio(filepath)</code>","text":"<p>For some reason, rasterio doesn't actually finish writing a file after finishing a <code>with rio.open(...) as ...:</code> block Trying to open the file for reading seems to force a flush</p> Source code in <code>src/thaw_slump_segmentation/scripts/inference.py</code> <pre><code>def flush_rio(filepath):\n    \"\"\"For some reason, rasterio doesn't actually finish writing\n    a file after finishing a `with rio.open(...) as ...:` block\n    Trying to open the file for reading seems to force a flush\"\"\"\n\n    with rio.open(filepath) as _:\n        pass\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/scripts/inference/#src.thaw_slump_segmentation.scripts.inference.inference","title":"<code>inference(name, model_path, tile_to_predict, gdal_bin='/usr/bin', gdal_path='/usr/bin', n_jobs=-1, ckpt='latest', data_dir=Path('data'), log_dir=Path('logs'), inference_dir=Path('inference'), margin_size=256, patch_size=1024)</code>","text":"<p>Inference Script</p> Source code in <code>src/thaw_slump_segmentation/scripts/inference.py</code> <pre><code>def inference(\n    name: Annotated[\n        str, typer.Option('--name', '-n', help='Name of inference run, data will be stored in subdirectory')\n    ],\n    model_path: Annotated[str, typer.Argument(help='path to model, use the model base path')],\n    tile_to_predict: Annotated[\n        List[str], \n        typer.Argument(help='one or multiple IDs of tiles/scenes, primarily the PLANET-IDs (e.g. \"20230807_195407_62_248c\")')\n        ],\n    gdal_bin: Annotated[str, typer.Option('--gdal_bin', help='Path to gdal binaries', envvar='GDAL_BIN')] = '/usr/bin',\n    gdal_path: Annotated[\n        str, typer.Option('--gdal_path', help='Path to gdal scripts', envvar='GDAL_PATH')\n    ] = '/usr/bin',\n    n_jobs: Annotated[int, typer.Option('--n_jobs', help='number of parallel joblib jobs')] = -1,\n    ckpt: Annotated[str, typer.Option(help='Checkpoint to use')] = 'latest',\n    data_dir: Annotated[Path, typer.Option('--data_dir', help='Path to data processing dir')] = Path('data'),\n    log_dir: Annotated[Path, typer.Option('--log_dir', help='Path to log dir')] = Path('logs'),\n    inference_dir: Annotated[Path, typer.Option('--inference_dir', help='Main inference directory')] = Path(\n        'inference'\n    ),\n    margin_size: Annotated[int, typer.Option('--margin_size', '-n', help='Size of patch overlap')] = 256,\n    patch_size: Annotated[int, typer.Option('--patch_size', '-p', help='Size of patches')] = 1024,\n):\n    \"\"\"Inference Script\"\"\"\n\n    # Mock old args object\n    gdal.initialize(bin=gdal_bin, path=gdal_path)\n\n    timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n    log_path = Path(log_dir) / f'inference-{timestamp}.log'\n    if not Path(log_dir).exists():\n        os.mkdir(Path(log_dir))\n    init_logging(log_path)\n    logger = get_logger('inference')\n\n    # ===== LOAD THE MODEL =====\n    cuda = True if torch.cuda.is_available() else False\n    dev = torch.device('cpu') if not cuda else torch.device('cuda')\n    logger.info(f'Running on {dev} device')\n\n    if not model_path:\n        last_modified = 0\n        last_modeldir = None\n\n        for config_file in Path(log_dir).glob('*/config.yml'):\n            modified = config_file.stat().st_mtime\n            if modified &gt; last_modified:\n                last_modified = modified\n                last_modeldir = config_file.parent\n        model_path = last_modeldir\n\n    model_dir = Path(model_path)\n    config = yaml.load((model_dir / 'config.yml').open(), Loader=yaml.SafeLoader)\n\n    m = config['model']\n    # print(m['architecture'],m['encoder'], m['input_channels'])\n    model = create_model(\n        arch=m['architecture'],\n        encoder_name=m['encoder'],\n        encoder_weights=None if m['encoder_weights'] == 'random' else m['encoder_weights'],\n        classes=1,\n        in_channels=m['input_channels'],\n    )\n\n    if ckpt == 'latest':\n        ckpt_nums = [int(ckpt.stem) for ckpt in model_dir.glob('checkpoints/*.pt')]\n        last_ckpt = max(ckpt_nums)\n    else:\n        last_ckpt = int(ckpt)\n    ckpt = model_dir / 'checkpoints' / f'{last_ckpt:02d}.pt'\n    logger.info(f'Loading checkpoint {ckpt}')\n\n    # Parallelized Model needs to be declared before loading\n    try:\n        model.load_state_dict(torch.load(ckpt, map_location=dev))\n    except Exception:\n        model = nn.DataParallel(model)\n        model.load_state_dict(torch.load(ckpt, map_location=dev))\n\n    model = model.to(dev)\n\n    sources = DataSources(config['data_sources'])\n\n    torch.set_grad_enabled(False)\n\n    for tilename in tqdm(tile_to_predict):\n        do_inference(\n            tilename, sources, model, dev, logger, name, data_dir, inference_dir, patch_size, margin_size, log_path\n        )\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/scripts/inference/#src.thaw_slump_segmentation.scripts.inference.main","title":"<code>main()</code>","text":"Source code in <code>src/thaw_slump_segmentation/scripts/inference.py</code> <pre><code>def main():\n    parser = argparse.ArgumentParser(\n        description='Inference Script', formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n    parser.add_argument('--gdal_bin', default='', help='Path to gdal binaries')\n    parser.add_argument('--gdal_path', default='', help='Path to gdal scripts')\n    parser.add_argument('--n_jobs', default=-1, type=int, help='number of parallel joblib jobs')\n    parser.add_argument('--ckpt', default='latest', type=str, help='Checkpoint to use')\n    parser.add_argument('--data_dir', default='data', type=Path, help='Path to data processing dir')\n    parser.add_argument('--log_dir', default='logs', type=Path, help='Path to log dir')\n    parser.add_argument('--inference_dir', default='inference', type=Path, help='Main inference directory')\n    parser.add_argument(\n        '-n', '--name', default=None, type=str, help='Name of inference run, data will be stored in subdirectory'\n    )\n    parser.add_argument('-m', '--margin_size', default=256, type=int, help='Size of patch overlap')\n    parser.add_argument('-p', '--patch_size', default=1024, type=int, help='Size of patches')\n    parser.add_argument('model_path', type=str, help='path to model, use the model base path')\n    parser.add_argument(\n        'tile_to_predict', \n        type=str, \n        help='IDs of tiles/scenes, primarily the PLANET-IDs (e.g. \"20230807_195407_62_248c\" )',\n        nargs='+'\n        )\n\n    args = parser.parse_args()\n\n    inference(\n        name=args.name,\n        model_path=args.model_path,\n        tile_to_predict=args.tile_to_predict,\n        gdal_bin=args.gdal_bin,\n        gdal_path=args.gdal_path,\n        n_jobs=args.n_jobs,\n        ckpt=args.ckpt,\n        data_dir=args.data_dir,\n        log_dir=args.log_dir,\n        inference_dir=args.inference_dir,\n        margin_size=args.margin_size,\n        patch_size=args.patch_size,\n    )\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/scripts/inference/#src.thaw_slump_segmentation.scripts.inference.predict","title":"<code>predict(model, imagery, patch_size, margin_size, device='cpu')</code>","text":"Source code in <code>src/thaw_slump_segmentation/scripts/inference.py</code> <pre><code>def predict(model, imagery, patch_size, margin_size, device='cpu'):\n    prediction = torch.zeros(1, *imagery.shape[2:])\n    weights = torch.zeros(1, *imagery.shape[2:])\n\n    margin_ramp = torch.cat(\n        [\n            torch.linspace(0, 1, margin_size),\n            torch.ones(patch_size - 2 * margin_size),\n            torch.linspace(1, 0, margin_size),\n        ]\n    )\n\n    soft_margin = margin_ramp.reshape(1, 1, patch_size) * margin_ramp.reshape(1, patch_size, 1)\n\n    for y in np.arange(0, imagery.shape[2], (patch_size - margin_size)):\n        for x in np.arange(0, imagery.shape[3], (patch_size - margin_size)):\n            if y + patch_size &gt; imagery.shape[2]:\n                y = imagery.shape[2] - patch_size\n            if x + patch_size &gt; imagery.shape[3]:\n                x = imagery.shape[3] - patch_size\n            patch_imagery = imagery[:, :, y : y + patch_size, x : x + patch_size]\n            patch_pred = torch.sigmoid(model(patch_imagery.to(device))[0].cpu())\n\n            # Essentially premultiplied alpha blending\n            prediction[:, y : y + patch_size, x : x + patch_size] += patch_pred * soft_margin\n            weights[:, y : y + patch_size, x : x + patch_size] += soft_margin\n\n    # Avoid division by zero\n    weights = torch.where(weights == 0, torch.ones_like(weights), weights)\n    return prediction / weights\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/scripts/prepare_data/","title":"Prepare data","text":"<p>Usecase 2 Data Preprocessing Script</p>"},{"location":"reference/src/thaw_slump_segmentation/scripts/prepare_data/#src.thaw_slump_segmentation.scripts.prepare_data.RASTERFILTER","title":"<code>RASTERFILTER = '*_SR*.tif'</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/scripts/prepare_data/#src.thaw_slump_segmentation.scripts.prepare_data.VECTORFILTER","title":"<code>VECTORFILTER = '*.shp'</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/scripts/prepare_data/#src.thaw_slump_segmentation.scripts.prepare_data.args","title":"<code>args = parser.parse_args()</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/scripts/prepare_data/#src.thaw_slump_segmentation.scripts.prepare_data.parser","title":"<code>parser = argparse.ArgumentParser(description='Make data ready for training', formatter_class=argparse.ArgumentDefaultsHelpFormatter)</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/scripts/prepare_data/#src.thaw_slump_segmentation.scripts.prepare_data.do_gdal_calls","title":"<code>do_gdal_calls(DATASET, xsize, ysize, overlap, aux_data=['ndvi', 'tcvis', 'slope', 'relative_elevation'], logger=None)</code>","text":"Source code in <code>src/thaw_slump_segmentation/scripts/prepare_data.py</code> <pre><code>def do_gdal_calls(\n    DATASET,\n    xsize: int,\n    ysize: int,\n    overlap: int,\n    aux_data=['ndvi', 'tcvis', 'slope', 'relative_elevation'],\n    logger=None,\n):\n    rasterfile = glob_file(DATASET, RASTERFILTER, logger)\n    image_name = rasterfile.name[0:-7]\n\n    maskfile = DATASET / f'{image_name}_mask.tif'\n\n    tile_dir_data = DATASET / 'tiles' / 'data'\n    tile_dir_mask = DATASET / 'tiles' / 'mask'\n\n    # Create parents on the first data folder\n    tile_dir_data.mkdir(exist_ok=True, parents=True)\n    tile_dir_mask.mkdir(exist_ok=True)\n\n\n    # Retile data, mask\n    # log_run(f'python {gdal.retile} -ps {XSIZE} {YSIZE} -overlap {OVERLAP} -targetDir {tile_dir_data} {rasterfile}', logger)\n    # log_run(f'python {gdal.retile} -ps {XSIZE} {YSIZE} -overlap {OVERLAP} -targetDir {tile_dir_mask} {maskfile}', logger)\n    log_run(f'{gdal.retile} -ps {xsize} {ysize} -overlap {overlap} -targetDir {tile_dir_data} {rasterfile}', logger)\n    log_run(f'{gdal.retile} -ps {xsize} {ysize} -overlap {overlap} -targetDir {tile_dir_mask} {maskfile}', logger)\n\n    # Retile additional data\n    for aux in aux_data:\n        auxfile = DATASET / f'{aux}.tif'\n        tile_dir_aux = DATASET / 'tiles' / aux\n        tile_dir_aux.mkdir(exist_ok=True)\n        # log_run(f'python {gdal.retile} -ps {XSIZE} {YSIZE} -overlap {OVERLAP} -targetDir {tile_dir_aux} {auxfile}', logger)\n        log_run(f'{gdal.retile} -ps {xsize} {ysize} -overlap {overlap} -targetDir {tile_dir_aux} {auxfile}', logger)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/scripts/prepare_data/#src.thaw_slump_segmentation.scripts.prepare_data.get_planet_product_type","title":"<code>get_planet_product_type(img_path)</code>","text":"<p>return if file is scene or OrthoTile</p> Source code in <code>src/thaw_slump_segmentation/scripts/prepare_data.py</code> <pre><code>def get_planet_product_type(img_path):\n    \"\"\"\n    return if file is scene or OrthoTile\"\"\"\n    split = img_path.stem.split('_')\n    # check if 4th last imagename segment is BGRN\n    if split[-4] == 'BGRN':\n        pl_type = 'OrthoTile'\n    elif len(split) == 6 and len(split[2]) == 6:\n        pl_type = \"Sentinel2\"\n    else:\n        pl_type = 'Scene'\n\n    return pl_type\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/scripts/prepare_data/#src.thaw_slump_segmentation.scripts.prepare_data.glob_file","title":"<code>glob_file(DATASET, filter_string, logger=None)</code>","text":"Source code in <code>src/thaw_slump_segmentation/scripts/prepare_data.py</code> <pre><code>def glob_file(DATASET, filter_string, logger=None):\n    candidates = list(DATASET.glob(f'{filter_string}'))\n    if len(candidates) == 1:\n        logger.debug(f'Found file: {candidates[0]}')\n        return candidates[0]\n    else:\n        raise ValueError(f'Found {len(candidates)} candidates.' 'Please make selection more specific!')\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/scripts/prepare_data/#src.thaw_slump_segmentation.scripts.prepare_data.main_function","title":"<code>main_function(dataset, log_path, h5dir, xsize, ysize, overlap, threshold, skip_gdal, gdal_bin, gdal_path)</code>","text":"Source code in <code>src/thaw_slump_segmentation/scripts/prepare_data.py</code> <pre><code>def main_function(\n    dataset, log_path, h5dir: Path, xsize: int, ysize: int, overlap: int, threshold: float, skip_gdal: bool, gdal_bin:str, gdal_path:str\n):\n    init_logging(log_path)\n    thread_logger = get_logger(f'prepare_data.{dataset.name}')\n    thread_logger.info(f'Starting preparation on dataset {dataset}')\n    if not skip_gdal:\n        gdal.initialize(bin=gdal_bin, path=gdal_bin)\n        thread_logger.info('Doing GDAL Calls')\n        do_gdal_calls(dataset, xsize, ysize, overlap, logger=thread_logger)\n    else:\n        thread_logger.info('Skipping GDAL Calls')\n\n    tifs = list(sorted(dataset.glob('tiles/data/*.tif')))\n    if len(tifs) == 0:\n        thread_logger.warning(f'No tiles found for {dataset}, skipping this directory.')\n        return\n\n    h5_path = h5dir / f'{dataset.name}.h5'\n    info_dir = h5dir / dataset.name\n    info_dir.mkdir(parents=True)\n\n    thread_logger.info(f'Creating H5File at {h5_path}')\n    h5 = h5py.File(\n        h5_path,\n        'w',\n        rdcc_nbytes=2 * (1 &lt;&lt; 30),  # 2 GiB\n        rdcc_nslots=200003,\n    )\n    channel_numbers = dict(planet=4, ndvi=1, tcvis=3, relative_elevation=1, slope=1)\n\n    datasets = dict()\n    for dataset_name, nchannels in channel_numbers.items():\n        ds = h5.create_dataset(\n            dataset_name,\n            dtype=np.float32,\n            shape=(len(tifs), nchannels, xsize, ysize),\n            maxshape=(len(tifs), nchannels, xsize, ysize),\n            chunks=(1, nchannels, xsize, ysize),\n            compression='lzf',\n            scaleoffset=3,\n        )\n        datasets[dataset_name] = ds\n\n    datasets['mask'] = h5.create_dataset(\n        'mask',\n        dtype=np.uint8,\n        shape=(len(tifs), 1, xsize, ysize),\n        maxshape=(len(tifs), 1, xsize, ysize),\n        chunks=(1, 1, xsize, ysize),\n        compression='lzf',\n    )\n\n    # Convert data to HDF5 storage for efficient data loading\n    i = 0\n    bad_tiles = 0\n    for img in tifs:\n        tile = {}\n        with rio.open(img) as raster:\n            tile['planet'] = raster.read()\n\n        if (tile['planet'] == 0).all(axis=0).mean() &gt; threshold:\n            bad_tiles += 1\n            continue\n\n        with rio.open(mask_from_img(img)) as raster:\n            tile['mask'] = raster.read()\n        assert tile['mask'].max() &lt;= 1, \"Mask can't contain values &gt; 1\"\n\n        for other in channel_numbers:\n            if other == 'planet':\n                continue  # We already did this!\n            with rio.open(other_from_img(img, other)) as raster:\n                data = raster.read()\n            if data.shape[0] &gt; channel_numbers[other]:\n                # This is for tcvis mostly\n                data = data[: channel_numbers[other]]\n            tile[other] = data\n\n        # gdal_retile leaves narrow stripes at the right and bottom border,\n        # which are filtered out here:\n        is_narrow = False\n        for tensor in tile.values():\n            if tensor.shape[-2:] != (xsize, ysize):\n                is_narrow = True\n                break\n        if is_narrow:\n            bad_tiles += 1\n            continue\n\n        for t in tile:\n            datasets[t][i] = tile[t]\n\n        make_info_picture(tile, info_dir / f'{i}.jpg')\n        i += 1\n\n    for t in datasets:\n        datasets[t].resize(i, axis=0)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/scripts/prepare_data/#src.thaw_slump_segmentation.scripts.prepare_data.make_info_picture","title":"<code>make_info_picture(tile, filename)</code>","text":"<p>Make overview picture</p> Source code in <code>src/thaw_slump_segmentation/scripts/prepare_data.py</code> <pre><code>def make_info_picture(tile, filename):\n    \"Make overview picture\"\n    rgbn = np.clip(tile['planet'][:4, :, :].transpose(1, 2, 0) / 3000 * 255, 0, 255).astype(np.uint8)\n    tcvis = np.clip(tile['tcvis'].transpose(1, 2, 0), 0, 255).astype(np.uint8)\n\n    rgb = rgbn[:, :, :3]\n    nir = rgbn[:, :, [3, 2, 1]]\n    mask = (tile['mask'][[0, 0, 0]].transpose(1, 2, 0) * 255).astype(np.uint8)\n\n    img = np.concatenate(\n        [\n            np.concatenate([rgb, nir], axis=1),\n            np.concatenate([tcvis, mask], axis=1),\n        ]\n    )\n    imsave(filename, img)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/scripts/prepare_data/#src.thaw_slump_segmentation.scripts.prepare_data.mask_from_img","title":"<code>mask_from_img(img_path)</code>","text":"<p>Given an image path, return path for the mask</p> Source code in <code>src/thaw_slump_segmentation/scripts/prepare_data.py</code> <pre><code>def mask_from_img(img_path):\n    \"\"\"\n    Given an image path, return path for the mask\n    \"\"\"\n    # change for\n    product_type = get_planet_product_type(img_path)\n    base = img_path.parent.parent\n\n    if product_type == 'Scene':\n        date, time, *block, platform, _, sr, row, col = img_path.stem.split('_')\n        block = '_'.join(block)\n        mask_path = base / 'mask' / f'{date}_{time}_{block}_mask_{row}_{col}.tif'\n    elif product_type == \"Sentinel2\":\n        date, processing_date, tile_id, _, row, col = img_path.stem.split('_')\n        mask_path = base / 'mask' / f'{date}_{processing_date}_{tile_id}_mask_{row}_{col}.tif'\n    else:\n        block, tile, date, sensor, bgrn, sr, row, col = img_path.stem.split('_')\n        mask_path = base / 'mask' / f'{block}_{tile}_{date}_{sensor}_mask_{row}_{col}.tif'\n\n    assert mask_path.exists()\n\n    return mask_path\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/scripts/prepare_data/#src.thaw_slump_segmentation.scripts.prepare_data.other_from_img","title":"<code>other_from_img(img_path, other)</code>","text":"<p>Given an image path, return paths for mask and tcvis</p> Source code in <code>src/thaw_slump_segmentation/scripts/prepare_data.py</code> <pre><code>def other_from_img(img_path, other):\n    \"\"\"\n    Given an image path, return paths for mask and tcvis\n    \"\"\"\n    product_type = get_planet_product_type(img_path)\n    if product_type == 'Scene':\n        date, time, *block, platform, _, sr, row, col = img_path.stem.split('_')\n    elif product_type == \"Sentinel2\":\n        date, processing_date, tile_id, _, row, col = img_path.stem.split('_')\n    else:\n        block, tile, date, sensor, bgrn, sr, row, col = img_path.stem.split('_')\n\n    base = img_path.parent.parent\n\n    path = base / other / f'{other}_{row}_{col}.tif'\n    assert path.exists()\n\n    return path\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/scripts/prepare_data/#src.thaw_slump_segmentation.scripts.prepare_data.prepare_data","title":"<code>prepare_data(data_dir=Path('data'), log_dir=Path('logs'), skip_gdal=False, gdal_bin='/usr/bin', gdal_path='/usr/bin', n_jobs=-1, nodata_threshold=50, tile_size='256x256', tile_overlap=25)</code>","text":"<p>Make data ready for training</p> Source code in <code>src/thaw_slump_segmentation/scripts/prepare_data.py</code> <pre><code>def prepare_data(\n    data_dir: Annotated[Path, typer.Option('--data_dir', help='Path to data processing dir')] = Path('data'),\n    log_dir: Annotated[Path, typer.Option('--log_dir', help='Path to log dir')] = Path('logs'),\n    skip_gdal: Annotated[\n        bool,\n        typer.Option('--skip_gdal/--use_gdal', help='Skip the Gdal conversion stage (if it has already been done)'),\n    ] = False,\n    gdal_bin: Annotated[\n        str,\n        typer.Option('--gdal_bin', help='Path to gdal binaries (ignored if --skip_gdal is passed)', envvar='GDAL_BIN'),\n    ] = '/usr/bin',\n    gdal_path: Annotated[\n        str,\n        typer.Option('--gdal_path', help='Path to gdal scripts (ignored if --skip_gdal is passed)', envvar='GDAL_PATH'),\n    ] = '/usr/bin',\n    n_jobs: Annotated[int, typer.Option('--n_jobs', help='number of parallel joblib jobs')] = -1,\n    nodata_threshold: Annotated[\n        float, typer.Option('--nodata_threshold', help='Throw away data with at least this % of nodata pixels')\n    ] = 50,\n    tile_size: Annotated[\n        str, typer.Option('--tile_size', help=\"Tiling size in pixels e.g. '256x256'\", callback=tile_size_callback)\n    ] = '256x256',\n    tile_overlap: Annotated[int, typer.Option('--tile_overlap', help='Overlap of the tiles in pixels')] = 25,\n):\n    \"\"\"Make data ready for training\"\"\"\n\n    # Tiling Settings\n    if isinstance(tile_size, tuple):\n        xsize, ysize = tile_size\n    else:\n        xsize, ysize = map(int, tile_size.split('x'))\n    overlap = tile_overlap\n\n    timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n    log_path = log_dir / f'prepare_data-{timestamp}.log'\n    log_dir.mkdir(exist_ok=True)\n    init_logging(log_path)\n    logger = get_logger('prepare_data')\n    logger.info('#############################')\n    logger.info('# Starting Data Preparation #')\n    logger.info('#############################')\n\n    threshold = nodata_threshold / 100\n\n    DATA_ROOT = data_dir\n    DATA_DIR = DATA_ROOT / 'tiles'\n    h5dir = DATA_ROOT / 'h5'\n    h5dir.mkdir(exist_ok=True)\n\n    # All folders that contain the big raster (...AnalyticsML_SR.tif) are assumed to be a dataset\n    datasets = [raster.parent for raster in DATA_DIR.glob('*/' + RASTERFILTER)]\n\n    overwrite_conflicts = []\n    for dataset in datasets:\n        check_dir = h5dir / dataset.name\n        if check_dir.exists():\n            overwrite_conflicts.append(check_dir)\n\n    if overwrite_conflicts:\n        logger.warning(f\"Found old data directories: {', '.join(dir.name for dir in overwrite_conflicts)}.\")\n        decision = input('Delete and recreate them [d], skip them [s] or abort [a]? ').lower()\n        if decision == 'd':\n            logger.info('User chose to delete old directories.')\n            for old_dir in overwrite_conflicts:\n                shutil.rmtree(old_dir)\n        elif decision == 's':\n            logger.info('User chose to skip old directories.')\n            already_done = [d.name for d in overwrite_conflicts]\n            datasets = [d for d in datasets if d.name not in already_done]\n        else:\n            # When in doubt, don't overwrite/change anything to be safe\n            logger.error('Aborting due to conflicts with existing data directories.')\n            sys.exit(1)\n\n    Parallel(n_jobs=n_jobs)(\n        delayed(main_function)(dataset, log_path, h5dir, xsize, ysize, overlap, threshold, skip_gdal, gdal_bin, gdal_path)\n        for dataset in datasets\n    )\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/scripts/prepare_data/#src.thaw_slump_segmentation.scripts.prepare_data.read_and_assert_imagedata","title":"<code>read_and_assert_imagedata(image_path)</code>","text":"Source code in <code>src/thaw_slump_segmentation/scripts/prepare_data.py</code> <pre><code>def read_and_assert_imagedata(image_path):\n    with rio.open(image_path) as raster:\n        if raster.count &lt;= 3:\n            data = raster.read()\n        else:\n            data = raster.read()[:3]\n        # Assert data can safely be coerced to int16\n        assert data.max() &lt; 2**15\n        return data\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/scripts/prepare_data/#src.thaw_slump_segmentation.scripts.prepare_data.tile_size_callback","title":"<code>tile_size_callback(value)</code>","text":"Source code in <code>src/thaw_slump_segmentation/scripts/prepare_data.py</code> <pre><code>def tile_size_callback(value: str):\n    try:\n        x, y = map(int, value.split('x'))\n    except ValueError:\n        raise typer.BadParameter('Tile size must be in the format \"256x256\"')\n    return x, y\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/scripts/process_02_inference/","title":"Process 02 inference","text":""},{"location":"reference/src/thaw_slump_segmentation/scripts/process_02_inference/#src.thaw_slump_segmentation.scripts.process_02_inference.main","title":"<code>main()</code>","text":"Source code in <code>src/thaw_slump_segmentation/scripts/process_02_inference.py</code> <pre><code>def main():\n    # ### Settings\n\n    # Add argument definitions\n    parser = argparse.ArgumentParser(\n        description='Script to run auto inference for RTS', formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n    parser.add_argument(\n        '--code_dir',\n        type=Path,\n        default=Path('/isipd/projects/p_aicore_pf/initze/code/aicore_inference'),\n        help='Local code directory',\n    )\n    parser.add_argument(\n        '--raw_data_dir',\n        type=Path,\n        nargs='+',\n        default=[\n            Path('/isipd/projects/p_aicore_pf/initze/data/planet/planet_data_inference_grid/scenes'),\n            Path('/isipd/projects/p_aicore_pf/initze/data/planet/planet_data_inference_grid/tiles'),\n        ],\n        help='Location of raw data',\n    )\n    parser.add_argument(\n        '--processing_dir',\n        type=Path,\n        default=Path('/isipd/projects/p_aicore_pf/initze/processing'),\n        help='Location for data processing',\n    )\n    parser.add_argument(\n        '--inference_dir',\n        type=Path,\n        default=Path('/isipd/projects/p_aicore_pf/initze/processed/inference'),\n        help='Target directory for inference results',\n    )\n    parser.add_argument(\n        '--model_dir',\n        type=Path,\n        default=Path('/isipd/projects/p_aicore_pf/initze/models/thaw_slumps'),\n        help='Target directory for models',\n    )\n    parser.add_argument(\n        '--model', type=str, default='RTS_v6_tcvis', help=\"Model name, examples ['RTS_v6_tcvis', 'RTS_v6_notcvis']\"\n    )\n    parser.add_argument('--use_gpu', nargs='+', type=int, default=[0], help='List of GPU IDs to use, space separated')\n    parser.add_argument('--runs_per_gpu', type=int, default=5, help='Number of runs per GPU')\n    parser.add_argument('--max_images', type=int, default=None, help='Maximum number of images to process (optional)')\n    parser.add_argument('--skip_vrt', action='store_true', help='set to skip DEM vrt creation')\n    parser.add_argument('--skip_vector_save', action='store_true', help='set to skip output vector creation')\n\n    # TODO, make flag to skip vrt\n    args = parser.parse_args()\n\n    process_02_inference(\n        code_dir=args.code_dir,\n        raw_data_dir=args.raw_data_dir,\n        processing_dir=args.processing_dir,\n        inference_dir=args.inference_dir,\n        model_dir=args.model_dir,\n        model=args.model,\n        use_gpu=args.use_gpu,\n        runs_per_gpu=args.runs_per_gpu,\n        max_images=args.max_images,\n        skip_vrt=args.skip_vrt,\n        skip_vector_save=args.skip_vector_save,\n    )\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/scripts/process_02_inference/#src.thaw_slump_segmentation.scripts.process_02_inference.process_02_inference","title":"<code>process_02_inference(code_dir=Path('/isipd/projects/p_aicore_pf/initze/code/aicore_inference'), raw_data_dir=[Path('/isipd/projects/p_aicore_pf/initze/data/planet/planet_data_inference_grid/scenes'), Path('/isipd/projects/p_aicore_pf/initze/data/planet/planet_data_inference_grid/tiles')], processing_dir=Path('/isipd/projects/p_aicore_pf/initze/processing'), inference_dir=Path('/isipd/projects/p_aicore_pf/initze/processed/inference'), model_dir=Path('/isipd/projects/p_aicore_pf/initze/models/thaw_slumps'), model='RTS_v6_tcvis', use_gpu=[0], runs_per_gpu=5, max_images=None, skip_vrt=False, skip_vector_save=False)</code>","text":"<p>Script to run auto inference for RTS</p> Source code in <code>src/thaw_slump_segmentation/scripts/process_02_inference.py</code> <pre><code>def process_02_inference(\n    code_dir: Annotated[Path, typer.Option(help='Local code directory')] = Path(\n        '/isipd/projects/p_aicore_pf/initze/code/aicore_inference'\n    ),\n    raw_data_dir: Annotated[List[Path], typer.Option(help='Location of raw data')] = [\n        Path('/isipd/projects/p_aicore_pf/initze/data/planet/planet_data_inference_grid/scenes'),\n        Path('/isipd/projects/p_aicore_pf/initze/data/planet/planet_data_inference_grid/tiles'),\n    ],\n    processing_dir: Annotated[Path, typer.Option(help='Location for data processing')] = Path(\n        '/isipd/projects/p_aicore_pf/initze/processing'\n    ),\n    inference_dir: Annotated[Path, typer.Option(help='Target directory for inference results')] = Path(\n        '/isipd/projects/p_aicore_pf/initze/processed/inference'\n    ),\n    model_dir: Annotated[Path, typer.Option(help='Target directory for models')] = Path(\n        '/isipd/projects/p_aicore_pf/initze/models/thaw_slumps'\n    ),\n    model: Annotated[\n        str, typer.Option(help=\"Model name, examples ['RTS_v6_tcvis', 'RTS_v6_notcvis']\")\n    ] = 'RTS_v6_tcvis',\n    use_gpu: Annotated[List[int], typer.Option(help='List of GPU IDs to use, space separated')] = [0],\n    runs_per_gpu: Annotated[int, typer.Option(help='Number of runs per GPU')] = 5,\n    max_images: Annotated[int, typer.Option(help='Maximum number of images to process (optional)')] = None,\n    skip_vrt: Annotated[bool, typer.Option(help='set to skip DEM vrt creation')] = False,\n    skip_vector_save: Annotated[bool, typer.Option(help='set to skip output vector creation')] = False,\n):\n    \"\"\"Script to run auto inference for RTS\"\"\"\n    # ### List all files with properties\n    # TODO: run double for both paths\n    print('Checking processing status!')\n    # read processing status for raw data list\n    # TODO: check here - produces very large output when double checking\n    df_processing_status_list = [\n        get_processing_status(rdd, processing_dir, inference_dir, model) for rdd in raw_data_dir\n    ]\n\n    # get df for preprocessing\n    df_final = pd.concat(df_processing_status_list).drop_duplicates()\n\n    # TODO: move to function\n    # print basic information\n    total_images = int(len(df_final))\n    preprocessed_images = int(df_final.preprocessed.sum())\n    preprocessing_images = int(total_images - preprocessed_images)\n    finished_images = int(df_final.inference_finished.sum())\n    print(f'Number of images: {total_images}')\n    print(f'Number of preprocessed images: {preprocessed_images}')\n    print(f'Number of images for preprocessing: {preprocessing_images}')\n    print(f'Number of images for inference: {preprocessed_images - finished_images}')\n    print(f'Number of finished images: {finished_images}')\n\n    # TODO: images with processing status True but Inference False are crappy\n\n    if total_images == finished_images:\n        print('No processing needed: all images are already processed!')\n        return 0\n\n    ## Preprocessing\n    # #### Update Arctic DEM data\n    if skip_vrt:\n        print('Skipping Elevation VRT creation!')\n    else:\n        print('Updating Elevation VRTs!')\n        dem_data_dir = Path('/isipd/projects/p_aicore_pf/initze/data/ArcticDEM')\n        vrt_target_dir = Path('/isipd/projects/p_aicore_pf/initze/processing/auxiliary/ArcticDEM')\n        update_DEM2(dem_data_dir=dem_data_dir, vrt_target_dir=vrt_target_dir)\n\n    # #### Copy data for Preprocessing\n    # make better documentation\n\n    df_preprocess = df_final[df_final.preprocessed == False]\n    print(f'Number of images to preprocess: {len(df_preprocess)}')\n\n    # TODO make better check\n    # Cleanup processing directories to avoid incomplete processing\n    input_dir_dslist = list((processing_dir / 'input').glob('*'))\n    if len(input_dir_dslist) &gt; 0:\n        print(f\"Cleaning up {(processing_dir / 'input')}\")\n        for d in input_dir_dslist:\n            if len(list(d.glob('*'))) &lt; 4:\n                print('Delete', d)\n                shutil.rmtree(d)\n    else:\n        print('Processing directory is ready, nothing to do!')\n\n    # Copy Data\n    _ = df_preprocess.swifter.apply(lambda x: copy_unprocessed_files(x, processing_dir), axis=1)\n\n    # #### Run Preprocessing\n    import warnings\n\n    warnings.filterwarnings('ignore')\n\n    N_JOBS = 40\n    print(f'Preprocessing {len(df_preprocess)} images')  # fix this\n    if len(df_preprocess) &gt; 0:\n        pp_string = f'setup_raw_data --data_dir {processing_dir} --n_jobs {N_JOBS} --nolabel'\n        os.system(pp_string)\n\n    # ## Processing/Inference\n    # rerun processing status\n    df_processing_status2 = pd.concat(\n        [get_processing_status(rdd, processing_dir, inference_dir, model) for rdd in raw_data_dir]\n    ).drop_duplicates()\n    # Filter to images that are not preprocessed yet\n    df_process = df_final[df_final.inference_finished == False]\n    # update overview and filter accordingly - really necessary?\n    df_process_final = (\n        df_process.set_index('name')\n        .join(df_processing_status2[df_processing_status2['preprocessed']][['name']].set_index('name'), how='inner')\n        .reset_index(drop=False)\n        .iloc[:max_images]\n    )\n    # validate if images are correctly preprocessed\n    df_process_final['preprocessing_valid'] = (\n        df_process_final.apply(lambda x: len(list(x['path'].glob('*'))), axis=1) &gt;= 5\n    )\n    # final filtering process to remove incorrectly preprocessed data\n    df_process_final = df_process_final[df_process_final['preprocessing_valid']]\n\n    # TODO: check for empty files and processing\n    print(f'Number of images: {len(df_process_final)}')\n\n    # #### Parallel runs\n    # Make splits to distribute the processing\n    n_splits = len(use_gpu) * runs_per_gpu\n    df_split = np.array_split(df_process_final, n_splits)\n    gpu_split = use_gpu * runs_per_gpu\n\n    # for split in df_split:\n    #    print(f'Number of images: {len(split)}')\n\n    print('Run inference!')\n    # ### Parallel Inference execution\n    _ = Parallel(n_jobs=n_splits)(\n        delayed(run_inference)(\n            df_split[split],\n            model=model,\n            processing_dir=processing_dir,\n            inference_dir=inference_dir,\n            model_dir=model_dir,\n            gpu=gpu_split[split],\n            run=True,\n        )\n        for split in range(n_splits)\n    )\n    # #### Merge output files\n\n    if not skip_vector_save:\n        if len(df_process_final) &gt; 0:\n            # read all files which following the above defined threshold\n            flist = list((inference_dir / model).glob('*/*pred_binarized.shp'))\n            len(flist)\n\n            # Save output vectors to merged file\n            # load them in parallel\n            out = Parallel(n_jobs=6)(delayed(load_and_parse_vector)(f) for f in tqdm(flist[:]))\n\n            # merge them and save to geopackage file\n            merged_gdf = gpd.pd.concat(out)\n            save_file = inference_dir / model / f'{model}_merged.gpkg'\n\n            # check if file already exists, create backup file if exists\n            if save_file.exists():\n                # Get the current timestamp\n                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n                # Create the backup file name\n                save_file_bk = inference_dir / model / f'{model}_merged_bk_{timestamp}.gpkg'\n                print(f'Creating backup of file {save_file} to {save_file_bk}')\n                shutil.move(save_file, save_file_bk)\n\n            # save to files\n            print(f'Saving vectors to {save_file}')\n            merged_gdf.to_file(save_file)\n    else:\n        print('Skipping output vector creation!')\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/scripts/process_03_ensemble/","title":"Process 03 ensemble","text":""},{"location":"reference/src/thaw_slump_segmentation/scripts/process_03_ensemble/#src.thaw_slump_segmentation.scripts.process_03_ensemble.main","title":"<code>main()</code>","text":"Source code in <code>src/thaw_slump_segmentation/scripts/process_03_ensemble.py</code> <pre><code>def main():\n    # Add argument definitions\n    parser = argparse.ArgumentParser(\n        description='Script to run auto inference for RTS', formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n    parser.add_argument(\n        '--raw_data_dir',\n        type=Path,\n        default=Path('/isipd/projects/p_aicore_pf/initze/data/planet/planet_data_inference_grid/tiles'),\n        help='Location of raw data',\n    )\n    parser.add_argument(\n        '--processing_dir',\n        type=Path,\n        default=Path('/isipd/projects/p_aicore_pf/initze/processing'),\n        help='Location for data processing',\n    )\n    parser.add_argument(\n        '--inference_dir',\n        type=Path,\n        default=Path('/isipd/projects/p_aicore_pf/initze/processed/inference'),\n        help='Target directory for inference results',\n    )\n    parser.add_argument(\n        '--model_dir',\n        type=Path,\n        default=Path('/isipd/projects/p_aicore_pf/initze/models/thaw_slumps'),\n        help='Target directory for models',\n    )\n    parser.add_argument('--ensemble_name', type=str, default='RTS_v6_ensemble_v2', help='Target directory for models')\n    parser.add_argument(\n        '--model_names',\n        type=str,\n        nargs='+',\n        default=['RTS_v6_tcvis', 'RTS_v6_notcvis'],\n        help=\"Model name, examples ['RTS_v6_tcvis', 'RTS_v6_notcvis']\",\n    )\n    parser.add_argument('--gpu', type=int, default=0, help='GPU IDs to use for edge cleaning')\n    parser.add_argument('--n_jobs', type=int, default=15, help='number of CPU jobs for ensembling')\n    parser.add_argument(\n        '--n_vector_loaders', type=int, default=6, help='number of parallel vector loaders for final merge'\n    )\n    parser.add_argument('--max_images', type=int, default=None, help='Maximum number of images to process (optional)')\n    parser.add_argument(\n        '--vector_output_format',\n        type=str,\n        nargs='+',\n        default=['gpkg', 'parquet'],\n        help='Output format extension of ensembled vector files',\n    )\n    parser.add_argument(\n        '--ensemble_thresholds',\n        type=float,\n        nargs='+',\n        default=[0.4, 0.45, 0.5],\n        help='Thresholds for polygonized outputs of the ensemble, needs to be string, see examples',\n    )\n    parser.add_argument(\n        '--ensemble_border_size', type=int, default=10, help='Number of pixels to remove around the border and no data'\n    )\n    parser.add_argument('--ensemble_mmu', type=int, default=32, help='Minimum mapping unit of output objects in pixels')\n    parser.add_argument('--save_binary', action='store_true', help='set to keep intermediate binary rasters')\n    parser.add_argument('--save_probability', action='store_true', help='set to keep intermediate probability rasters')\n    parser.add_argument('--try_gpu', action='store_true', help='set to try image processing with gpu')\n    parser.add_argument(\n        '--force_vector_merge',\n        action='store_true',\n        help='force merging of output vectors even if no new ensemble tiles were processed',\n    )\n    parser.add_argument('--filter_water', action='store_true', help='set to remove polygons over water')\n\n    args = parser.parse_args()\n\n    process_03_ensemble(\n        raw_data_dir=args.raw_data_dir,\n        processing_dir=args.processing_dir,\n        inference_dir=args.inference_dir,\n        model_dir=args.model_dir,\n        ensemble_name=args.ensemble_name,\n        model_names=args.model_names,\n        gpu=args.gpu,\n        n_jobs=args.n_jobs,\n        n_vector_loaders=args.n_vector_loaders,\n        max_images=args.max_images,\n        vector_output_format=args.vector_output_format,\n        ensemble_thresholds=args.ensemble_thresholds,\n        ensemble_border_size=args.ensemble_border_size,\n        ensemble_mmu=args.ensemble_mmu,\n        try_gpu=args.try_gpu,\n        force_vector_merge=args.force_vector_merge,\n        save_binary=args.save_binary,\n        save_probability=args.save_probability,\n        filter_water=args.filter_water,\n    )\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/scripts/process_03_ensemble/#src.thaw_slump_segmentation.scripts.process_03_ensemble.process_03_ensemble","title":"<code>process_03_ensemble(raw_data_dir=Path('/isipd/projects/p_aicore_pf/initze/data/planet/planet_data_inference_grid/tiles'), processing_dir=Path('/isipd/projects/p_aicore_pf/initze/processing'), inference_dir=Path('/isipd/projects/p_aicore_pf/initze/processed/inference'), model_dir=Path('/isipd/projects/p_aicore_pf/initze/models/thaw_slumps'), ensemble_name='RTS_v6_ensemble_v2', model_names=['RTS_v6_tcvis', 'RTS_v6_notcvis'], gpu=0, n_jobs=15, n_vector_loaders=6, max_images=None, vector_output_format=['gpkg', 'parquet'], ensemble_thresholds=[0.4, 0.45, 0.5], ensemble_border_size=10, ensemble_mmu=32, try_gpu=False, force_vector_merge=False, save_binary=False, save_probability=False, filter_water=False)</code>","text":"Source code in <code>src/thaw_slump_segmentation/scripts/process_03_ensemble.py</code> <pre><code>def process_03_ensemble(\n    raw_data_dir: Annotated[Path, typer.Option(help='Location of raw data')] = Path(\n        '/isipd/projects/p_aicore_pf/initze/data/planet/planet_data_inference_grid/tiles'\n    ),\n    processing_dir: Annotated[Path, typer.Option(help='Location for data processing')] = Path(\n        '/isipd/projects/p_aicore_pf/initze/processing'\n    ),\n    inference_dir: Annotated[Path, typer.Option(help='Target directory for inference results')] = Path(\n        '/isipd/projects/p_aicore_pf/initze/processed/inference'\n    ),\n    model_dir: Annotated[Path, typer.Option(help='Target directory for models')] = Path(\n        '/isipd/projects/p_aicore_pf/initze/models/thaw_slumps'\n    ),\n    ensemble_name: Annotated[str, typer.Option(help='Target directory for models')] = 'RTS_v6_ensemble_v2',\n    model_names: Annotated[List[str], typer.Option(help=\"Model name, examples ['RTS_v6_tcvis', 'RTS_v6_notcvis']\")] = [\n        'RTS_v6_tcvis',\n        'RTS_v6_notcvis',\n    ],\n    gpu: Annotated[int, typer.Option(help='GPU IDs to use for edge cleaning')] = 0,\n    n_jobs: Annotated[int, typer.Option(help='number of CPU jobs for ensembling')] = 15,\n    n_vector_loaders: Annotated[int, typer.Option(help='number of parallel vector loaders for final merge')] = 6,\n    max_images: Annotated[int, typer.Option(help='Maximum number of images to process (optional)')] = None,\n    vector_output_format: Annotated[\n        List[str], typer.Option(help='Output format extension of ensembled vector files')\n    ] = ['gpkg', 'parquet'],\n    ensemble_thresholds: Annotated[\n        List[float],\n        typer.Option(help='Thresholds for polygonized outputs of the ensemble, needs to be string, see examples'),\n    ] = [0.4, 0.45, 0.5],\n    ensemble_border_size: Annotated[\n        int, typer.Option(help='Number of pixels to remove around the border and no data')\n    ] = 10,\n    ensemble_mmu: Annotated[int, typer.Option(help='Minimum mapping unit of output objects in pixels')] = 32,\n    try_gpu: Annotated[bool, typer.Option(help='set to try image processing with gpu')] = False,\n    force_vector_merge: Annotated[\n        bool, typer.Option(help='force merging of output vectors even if no new ensemble tiles were processed')\n    ] = False,\n    save_binary: Annotated[bool, typer.Option(help='set to keep intermediate binary rasters')] = False,\n    save_probability: Annotated[bool, typer.Option(help='set to keep intermediate probability rasters')] = False,\n    filter_water: Annotated[bool, typer.Option(help='set to remove polygons over water')] = False,\n):\n    ### Start\n    # check if cucim is available\n    try:\n        import cucim  # type: ignore  # noqa: F401\n\n        if try_gpu:\n            try_gpu = True\n            print('Running ensembling with GPU!')\n        else:\n            try_gpu = False\n            print('Running ensembling with CPU!')\n    except Exception as e:\n        try_gpu = False\n        print(f'Cucim import failed: {e}')\n\n    # setup all params\n    kwargs_ensemble = {\n        'ensemblename': ensemble_name,\n        'inference_dir': inference_dir,\n        'modelnames': model_names,\n        'binary_threshold': ensemble_thresholds,\n        'border_size': ensemble_border_size,\n        'minimum_mapping_unit': ensemble_mmu,\n        'save_binary': save_binary,\n        'save_probability': save_probability,\n        'try_gpu': try_gpu,  # currently default to CPU only\n        'gpu': gpu,\n    }\n\n    # Check for finalized products\n    get_processing_status(raw_data_dir, processing_dir, inference_dir, model=kwargs_ensemble['ensemblename'])\n    df_ensemble_status = get_processing_status_ensemble(\n        inference_dir,\n        model_input_names=kwargs_ensemble['modelnames'],\n        model_ensemble_name=kwargs_ensemble['ensemblename'],\n    )\n    # Check which need to be process - check for already processed and invalid files\n    process = df_ensemble_status[df_ensemble_status['process']]\n    n_images = len(process.iloc[:max_images])\n    # #### Run Ensemble Merging\n    if len(process) &gt; 0:\n        print(f'Start running ensemble for {n_images} images with {n_jobs} parallel jobs!')\n        print(f'Target ensemble name: {ensemble_name}')\n        print(f'Source model output {model_names}')\n        _ = Parallel(n_jobs=n_jobs)(\n            delayed(create_ensemble_v2)(image_id=process.iloc[row]['name'], **kwargs_ensemble)\n            for row in tqdm(range(n_images))\n        )\n    else:\n        print(f'Skipped ensembling, all files ready for {ensemble_name}!')\n\n    # # #### run parallelized batch\n\n    if (len(process) &gt; 0) or force_vector_merge:\n        # ### Merge vectors to complete dataset\n        # set probability levels: 'class_05' means 50%, 'class_045' means 45%. This is the regex to search for vector naming\n        # proba_strings = args.ensemble_thresholds\n        # TODO: needs to be 'class_04',\n        proba_strings = [f'class_{thresh}'.replace('.', '') for thresh in ensemble_thresholds]\n\n        for proba_string in proba_strings:\n            # read all files which follow the above defined threshold\n            flist = list((inference_dir / ensemble_name).glob(f'*/*_{proba_string}.gpkg'))\n            len(flist)\n            # load them in parallel\n            print(f'Loading results {proba_string}')\n            out = Parallel(n_jobs=6)(\n                delayed(load_and_parse_vector)(f, filter_water=filter_water) for f in tqdm(flist[:max_images])\n            )\n            # merge them and save to geopackage file\n            print('Merging results')\n            merged_gdf = gpd.pd.concat(out)\n\n            # if filter_water:\n            #     # water removal here\n            #     merged_gdf = filter_remove_water(merged_gdf)\n\n            for vector_format in vector_output_format:\n                # Save output to vector\n                save_file = inference_dir / ensemble_name / f'merged_{proba_string}.{vector_format}'\n\n                # make file backup if necessary\n                if save_file.exists():\n                    # Get the current timestamp\n                    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n                    # Create the backup file name\n                    save_file_bk = (\n                        inference_dir / ensemble_name / f'merged_{proba_string}_bk_{timestamp}.{vector_format}'\n                    )\n                    print(f'Creating backup of file {save_file} to {save_file_bk}')\n                    shutil.move(save_file, save_file_bk)\n\n                # save to files\n                print(f'Saving vectors to {save_file}')\n                if vector_format in ['shp', 'gpkg']:\n                    merged_gdf.to_file(save_file)\n                elif vector_format in ['parquet']:\n                    merged_gdf.to_parquet(save_file)\n                else:\n                    print(f'Unknown format {vector_format}!')\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/scripts/setup_raw_data/","title":"Setup raw data","text":"<p>Usecase 2 Data Preprocessing Script</p>"},{"location":"reference/src/thaw_slump_segmentation/scripts/setup_raw_data/#src.thaw_slump_segmentation.scripts.setup_raw_data.STATUS","title":"<code>STATUS = {0: 'failed', 1: 'success', 2: 'skipped'}</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/scripts/setup_raw_data/#src.thaw_slump_segmentation.scripts.setup_raw_data.SUCCESS_STATES","title":"<code>SUCCESS_STATES = ['rename', 'label', 'ndvi', 'tcvis', 'rel_dem', 'slope', 'mask', 'move']</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/scripts/setup_raw_data/#src.thaw_slump_segmentation.scripts.setup_raw_data.is_ee_initialized","title":"<code>is_ee_initialized = False</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/scripts/setup_raw_data/#src.thaw_slump_segmentation.scripts.setup_raw_data.main","title":"<code>main()</code>","text":"Source code in <code>src/thaw_slump_segmentation/scripts/setup_raw_data.py</code> <pre><code>def main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--gdal_bin', default=None, help='Path to gdal binaries (ignored if --skip_gdal is passed)')\n    parser.add_argument('--gdal_path', default=None, help='Path to gdal scripts (ignored if --skip_gdal is passed)')\n    parser.add_argument('--n_jobs', default=-1, type=int, help='number of parallel joblib jobs')\n    parser.add_argument('--nolabel', action='store_false', help='Set flag to do preprocessing without label file')\n    parser.add_argument('--data_dir', default='data', type=Path, help='Path to data processing dir')\n    parser.add_argument('--log_dir', default='logs', type=Path, help='Path to log dir')\n\n    args = parser.parse_args()\n\n    setup_raw_data(\n        gdal_bin=args.gdal_bin,\n        gdal_path=args.gdal_path,\n        n_jobs=args.n_jobs,\n        label=args.nolabel,\n        data_dir=args.data_dir,\n        log_dir=args.log_dir,\n    )\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/scripts/setup_raw_data/#src.thaw_slump_segmentation.scripts.setup_raw_data.preprocess_directory","title":"<code>preprocess_directory(image_dir, data_dir, aux_dir, backup_dir, log_path, gdal_bin, gdal_path, label_required=True)</code>","text":"<p>Preprocess a folder of a source imagery dataset to use in the inference processing. Notably this method generates additional data files to be used as auxiliary input of the inference model: DEM derived data (relative elevation and slope), NDVI and the landsat trend data (tcvis).</p> <p>In particular, the extend of the imagery in image_dir is used to generate a regional excerpt of the other data from the aux_dir as well as Google Earth Engine and the input data itself. The regional excerpts are stored as GeoTIFF alongside the source data in the data_dir folder.</p> <p>Additionally, some data is copied to <code>backup_dir</code>.</p> <p>GDAL is heavily used in the preprocessing, so the paths to a working gdal environment are required,  usually the platform binaries or installed via conda.</p> <p>To accesss data in Google Earth Engine this method will authenticate with the Google Earth Engine API, so an active account at GEE is required.</p> <p>The method returns a dictionary describing the success states of each processing stage. The keys are: * rename * label * ndvi * tcvis * rel_dem * slope * mask * move</p> <p>Parameters:</p> Name Type Description Default <code>image_dir</code> <code>Path</code> <p>The source folder</p> required <code>data_dir</code> <code>Path</code> <p>The folder where to store the preprocessed dataset</p> required <code>aux_dir</code> <code>Path</code> <p>source of auxiliary data, for now the DEM VRT files</p> required <code>backup_dir</code> <code>Path</code> <p>path to a folder where to store backups of the input data</p> required <code>log_path</code> <code>Path</code> <p>path where to write logging</p> required <code>gdal_bin</code> <code>Path</code> <p>Path to a folder with the gdal_binaries (gdalwarp etc.)</p> required <code>gdal_path</code> <code>_type_</code> <p>Path to a folder with gdal_scripts (gdal_retile.py etc)</p> required <code>label_required</code> <code>bool</code> <p>If label data should be baked into the source data. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>a dictionary with the results of the individual processing stages</p> Source code in <code>src/thaw_slump_segmentation/scripts/setup_raw_data.py</code> <pre><code>def preprocess_directory(image_dir, data_dir, aux_dir, backup_dir, log_path, gdal_bin, gdal_path, label_required=True):\n    \"\"\"Preprocess a folder of a source imagery dataset to use in the inference processing.\n    Notably this method generates additional data files to be used as auxiliary input of the inference\n    model: DEM derived data (relative elevation and slope), NDVI and the landsat trend data (tcvis).\n\n    In particular, the extend of the imagery in image_dir is used to generate a regional excerpt of the\n    other data from the aux_dir as well as Google Earth Engine and the input data itself. The regional\n    excerpts are stored as GeoTIFF alongside the source data in the data_dir folder.\n\n    Additionally, some data is copied to `backup_dir`.\n\n    GDAL is heavily used in the preprocessing, so the paths to a working gdal environment are required, \n    usually the platform binaries or installed via conda.\n\n    To accesss data in Google Earth Engine this method will authenticate with the Google Earth Engine API,\n    so an active account at GEE is required.\n\n    The method returns a dictionary describing the success states of each processing stage. The keys are:\n    * rename\n    * label\n    * ndvi\n    * tcvis\n    * rel_dem\n    * slope\n    * mask\n    * move\n\n    Args:\n        image_dir (Path): The source folder\n        data_dir (Path): The folder where to store the preprocessed dataset\n        aux_dir (Path): source of auxiliary data, for now the DEM VRT files\n        backup_dir (Path): path to a folder where to store backups of the input data\n        log_path (Path): path where to write logging\n        gdal_bin (Path): Path to a folder with the gdal_binaries (gdalwarp etc.)\n        gdal_path (_type_): Path to a folder with gdal_scripts (gdal_retile.py etc)\n        label_required (bool, optional): If label data should be baked into the source data. Defaults to True.\n\n    Returns:\n        dict: a dictionary with the results of the individual processing stages\n    \"\"\"\n\n    # Mock old args object\n    gdal.initialize(bin=gdal_bin, path=gdal_path)\n\n    init_logging(log_path)\n    image_name = os.path.basename(image_dir)\n    thread_logger = get_logger(f'setup_raw_data.{image_name}')\n    data_pre_processing.earthengine._logger = get_logger(f'setup_raw_data.{image_name}.ee')\n    data_pre_processing.utils._logger = get_logger(f'setup_raw_data.{image_name}.utils')\n\n    global is_ee_initialized\n    if not is_ee_initialized:\n        try:\n            thread_logger.debug('Initializing Earth Engine')\n            ee.Initialize()\n        except Exception:\n            thread_logger.warn('Initializing Earth Engine failed, trying to authenticate')\n            ee.Authenticate()\n            ee.Initialize()\n        is_ee_initialized = True\n    success_state = dict(rename=0, label=0, ndvi=0, tcvis=0, rel_dem=0, slope=0, mask=0, move=0)\n    thread_logger.info(f'Starting preprocessing {image_name}')\n\n    pre_cleanup(image_dir)\n\n    success_state['rename'] = rename_clip_to_standard(image_dir)\n\n    if not has_projection(image_dir):\n        thread_logger.error('Input File has no valid Projection!')\n        return\n\n    if label_required:\n        success_state['label'] = vector_to_raster_mask(image_dir)\n    else:\n        success_state['label'] = 2\n\n    success_state['ndvi'] = make_ndvi_file(image_dir)\n\n    ee_image_tcvis = ee.ImageCollection('users/ingmarnitze/TCTrend_SR_2000-2019_TCVIS').mosaic()\n    success_state['tcvis'] = get_tcvis_from_gee(image_dir, ee_image_tcvis, out_filename='tcvis.tif', resolution=3)\n\n    success_state['rel_dem'] = aux_data_to_tiles(\n        image_dir, aux_dir / 'ArcticDEM' / 'elevation.vrt', 'relative_elevation.tif'\n    )\n\n    success_state['slope'] = aux_data_to_tiles(image_dir, aux_dir / 'ArcticDEM' / 'slope.vrt', 'slope.tif')\n\n    success_state['mask'] = mask_input_data(image_dir, data_dir)\n\n    # backup_dir_full = os.path.join(backup_dir, os.path.basename(image_dir))\n    if backup_dir is not None:\n        backup_dir_full = backup_dir / image_dir.name\n        success_state['move'] = move_files(image_dir, backup_dir_full)\n\n    for status in SUCCESS_STATES:\n        thread_logger.info(status + ': ' + STATUS[success_state[status]])\n    return success_state\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/scripts/setup_raw_data/#src.thaw_slump_segmentation.scripts.setup_raw_data.setup_raw_data","title":"<code>setup_raw_data(gdal_bin='/usr/bin', gdal_path='/usr/bin', n_jobs=-1, label=False, data_dir=Path('data'), log_dir=Path('logs'))</code>","text":"Source code in <code>src/thaw_slump_segmentation/scripts/setup_raw_data.py</code> <pre><code>def setup_raw_data(\n    gdal_bin: Annotated[str, typer.Option('--gdal_bin', help='Path to gdal binaries', envvar='GDAL_BIN')] = '/usr/bin',\n    gdal_path: Annotated[\n        str, typer.Option('--gdal_path', help='Path to gdal scripts', envvar='GDAL_PATH')\n    ] = '/usr/bin',\n    n_jobs: Annotated[int, typer.Option('--n_jobs', help='number of parallel joblib jobs, pass 0 to disable joblib')] = -1,\n    label: Annotated[\n        bool, typer.Option('--label/--nolabel', help='Set flag to do preprocessing with label file')\n    ] = False,\n    data_dir: Annotated[Path, typer.Option('--data_dir', help='Path to data processing dir')] = Path('data'),\n    log_dir: Annotated[Path, typer.Option('--log_dir', help='Path to log dir')] = Path('logs'),\n):\n    INPUT_DATA_DIR = data_dir / 'input'\n    BACKUP_DIR = data_dir / 'backup'\n    DATA_DIR = data_dir / 'tiles'\n    AUX_DIR = data_dir / 'auxiliary'\n\n    timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n    log_path = Path(log_dir) / f'setup_raw_data-{timestamp}.log'\n    if not Path(log_dir).exists():\n        os.mkdir(Path(log_dir))\n    init_logging(log_path)\n    logger = get_logger('setup_raw_data')\n    logger.info('###########################')\n    logger.info('# Starting Raw Data Setup #')\n    logger.info('###########################')\n\n    logger.info(f\"images to preprocess in {INPUT_DATA_DIR}\")\n    dir_list = check_input_data(INPUT_DATA_DIR)\n    [logger.info(f\" * '{f.relative_to(INPUT_DATA_DIR)}'\") for f in dir_list ]\n    if len(dir_list) &gt; 0:\n        if n_jobs == 0:\n            for image_dir in dir_list:\n                preprocess_directory(image_dir, DATA_DIR, AUX_DIR, BACKUP_DIR, log_path, gdal_bin, gdal_path, label)\n        else:\n            Parallel(n_jobs=n_jobs)(\n                delayed(preprocess_directory)(\n                    image_dir, DATA_DIR, AUX_DIR, BACKUP_DIR, log_path, gdal_bin, gdal_path, label\n                )\n                for image_dir in dir_list\n        )\n    else:\n        logger.error('Empty Input Data Directory! No Data available to process!')\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/scripts/train/","title":"Train","text":"<p>Usecase 2 Training Script</p>"},{"location":"reference/src/thaw_slump_segmentation/scripts/train/#src.thaw_slump_segmentation.scripts.train.args","title":"<code>args = parser.parse_args()</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/scripts/train/#src.thaw_slump_segmentation.scripts.train.parser","title":"<code>parser = argparse.ArgumentParser(description='Training script', formatter_class=argparse.ArgumentDefaultsHelpFormatter)</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/scripts/train/#src.thaw_slump_segmentation.scripts.train.Engine","title":"<code>Engine</code>","text":"Source code in <code>src/thaw_slump_segmentation/scripts/train.py</code> <pre><code>class Engine:\n    def __init__(\n        self,\n        config: Path,\n        data_dir: Path,\n        name: str,\n        log_dir: Path,\n        resume: str,\n        summary: bool,\n        wandb_project: str,\n        wandb_name: str,\n    ):\n        self.config = yaml.load(config.open(), Loader=yaml_custom.SaneYAMLLoader)\n        self.DATA_ROOT = data_dir\n        # Logging setup\n        timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n        if name:\n            log_dir_name = f'{name}_{timestamp}'\n        else:\n            log_dir_name = timestamp\n        self.log_dir = Path(log_dir) / log_dir_name\n        self.log_dir.mkdir(exist_ok=False)\n\n        init_logging(self.log_dir / 'train.log')\n        self.logger = get_logger('train')\n\n        self.data_sources = DataSources(self.config['data_sources'])\n        self.config['model']['input_channels'] = sum(src.channels for src in self.data_sources)\n\n        m = self.config['model']\n        self.model = create_model(\n            arch=m['architecture'],\n            encoder_name=m['encoder'],\n            encoder_weights=None if m['encoder_weights'] == 'random' else m['encoder_weights'],\n            classes=1,\n            in_channels=m['input_channels'],\n        )\n\n        # make parallel\n        self.model = nn.DataParallel(self.model)\n\n        if resume:\n            self.config['resume'] = resume\n\n        if 'resume' in self.config and self.config['resume']:\n            checkpoint = Path(self.config['resume'])\n            if not checkpoint.exists():\n                raise ValueError(f\"There is no Checkpoint at {self.config['resume']} to resume from!\")\n            if checkpoint.is_dir():\n                # Load last checkpoint in run dir\n                ckpt_nums = [int(ckpt.stem) for ckpt in checkpoint.glob('checkpoints/*.pt')]\n                last_ckpt = max(ckpt_nums)\n                self.config['resume'] = str(checkpoint / 'checkpoints' / f'{last_ckpt:02d}.pt')\n            self.logger.info(f\"Resuming training from checkpoint {self.config['resume']}\")\n            self.model.load_state_dict(torch.load(self.config['resume']))\n\n        self.dev = torch.device('cpu') if not torch.cuda.is_available() else torch.device('cuda')\n        self.logger.info(f'Training on {self.dev} device')\n\n        self.model = self.model.to(self.dev)\n        self.model = torch.compile(self.model, mode='max-autotune', fullgraph=True)\n\n        self.learning_rate = self.config['learning_rate']\n        self.opt = torch.optim.AdamW(self.model.parameters(), lr=self.learning_rate)\n        self.setup_lr_scheduler()\n\n        self.board_idx = 0\n        self.epoch = 0\n\n        if summary:\n            from torchsummary import summary\n\n            summary(self.model, [(self.config['model']['input_channels'], 256, 256)])\n            sys.exit(0)\n\n        self.dataset_cache = {}\n\n        self.vis_predictions = None\n        self.vis_loader, self.vis_names = get_vis_loader(\n            self.config['visualization_tiles'],\n            batch_size=self.config['batch_size'],\n            data_sources=self.data_sources,\n            data_root=self.DATA_ROOT,\n        )\n\n        # Write the config YML to the run-folder\n        self.config['run_info'] = {\n            'timestamp': timestamp,\n            'git_head': subprocess.check_output(['git', 'describe'], encoding='utf8').strip(),\n        }\n        with open(self.log_dir / 'config.yml', 'w') as f:\n            yaml.dump(self.config, f)\n\n        self.checkpoints = self.log_dir / 'checkpoints'\n        self.checkpoints.mkdir()\n\n        # Weights and Biases initialization\n        print('wandb project:', wandb_project)\n        print('wandb name:', wandb_name)\n        print('config:', self.config)\n        print('entity:', 'ml4earth')\n        wandb.init(project=wandb_project, name=wandb_name, config=self.config, entity='ingmarnitze_team')\n\n    def run(self):\n        for phase in self.config['schedule']:\n            self.logger.info(f'Starting phase \"{phase[\"phase\"]}\"')\n\n            self.phase_should_log_images = phase.get('log_images', False)\n            self.setup_metrics(phase)\n            self.current_key = None\n            for epoch in range(phase['epochs']):\n                # Epoch setup\n                self.epoch = epoch\n                self.loss_function = create_loss(scoped_get('loss_function', phase, self.config)).to(self.dev)\n\n                for step in phase['steps']:\n                    command, key = parse_step(step)\n                    self.current_key = key\n\n                    if command == 'train_on':\n                        # Training step\n                        data_loader = self.get_dataloader(key)\n                        self.train_epoch(data_loader)\n                    elif command == 'validate_on':\n                        # Validation step\n                        data_loader = self.get_dataloader(key)\n                        self.val_epoch(data_loader)\n                    elif command == 'log_images':\n                        self.logger.warn(\n                            \"Step 'log_images' is deprecated. Please use 'log_images' in the phase instead.\"\n                        )\n                        continue\n                    else:\n                        raise ValueError(f\"Unknown command '{command}'\")\n\n                    # Reset the metrics after each step\n                    if self.current_key:\n                        self.metrics[self.current_key].reset()\n                        gc.collect()\n                        torch.cuda.empty_cache()\n\n                # Log images after each epoch\n                if self.phase_should_log_images:\n                    self.log_images()\n\n                if self.scheduler:\n                    print('before step:', self.scheduler.get_last_lr())\n                    self.scheduler.step()\n                    print('after step:', self.scheduler.get_last_lr())\n\n    def setup_metrics(self, phase: dict):\n        # Setup Metrics for this phase\n        # Check if the phase has a log_images command -&gt; Relevant for memory management of metrics (PRC, ROC and Confusion Matrix)\n        nthresholds = (\n            100  # Make sure that the thresholds for the PRC-based metrics are equal to benefit from grouped computing\n        )\n        # Make sure that the matching args are the same for all instance metrics\n        matching_threshold = 0.5\n        matching_metric = 'iou'\n        boundary_dilation = 0.02\n\n        metrics = MetricCollection(\n            {\n                'Accuracy': Accuracy(task='binary', validate_args=False),\n                'Precision': Precision(task='binary', validate_args=False),\n                'Specificity': Specificity(task='binary', validate_args=False),\n                'Recall': Recall(task='binary', validate_args=False),\n                'F1Score': F1Score(task='binary', validate_args=False),\n                'JaccardIndex': JaccardIndex(task='binary', validate_args=False),\n                'AUROC': AUROC(task='binary', thresholds=nthresholds, validate_args=False),\n                'AveragePrecision': AveragePrecision(task='binary', thresholds=nthresholds, validate_args=False),\n                # Calibration errors: https://arxiv.org/abs/1909.10155, they take a lot of memory!\n                #'ExpectedCalibrationError': CalibrationError(task='binary', norm='l1'),\n                #'RootMeanSquaredCalibrationError': CalibrationError(task='binary', norm='l2'),\n                #'MaximumCalibrationError': CalibrationError(task='binary', norm='max'),\n                'CohenKappa': CohenKappa(task='binary', validate_args=False),\n                'HammingDistance': HammingDistance(task='binary', validate_args=False),\n                'HingeLoss': HingeLoss(task='binary', validate_args=False),\n                # MCC raised a weired error one time, skipping to be save\n                # 'MatthewsCorrCoef': MatthewsCorrCoef(task='binary'),\n            }\n        )\n        self.metrics = {}\n        self.rocs = {}\n        self.prcs = {}\n        self.confmats = {}\n        self.instance_prcs = {}\n        self.instance_confmats = {}\n\n        if self.phase_should_log_images:\n            self.metric_tracker = defaultdict(list)\n\n        for step in phase['steps']:\n            command, key = parse_step(step)\n            if not key:\n                continue\n\n            self.metrics[key] = metrics.clone(f'{key}/').to(self.dev)\n            if command == 'validate_on':\n                # We don't want to log the confusion matrix, PRC and ROC curve for every step in the training loop\n                # We assign them seperately to have easy access to them in the log_images phase but still benefit from the MetricCollection in terms of compute_groups and update, compute &amp; reset calls\n                self.rocs[key] = ROC(task='binary', thresholds=nthresholds, validate_args=False).to(self.dev)\n                self.prcs[key] = PrecisionRecallCurve(task='binary', thresholds=nthresholds, validate_args=False).to(\n                    self.dev\n                )\n                self.confmats[key] = ConfusionMatrix(task='binary', normalize='true', validate_args=False).to(self.dev)\n                self.instance_prcs[key] = BinaryInstancePrecisionRecallCurve(\n                    thresholds=nthresholds,\n                    matching_threshold=matching_threshold,\n                    matching_metric=matching_metric,\n                    validate_args=False,\n                ).to(self.dev)\n                self.instance_confmats[key] = BinaryInstanceConfusionMatrix(\n                    matching_threshold=matching_threshold, matching_metric=matching_metric, validate_args=False\n                ).to(self.dev)\n                self.metrics[key].add_metrics(\n                    {\n                        'ROC': self.rocs[key],\n                        'PRC': self.prcs[key],\n                        'ConfusionMatrix': self.confmats[key],\n                        'Instance-PRC': self.instance_prcs[key],\n                        'Instance-ConfusionMatrix': self.instance_confmats[key],\n                    }\n                )\n                # We don't want to log the instance metrics for every step in the training loop, because they are memory intensive (and not very useful for training)\n                self.metrics[key].add_metrics(\n                    {\n                        'Instance-Accuracy': BinaryInstanceAccuracy(\n                            matching_threshold=matching_threshold, matching_metric=matching_metric, validate_args=False\n                        ).to(self.dev),\n                        'Instance-Precision': BinaryInstancePrecision(\n                            matching_threshold=matching_threshold, matching_metric=matching_metric, validate_args=False\n                        ).to(self.dev),\n                        'Instance-Recall': BinaryInstanceRecall(\n                            matching_threshold=matching_threshold, matching_metric=matching_metric, validate_args=False\n                        ).to(self.dev),\n                        'Instance-F1Score': BinaryInstanceF1Score(\n                            matching_threshold=matching_threshold, matching_metric=matching_metric, validate_args=False\n                        ).to(self.dev),\n                        'Instance-AveragePrecision': BinaryInstanceAveragePrecision(\n                            thresholds=nthresholds,\n                            matching_threshold=matching_threshold,\n                            matching_metric=matching_metric,\n                            validate_args=False,\n                        ).to(self.dev),\n                        'BoundaryIoU': BinaryBoundaryIoU(dilation=boundary_dilation, validate_args=False).to(self.dev),\n                    }\n                )\n\n        # Create the plot directory\n        if self.phase_should_log_images:\n            phase_name = phase['phase'].lower()\n            self.metric_plot_dir = self.log_dir / f'metrics_plots_{phase_name}'\n            self.metric_plot_dir.mkdir(exist_ok=True)\n\n    def get_dataloader(self, name):\n        if name in self.dataset_cache:\n            return self.dataset_cache[name]\n\n        if name in self.config['datasets']:\n            ds_config = self.config['datasets'][name]\n            if 'batch_size' not in ds_config:\n                ds_config['batch_size'] = self.config['batch_size']\n            ds_config['num_workers'] = self.config['data_threads']\n            ds_config['augment_types'] = self.config['datasets']\n            ds_config['data_sources'] = self.data_sources\n            ds_config['data_root'] = self.DATA_ROOT\n            self.dataset_cache[name] = get_loader(**ds_config)\n        else:\n            func, arg = re.search(r'(\\w+)\\((\\w+)\\)', name).groups()\n            if func == 'slump_tiles':\n                ds_config = self.config['datasets'][arg]\n                if 'batch_size' not in ds_config:\n                    ds_config['batch_size'] = self.config['batch_size']\n                ds_config['num_workers'] = self.config['data_threads']\n                ds_config['data_sources'] = self.data_sources\n                ds_config['data_root'] = self.DATA_ROOT\n                self.dataset_cache[name] = get_slump_loader(**ds_config)\n\n        return self.dataset_cache[name]\n\n    def train_epoch(self, train_loader):\n        self.logger.info(f'Epoch {self.epoch} - Training Started')\n        progress = tqdm(train_loader)\n        self.model.train(True)\n        epoch_loss = {\n            'Loss': [],\n            'Deep Supervision Loss': [],\n        }\n        for iteration, (img, target) in enumerate(progress):\n            self.board_idx += img.shape[0]\n\n            img = img.to(self.dev, torch.float)\n            target = target.to(self.dev, torch.long, non_blocking=True)\n\n            self.opt.zero_grad()\n            y_hat = self.model(img)\n\n            if isinstance(y_hat, (tuple, list)):\n                # Deep Supervision\n                deep_super_losses = [self.loss_function(pred.squeeze(1), target) for pred in y_hat]\n                y_hat = y_hat[0].squeeze(1)\n                loss = sum(deep_super_losses)\n                for dsl in deep_super_losses:\n                    epoch_loss['Loss'].append(dsl.detach())\n                epoch_loss['Deep Supervision Loss'].append(loss.detach())\n            else:\n                loss = self.loss_function(y_hat, target)\n                epoch_loss['Loss'].append(loss.detach())\n\n            loss.backward()\n            self.opt.step()\n\n            with torch.no_grad():\n                self.metrics[self.current_key].update(y_hat.squeeze(1), target)\n\n        # Compute epochs loss and metrics\n        epoch_loss = {k: torch.stack(v).mean().item() for k, v in epoch_loss.items() if v}\n        wandb.log({'train/Loss': epoch_loss}, step=self.board_idx)\n        epoch_metrics = self.log_metrics()\n        self.logger.info(f'Epoch {self.epoch} - Loss {epoch_loss} Training Metrics: {epoch_metrics}')\n        self.log_csv(epoch_metrics, epoch_loss)\n\n        # Update progress bar\n        progress.set_postfix(epoch_metrics)\n\n        # Save model Checkpoint\n        torch.save(self.model.state_dict(), self.checkpoints / f'{self.epoch:02d}.pt')\n\n    def val_epoch(self, val_loader):\n        self.logger.info(f'Epoch {self.epoch} - Validation of {self.current_key} Started')\n        self.model.train(False)\n        with torch.no_grad():\n            epoch_loss = []\n            for iteration, (img, target) in enumerate(val_loader):\n                img = img.to(self.dev, torch.float)\n                target = target.to(self.dev, torch.long, non_blocking=True)\n                y_hat = self.model(img).squeeze(1)\n\n                loss = self.loss_function(y_hat, target)\n                epoch_loss.append(loss.detach())\n                self.metrics[self.current_key].update(y_hat, target)\n\n            # Compute epochs loss and metrics\n            epoch_loss = torch.stack(epoch_loss).mean().item()\n            epoch_metrics = self.log_metrics()\n        wandb.log({'val/Loss': epoch_loss}, step=self.board_idx)\n        self.logger.info(f'Epoch {self.epoch} - Loss {epoch_loss} Validation Metrics: {epoch_metrics}')\n        self.log_csv(epoch_metrics, epoch_loss)\n\n        # Plot roc, prc and confusion matrix to disk and wandb\n        fig_roc, _ = self.rocs[self.current_key].plot(score=True)\n        fig_prc, _ = self.prcs[self.current_key].plot(score=True)\n        fig_confmat, _ = self.confmats[self.current_key].plot(cmap='Blues')\n        fig_instance_prc, _ = self.instance_prcs[self.current_key].plot(score=True)\n        fig_instance_confmat, _ = self.instance_confmats[self.current_key].plot(cmap='Blues')\n\n        # We need to wrap the figures into a wandb.Image to save storage -&gt; Maybe we can find a better solution in the future, e.g. using plotly\n        wandb.log({f'{self.current_key}/ROC': wandb.Image(fig_roc)}, step=self.board_idx)\n        wandb.log({f'{self.current_key}/PRC': wandb.Image(fig_prc)}, step=self.board_idx)\n        wandb.log({f'{self.current_key}/Confusion Matrix': wandb.Image(fig_confmat)}, step=self.board_idx)\n        wandb.log({f'{self.current_key}/Instance-PRC': wandb.Image(fig_instance_prc)}, step=self.board_idx)\n        wandb.log(\n            {f'{self.current_key}/Instance-Confusion Matrix': wandb.Image(fig_instance_confmat)}, step=self.board_idx\n        )\n\n        if self.phase_should_log_images:\n            fig_roc.savefig(self.metric_plot_dir / f'{self.current_key}_roc_curve.png')\n            fig_prc.savefig(self.metric_plot_dir / f'{self.current_key}_precision_recall_curve.png')\n            fig_confmat.savefig(self.metric_plot_dir / f'{self.current_key}_confusion_matrix.png')\n            fig_instance_prc.savefig(self.metric_plot_dir / f'{self.current_key}_instance_precision_recall_curve.png')\n            fig_instance_confmat.savefig(self.metric_plot_dir / f'{self.current_key}_instance_confusion_matrix.png')\n        fig_roc.clear()\n        fig_prc.clear()\n        fig_confmat.clear()\n        fig_instance_prc.clear()\n        fig_instance_confmat.clear()\n        plt.close('all')\n\n    def log_metrics(self) -&gt; dict:\n        epoch_metrics = self.metrics[self.current_key].compute()\n        # We need to filter our ROC, PRC and Confusion Matrix from the metrics because they are not scalar metrics\n\n        scalar_epoch_metrics = {\n            k: v.item() for k, v in epoch_metrics.items() if isinstance(v, torch.Tensor) and v.numel() == 1\n        }\n\n        # Log to WandB\n        wandb.log(scalar_epoch_metrics, step=self.board_idx)\n\n        # Plot the metrics to disk\n        if self.phase_should_log_images:\n            scalar_epoch_metrics_tensor = {\n                k: v for k, v in epoch_metrics.items() if isinstance(v, torch.Tensor) and v.numel() == 1\n            }\n            self.metric_tracker[self.current_key].append(scalar_epoch_metrics_tensor)\n            fig, ax = plt.subplots(figsize=(20, 10))\n            self.metrics[self.current_key].plot(self.metric_tracker[self.current_key], together=True, ax=ax)\n            fig.savefig(self.metric_plot_dir / f'{self.current_key}_metrics.png')\n            fig.clear()\n            plt.close('all')\n\n        return scalar_epoch_metrics\n\n    def log_csv(self, epoch_metrics: dict, epoch_loss: int | dict):\n        # Log to CSV\n        logfile = self.log_dir / f'{self.current_key}.csv'\n\n        if isinstance(epoch_loss, dict):\n            logstr = (\n                f'{self.epoch},'\n                + ','.join(f'{val}' for key, val in epoch_metrics.items())\n                + ','\n                + ','.join(f'{val}' for key, val in epoch_loss.items())\n            )\n        else:\n            logstr = f'{self.epoch},' + ','.join(f'{val}' for key, val in epoch_metrics.items()) + f',{epoch_loss}'\n\n        if not logfile.exists():\n            # Print header upon first log print\n            header = 'Epoch,' + ','.join(f'{key}' for key, val in epoch_metrics.items()) + ',Loss'\n            with logfile.open('w') as f:\n                print(header, file=f)\n                print(logstr, file=f)\n        else:\n            with logfile.open('a') as f:\n                print(logstr, file=f)\n\n    def log_images(self):\n        self.logger.debug(f'Epoch {self.epoch} - Image Logging')\n        self.model.train(False)\n        with torch.no_grad():\n            preds = []\n            for vis_imgs, vis_masks in self.vis_loader:\n                preds.append(self.model(vis_imgs.to(self.dev)).cpu().squeeze(1))\n            preds = torch.cat(preds).unsqueeze(1)\n            if self.vis_predictions is None:\n                self.vis_predictions = preds\n            else:\n                self.vis_predictions = torch.cat([self.vis_predictions, preds], dim=1)\n        (self.log_dir / 'tile_predictions').mkdir(exist_ok=True)\n        for i, tile in enumerate(self.vis_names):\n            filename = self.log_dir / 'tile_predictions' / f'{tile}.jpg'\n            showexample(\n                self.vis_loader.dataset[i], self.vis_predictions[i], filename, self.data_sources, step=self.board_idx\n            )\n        plt.close('all')\n\n    def setup_lr_scheduler(self):\n        # Scheduler\n        if 'learning_rate_scheduler' not in self.config.keys():\n            print('running without learning rate scheduler')\n            self.scheduler = None\n        elif self.config['learning_rate_scheduler'] == 'StepLR':\n            if 'lr_step_size' not in self.config.keys():\n                step_size = 10\n            else:\n                step_size = self.config['lr_step_size']\n            if 'lr_gamma' not in self.config.keys():\n                gamma = 0.1\n            else:\n                gamma = self.config['lr_gamma']\n            self.scheduler = torch.optim.lr_scheduler.StepLR(self.opt, step_size=step_size, gamma=gamma)\n            print(f\"running with 'StepLR' learning rate scheduler with step_size = {step_size} and gamma = {gamma}\")\n        elif self.config['learning_rate_scheduler'] == 'ExponentialLR':\n            if 'lr_gamma' not in self.config.keys():\n                gamma = 0.9\n            else:\n                gamma = self.config['lr_gamma']\n            self.scheduler = torch.optim.lr_scheduler.ExponentialLR(self.opt, gamma=gamma)\n            print(f\"running with 'ExponentialLR' learning rate scheduler with gamma = {gamma}\")\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/scripts/train/#src.thaw_slump_segmentation.scripts.train.Engine.DATA_ROOT","title":"<code>DATA_ROOT = data_dir</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/scripts/train/#src.thaw_slump_segmentation.scripts.train.Engine.board_idx","title":"<code>board_idx = 0</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/scripts/train/#src.thaw_slump_segmentation.scripts.train.Engine.checkpoints","title":"<code>checkpoints = self.log_dir / 'checkpoints'</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/scripts/train/#src.thaw_slump_segmentation.scripts.train.Engine.config","title":"<code>config = yaml.load(config.open(), Loader=yaml_custom.SaneYAMLLoader)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/scripts/train/#src.thaw_slump_segmentation.scripts.train.Engine.data_sources","title":"<code>data_sources = DataSources(self.config['data_sources'])</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/scripts/train/#src.thaw_slump_segmentation.scripts.train.Engine.dataset_cache","title":"<code>dataset_cache = {}</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/scripts/train/#src.thaw_slump_segmentation.scripts.train.Engine.dev","title":"<code>dev = torch.device('cpu') if not torch.cuda.is_available() else torch.device('cuda')</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/scripts/train/#src.thaw_slump_segmentation.scripts.train.Engine.epoch","title":"<code>epoch = 0</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/scripts/train/#src.thaw_slump_segmentation.scripts.train.Engine.learning_rate","title":"<code>learning_rate = self.config['learning_rate']</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/scripts/train/#src.thaw_slump_segmentation.scripts.train.Engine.log_dir","title":"<code>log_dir = Path(log_dir) / log_dir_name</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/scripts/train/#src.thaw_slump_segmentation.scripts.train.Engine.logger","title":"<code>logger = get_logger('train')</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/scripts/train/#src.thaw_slump_segmentation.scripts.train.Engine.model","title":"<code>model = torch.compile(self.model, mode='max-autotune', fullgraph=True)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/scripts/train/#src.thaw_slump_segmentation.scripts.train.Engine.opt","title":"<code>opt = torch.optim.AdamW(self.model.parameters(), lr=self.learning_rate)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/scripts/train/#src.thaw_slump_segmentation.scripts.train.Engine.vis_predictions","title":"<code>vis_predictions = None</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/scripts/train/#src.thaw_slump_segmentation.scripts.train.Engine.__init__","title":"<code>__init__(config, data_dir, name, log_dir, resume, summary, wandb_project, wandb_name)</code>","text":"Source code in <code>src/thaw_slump_segmentation/scripts/train.py</code> <pre><code>def __init__(\n    self,\n    config: Path,\n    data_dir: Path,\n    name: str,\n    log_dir: Path,\n    resume: str,\n    summary: bool,\n    wandb_project: str,\n    wandb_name: str,\n):\n    self.config = yaml.load(config.open(), Loader=yaml_custom.SaneYAMLLoader)\n    self.DATA_ROOT = data_dir\n    # Logging setup\n    timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n    if name:\n        log_dir_name = f'{name}_{timestamp}'\n    else:\n        log_dir_name = timestamp\n    self.log_dir = Path(log_dir) / log_dir_name\n    self.log_dir.mkdir(exist_ok=False)\n\n    init_logging(self.log_dir / 'train.log')\n    self.logger = get_logger('train')\n\n    self.data_sources = DataSources(self.config['data_sources'])\n    self.config['model']['input_channels'] = sum(src.channels for src in self.data_sources)\n\n    m = self.config['model']\n    self.model = create_model(\n        arch=m['architecture'],\n        encoder_name=m['encoder'],\n        encoder_weights=None if m['encoder_weights'] == 'random' else m['encoder_weights'],\n        classes=1,\n        in_channels=m['input_channels'],\n    )\n\n    # make parallel\n    self.model = nn.DataParallel(self.model)\n\n    if resume:\n        self.config['resume'] = resume\n\n    if 'resume' in self.config and self.config['resume']:\n        checkpoint = Path(self.config['resume'])\n        if not checkpoint.exists():\n            raise ValueError(f\"There is no Checkpoint at {self.config['resume']} to resume from!\")\n        if checkpoint.is_dir():\n            # Load last checkpoint in run dir\n            ckpt_nums = [int(ckpt.stem) for ckpt in checkpoint.glob('checkpoints/*.pt')]\n            last_ckpt = max(ckpt_nums)\n            self.config['resume'] = str(checkpoint / 'checkpoints' / f'{last_ckpt:02d}.pt')\n        self.logger.info(f\"Resuming training from checkpoint {self.config['resume']}\")\n        self.model.load_state_dict(torch.load(self.config['resume']))\n\n    self.dev = torch.device('cpu') if not torch.cuda.is_available() else torch.device('cuda')\n    self.logger.info(f'Training on {self.dev} device')\n\n    self.model = self.model.to(self.dev)\n    self.model = torch.compile(self.model, mode='max-autotune', fullgraph=True)\n\n    self.learning_rate = self.config['learning_rate']\n    self.opt = torch.optim.AdamW(self.model.parameters(), lr=self.learning_rate)\n    self.setup_lr_scheduler()\n\n    self.board_idx = 0\n    self.epoch = 0\n\n    if summary:\n        from torchsummary import summary\n\n        summary(self.model, [(self.config['model']['input_channels'], 256, 256)])\n        sys.exit(0)\n\n    self.dataset_cache = {}\n\n    self.vis_predictions = None\n    self.vis_loader, self.vis_names = get_vis_loader(\n        self.config['visualization_tiles'],\n        batch_size=self.config['batch_size'],\n        data_sources=self.data_sources,\n        data_root=self.DATA_ROOT,\n    )\n\n    # Write the config YML to the run-folder\n    self.config['run_info'] = {\n        'timestamp': timestamp,\n        'git_head': subprocess.check_output(['git', 'describe'], encoding='utf8').strip(),\n    }\n    with open(self.log_dir / 'config.yml', 'w') as f:\n        yaml.dump(self.config, f)\n\n    self.checkpoints = self.log_dir / 'checkpoints'\n    self.checkpoints.mkdir()\n\n    # Weights and Biases initialization\n    print('wandb project:', wandb_project)\n    print('wandb name:', wandb_name)\n    print('config:', self.config)\n    print('entity:', 'ml4earth')\n    wandb.init(project=wandb_project, name=wandb_name, config=self.config, entity='ingmarnitze_team')\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/scripts/train/#src.thaw_slump_segmentation.scripts.train.Engine.get_dataloader","title":"<code>get_dataloader(name)</code>","text":"Source code in <code>src/thaw_slump_segmentation/scripts/train.py</code> <pre><code>def get_dataloader(self, name):\n    if name in self.dataset_cache:\n        return self.dataset_cache[name]\n\n    if name in self.config['datasets']:\n        ds_config = self.config['datasets'][name]\n        if 'batch_size' not in ds_config:\n            ds_config['batch_size'] = self.config['batch_size']\n        ds_config['num_workers'] = self.config['data_threads']\n        ds_config['augment_types'] = self.config['datasets']\n        ds_config['data_sources'] = self.data_sources\n        ds_config['data_root'] = self.DATA_ROOT\n        self.dataset_cache[name] = get_loader(**ds_config)\n    else:\n        func, arg = re.search(r'(\\w+)\\((\\w+)\\)', name).groups()\n        if func == 'slump_tiles':\n            ds_config = self.config['datasets'][arg]\n            if 'batch_size' not in ds_config:\n                ds_config['batch_size'] = self.config['batch_size']\n            ds_config['num_workers'] = self.config['data_threads']\n            ds_config['data_sources'] = self.data_sources\n            ds_config['data_root'] = self.DATA_ROOT\n            self.dataset_cache[name] = get_slump_loader(**ds_config)\n\n    return self.dataset_cache[name]\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/scripts/train/#src.thaw_slump_segmentation.scripts.train.Engine.log_csv","title":"<code>log_csv(epoch_metrics, epoch_loss)</code>","text":"Source code in <code>src/thaw_slump_segmentation/scripts/train.py</code> <pre><code>def log_csv(self, epoch_metrics: dict, epoch_loss: int | dict):\n    # Log to CSV\n    logfile = self.log_dir / f'{self.current_key}.csv'\n\n    if isinstance(epoch_loss, dict):\n        logstr = (\n            f'{self.epoch},'\n            + ','.join(f'{val}' for key, val in epoch_metrics.items())\n            + ','\n            + ','.join(f'{val}' for key, val in epoch_loss.items())\n        )\n    else:\n        logstr = f'{self.epoch},' + ','.join(f'{val}' for key, val in epoch_metrics.items()) + f',{epoch_loss}'\n\n    if not logfile.exists():\n        # Print header upon first log print\n        header = 'Epoch,' + ','.join(f'{key}' for key, val in epoch_metrics.items()) + ',Loss'\n        with logfile.open('w') as f:\n            print(header, file=f)\n            print(logstr, file=f)\n    else:\n        with logfile.open('a') as f:\n            print(logstr, file=f)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/scripts/train/#src.thaw_slump_segmentation.scripts.train.Engine.log_images","title":"<code>log_images()</code>","text":"Source code in <code>src/thaw_slump_segmentation/scripts/train.py</code> <pre><code>def log_images(self):\n    self.logger.debug(f'Epoch {self.epoch} - Image Logging')\n    self.model.train(False)\n    with torch.no_grad():\n        preds = []\n        for vis_imgs, vis_masks in self.vis_loader:\n            preds.append(self.model(vis_imgs.to(self.dev)).cpu().squeeze(1))\n        preds = torch.cat(preds).unsqueeze(1)\n        if self.vis_predictions is None:\n            self.vis_predictions = preds\n        else:\n            self.vis_predictions = torch.cat([self.vis_predictions, preds], dim=1)\n    (self.log_dir / 'tile_predictions').mkdir(exist_ok=True)\n    for i, tile in enumerate(self.vis_names):\n        filename = self.log_dir / 'tile_predictions' / f'{tile}.jpg'\n        showexample(\n            self.vis_loader.dataset[i], self.vis_predictions[i], filename, self.data_sources, step=self.board_idx\n        )\n    plt.close('all')\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/scripts/train/#src.thaw_slump_segmentation.scripts.train.Engine.log_metrics","title":"<code>log_metrics()</code>","text":"Source code in <code>src/thaw_slump_segmentation/scripts/train.py</code> <pre><code>def log_metrics(self) -&gt; dict:\n    epoch_metrics = self.metrics[self.current_key].compute()\n    # We need to filter our ROC, PRC and Confusion Matrix from the metrics because they are not scalar metrics\n\n    scalar_epoch_metrics = {\n        k: v.item() for k, v in epoch_metrics.items() if isinstance(v, torch.Tensor) and v.numel() == 1\n    }\n\n    # Log to WandB\n    wandb.log(scalar_epoch_metrics, step=self.board_idx)\n\n    # Plot the metrics to disk\n    if self.phase_should_log_images:\n        scalar_epoch_metrics_tensor = {\n            k: v for k, v in epoch_metrics.items() if isinstance(v, torch.Tensor) and v.numel() == 1\n        }\n        self.metric_tracker[self.current_key].append(scalar_epoch_metrics_tensor)\n        fig, ax = plt.subplots(figsize=(20, 10))\n        self.metrics[self.current_key].plot(self.metric_tracker[self.current_key], together=True, ax=ax)\n        fig.savefig(self.metric_plot_dir / f'{self.current_key}_metrics.png')\n        fig.clear()\n        plt.close('all')\n\n    return scalar_epoch_metrics\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/scripts/train/#src.thaw_slump_segmentation.scripts.train.Engine.run","title":"<code>run()</code>","text":"Source code in <code>src/thaw_slump_segmentation/scripts/train.py</code> <pre><code>def run(self):\n    for phase in self.config['schedule']:\n        self.logger.info(f'Starting phase \"{phase[\"phase\"]}\"')\n\n        self.phase_should_log_images = phase.get('log_images', False)\n        self.setup_metrics(phase)\n        self.current_key = None\n        for epoch in range(phase['epochs']):\n            # Epoch setup\n            self.epoch = epoch\n            self.loss_function = create_loss(scoped_get('loss_function', phase, self.config)).to(self.dev)\n\n            for step in phase['steps']:\n                command, key = parse_step(step)\n                self.current_key = key\n\n                if command == 'train_on':\n                    # Training step\n                    data_loader = self.get_dataloader(key)\n                    self.train_epoch(data_loader)\n                elif command == 'validate_on':\n                    # Validation step\n                    data_loader = self.get_dataloader(key)\n                    self.val_epoch(data_loader)\n                elif command == 'log_images':\n                    self.logger.warn(\n                        \"Step 'log_images' is deprecated. Please use 'log_images' in the phase instead.\"\n                    )\n                    continue\n                else:\n                    raise ValueError(f\"Unknown command '{command}'\")\n\n                # Reset the metrics after each step\n                if self.current_key:\n                    self.metrics[self.current_key].reset()\n                    gc.collect()\n                    torch.cuda.empty_cache()\n\n            # Log images after each epoch\n            if self.phase_should_log_images:\n                self.log_images()\n\n            if self.scheduler:\n                print('before step:', self.scheduler.get_last_lr())\n                self.scheduler.step()\n                print('after step:', self.scheduler.get_last_lr())\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/scripts/train/#src.thaw_slump_segmentation.scripts.train.Engine.setup_lr_scheduler","title":"<code>setup_lr_scheduler()</code>","text":"Source code in <code>src/thaw_slump_segmentation/scripts/train.py</code> <pre><code>def setup_lr_scheduler(self):\n    # Scheduler\n    if 'learning_rate_scheduler' not in self.config.keys():\n        print('running without learning rate scheduler')\n        self.scheduler = None\n    elif self.config['learning_rate_scheduler'] == 'StepLR':\n        if 'lr_step_size' not in self.config.keys():\n            step_size = 10\n        else:\n            step_size = self.config['lr_step_size']\n        if 'lr_gamma' not in self.config.keys():\n            gamma = 0.1\n        else:\n            gamma = self.config['lr_gamma']\n        self.scheduler = torch.optim.lr_scheduler.StepLR(self.opt, step_size=step_size, gamma=gamma)\n        print(f\"running with 'StepLR' learning rate scheduler with step_size = {step_size} and gamma = {gamma}\")\n    elif self.config['learning_rate_scheduler'] == 'ExponentialLR':\n        if 'lr_gamma' not in self.config.keys():\n            gamma = 0.9\n        else:\n            gamma = self.config['lr_gamma']\n        self.scheduler = torch.optim.lr_scheduler.ExponentialLR(self.opt, gamma=gamma)\n        print(f\"running with 'ExponentialLR' learning rate scheduler with gamma = {gamma}\")\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/scripts/train/#src.thaw_slump_segmentation.scripts.train.Engine.setup_metrics","title":"<code>setup_metrics(phase)</code>","text":"Source code in <code>src/thaw_slump_segmentation/scripts/train.py</code> <pre><code>def setup_metrics(self, phase: dict):\n    # Setup Metrics for this phase\n    # Check if the phase has a log_images command -&gt; Relevant for memory management of metrics (PRC, ROC and Confusion Matrix)\n    nthresholds = (\n        100  # Make sure that the thresholds for the PRC-based metrics are equal to benefit from grouped computing\n    )\n    # Make sure that the matching args are the same for all instance metrics\n    matching_threshold = 0.5\n    matching_metric = 'iou'\n    boundary_dilation = 0.02\n\n    metrics = MetricCollection(\n        {\n            'Accuracy': Accuracy(task='binary', validate_args=False),\n            'Precision': Precision(task='binary', validate_args=False),\n            'Specificity': Specificity(task='binary', validate_args=False),\n            'Recall': Recall(task='binary', validate_args=False),\n            'F1Score': F1Score(task='binary', validate_args=False),\n            'JaccardIndex': JaccardIndex(task='binary', validate_args=False),\n            'AUROC': AUROC(task='binary', thresholds=nthresholds, validate_args=False),\n            'AveragePrecision': AveragePrecision(task='binary', thresholds=nthresholds, validate_args=False),\n            # Calibration errors: https://arxiv.org/abs/1909.10155, they take a lot of memory!\n            #'ExpectedCalibrationError': CalibrationError(task='binary', norm='l1'),\n            #'RootMeanSquaredCalibrationError': CalibrationError(task='binary', norm='l2'),\n            #'MaximumCalibrationError': CalibrationError(task='binary', norm='max'),\n            'CohenKappa': CohenKappa(task='binary', validate_args=False),\n            'HammingDistance': HammingDistance(task='binary', validate_args=False),\n            'HingeLoss': HingeLoss(task='binary', validate_args=False),\n            # MCC raised a weired error one time, skipping to be save\n            # 'MatthewsCorrCoef': MatthewsCorrCoef(task='binary'),\n        }\n    )\n    self.metrics = {}\n    self.rocs = {}\n    self.prcs = {}\n    self.confmats = {}\n    self.instance_prcs = {}\n    self.instance_confmats = {}\n\n    if self.phase_should_log_images:\n        self.metric_tracker = defaultdict(list)\n\n    for step in phase['steps']:\n        command, key = parse_step(step)\n        if not key:\n            continue\n\n        self.metrics[key] = metrics.clone(f'{key}/').to(self.dev)\n        if command == 'validate_on':\n            # We don't want to log the confusion matrix, PRC and ROC curve for every step in the training loop\n            # We assign them seperately to have easy access to them in the log_images phase but still benefit from the MetricCollection in terms of compute_groups and update, compute &amp; reset calls\n            self.rocs[key] = ROC(task='binary', thresholds=nthresholds, validate_args=False).to(self.dev)\n            self.prcs[key] = PrecisionRecallCurve(task='binary', thresholds=nthresholds, validate_args=False).to(\n                self.dev\n            )\n            self.confmats[key] = ConfusionMatrix(task='binary', normalize='true', validate_args=False).to(self.dev)\n            self.instance_prcs[key] = BinaryInstancePrecisionRecallCurve(\n                thresholds=nthresholds,\n                matching_threshold=matching_threshold,\n                matching_metric=matching_metric,\n                validate_args=False,\n            ).to(self.dev)\n            self.instance_confmats[key] = BinaryInstanceConfusionMatrix(\n                matching_threshold=matching_threshold, matching_metric=matching_metric, validate_args=False\n            ).to(self.dev)\n            self.metrics[key].add_metrics(\n                {\n                    'ROC': self.rocs[key],\n                    'PRC': self.prcs[key],\n                    'ConfusionMatrix': self.confmats[key],\n                    'Instance-PRC': self.instance_prcs[key],\n                    'Instance-ConfusionMatrix': self.instance_confmats[key],\n                }\n            )\n            # We don't want to log the instance metrics for every step in the training loop, because they are memory intensive (and not very useful for training)\n            self.metrics[key].add_metrics(\n                {\n                    'Instance-Accuracy': BinaryInstanceAccuracy(\n                        matching_threshold=matching_threshold, matching_metric=matching_metric, validate_args=False\n                    ).to(self.dev),\n                    'Instance-Precision': BinaryInstancePrecision(\n                        matching_threshold=matching_threshold, matching_metric=matching_metric, validate_args=False\n                    ).to(self.dev),\n                    'Instance-Recall': BinaryInstanceRecall(\n                        matching_threshold=matching_threshold, matching_metric=matching_metric, validate_args=False\n                    ).to(self.dev),\n                    'Instance-F1Score': BinaryInstanceF1Score(\n                        matching_threshold=matching_threshold, matching_metric=matching_metric, validate_args=False\n                    ).to(self.dev),\n                    'Instance-AveragePrecision': BinaryInstanceAveragePrecision(\n                        thresholds=nthresholds,\n                        matching_threshold=matching_threshold,\n                        matching_metric=matching_metric,\n                        validate_args=False,\n                    ).to(self.dev),\n                    'BoundaryIoU': BinaryBoundaryIoU(dilation=boundary_dilation, validate_args=False).to(self.dev),\n                }\n            )\n\n    # Create the plot directory\n    if self.phase_should_log_images:\n        phase_name = phase['phase'].lower()\n        self.metric_plot_dir = self.log_dir / f'metrics_plots_{phase_name}'\n        self.metric_plot_dir.mkdir(exist_ok=True)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/scripts/train/#src.thaw_slump_segmentation.scripts.train.Engine.train_epoch","title":"<code>train_epoch(train_loader)</code>","text":"Source code in <code>src/thaw_slump_segmentation/scripts/train.py</code> <pre><code>def train_epoch(self, train_loader):\n    self.logger.info(f'Epoch {self.epoch} - Training Started')\n    progress = tqdm(train_loader)\n    self.model.train(True)\n    epoch_loss = {\n        'Loss': [],\n        'Deep Supervision Loss': [],\n    }\n    for iteration, (img, target) in enumerate(progress):\n        self.board_idx += img.shape[0]\n\n        img = img.to(self.dev, torch.float)\n        target = target.to(self.dev, torch.long, non_blocking=True)\n\n        self.opt.zero_grad()\n        y_hat = self.model(img)\n\n        if isinstance(y_hat, (tuple, list)):\n            # Deep Supervision\n            deep_super_losses = [self.loss_function(pred.squeeze(1), target) for pred in y_hat]\n            y_hat = y_hat[0].squeeze(1)\n            loss = sum(deep_super_losses)\n            for dsl in deep_super_losses:\n                epoch_loss['Loss'].append(dsl.detach())\n            epoch_loss['Deep Supervision Loss'].append(loss.detach())\n        else:\n            loss = self.loss_function(y_hat, target)\n            epoch_loss['Loss'].append(loss.detach())\n\n        loss.backward()\n        self.opt.step()\n\n        with torch.no_grad():\n            self.metrics[self.current_key].update(y_hat.squeeze(1), target)\n\n    # Compute epochs loss and metrics\n    epoch_loss = {k: torch.stack(v).mean().item() for k, v in epoch_loss.items() if v}\n    wandb.log({'train/Loss': epoch_loss}, step=self.board_idx)\n    epoch_metrics = self.log_metrics()\n    self.logger.info(f'Epoch {self.epoch} - Loss {epoch_loss} Training Metrics: {epoch_metrics}')\n    self.log_csv(epoch_metrics, epoch_loss)\n\n    # Update progress bar\n    progress.set_postfix(epoch_metrics)\n\n    # Save model Checkpoint\n    torch.save(self.model.state_dict(), self.checkpoints / f'{self.epoch:02d}.pt')\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/scripts/train/#src.thaw_slump_segmentation.scripts.train.Engine.val_epoch","title":"<code>val_epoch(val_loader)</code>","text":"Source code in <code>src/thaw_slump_segmentation/scripts/train.py</code> <pre><code>def val_epoch(self, val_loader):\n    self.logger.info(f'Epoch {self.epoch} - Validation of {self.current_key} Started')\n    self.model.train(False)\n    with torch.no_grad():\n        epoch_loss = []\n        for iteration, (img, target) in enumerate(val_loader):\n            img = img.to(self.dev, torch.float)\n            target = target.to(self.dev, torch.long, non_blocking=True)\n            y_hat = self.model(img).squeeze(1)\n\n            loss = self.loss_function(y_hat, target)\n            epoch_loss.append(loss.detach())\n            self.metrics[self.current_key].update(y_hat, target)\n\n        # Compute epochs loss and metrics\n        epoch_loss = torch.stack(epoch_loss).mean().item()\n        epoch_metrics = self.log_metrics()\n    wandb.log({'val/Loss': epoch_loss}, step=self.board_idx)\n    self.logger.info(f'Epoch {self.epoch} - Loss {epoch_loss} Validation Metrics: {epoch_metrics}')\n    self.log_csv(epoch_metrics, epoch_loss)\n\n    # Plot roc, prc and confusion matrix to disk and wandb\n    fig_roc, _ = self.rocs[self.current_key].plot(score=True)\n    fig_prc, _ = self.prcs[self.current_key].plot(score=True)\n    fig_confmat, _ = self.confmats[self.current_key].plot(cmap='Blues')\n    fig_instance_prc, _ = self.instance_prcs[self.current_key].plot(score=True)\n    fig_instance_confmat, _ = self.instance_confmats[self.current_key].plot(cmap='Blues')\n\n    # We need to wrap the figures into a wandb.Image to save storage -&gt; Maybe we can find a better solution in the future, e.g. using plotly\n    wandb.log({f'{self.current_key}/ROC': wandb.Image(fig_roc)}, step=self.board_idx)\n    wandb.log({f'{self.current_key}/PRC': wandb.Image(fig_prc)}, step=self.board_idx)\n    wandb.log({f'{self.current_key}/Confusion Matrix': wandb.Image(fig_confmat)}, step=self.board_idx)\n    wandb.log({f'{self.current_key}/Instance-PRC': wandb.Image(fig_instance_prc)}, step=self.board_idx)\n    wandb.log(\n        {f'{self.current_key}/Instance-Confusion Matrix': wandb.Image(fig_instance_confmat)}, step=self.board_idx\n    )\n\n    if self.phase_should_log_images:\n        fig_roc.savefig(self.metric_plot_dir / f'{self.current_key}_roc_curve.png')\n        fig_prc.savefig(self.metric_plot_dir / f'{self.current_key}_precision_recall_curve.png')\n        fig_confmat.savefig(self.metric_plot_dir / f'{self.current_key}_confusion_matrix.png')\n        fig_instance_prc.savefig(self.metric_plot_dir / f'{self.current_key}_instance_precision_recall_curve.png')\n        fig_instance_confmat.savefig(self.metric_plot_dir / f'{self.current_key}_instance_confusion_matrix.png')\n    fig_roc.clear()\n    fig_prc.clear()\n    fig_confmat.clear()\n    fig_instance_prc.clear()\n    fig_instance_confmat.clear()\n    plt.close('all')\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/scripts/train/#src.thaw_slump_segmentation.scripts.train.parse_step","title":"<code>parse_step(step)</code>","text":"Source code in <code>src/thaw_slump_segmentation/scripts/train.py</code> <pre><code>def parse_step(step) -&gt; tuple[str, str | None]:\n    if isinstance(step, dict):\n        assert len(step) == 1\n        ((command, key),) = step.items()\n    else:\n        command, key = step, None\n    return command, key\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/scripts/train/#src.thaw_slump_segmentation.scripts.train.scoped_get","title":"<code>scoped_get(key, *scopestack)</code>","text":"Source code in <code>src/thaw_slump_segmentation/scripts/train.py</code> <pre><code>def scoped_get(key, *scopestack):\n    for scope in scopestack:\n        value = scope.get(key)\n        if value is not None:\n            return value\n    raise ValueError(f'Could not find \"{key}\" in any scope.')\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/scripts/train/#src.thaw_slump_segmentation.scripts.train.train","title":"<code>train(name, data_dir=Path('data'), log_dir=Path('logs'), config=Path('config.yml'), resume=None, summary=False, wandb_project='thaw-slump-segmentation', wandb_name=None)</code>","text":"<p>Training script</p> Source code in <code>src/thaw_slump_segmentation/scripts/train.py</code> <pre><code>def train(\n    name: Annotated[\n        str,\n        typer.Option(\n            '--name',\n            '-n',\n            prompt=True,\n            help='Give this run a name, so that it will be logged into logs/&lt;NAME&gt;_&lt;timestamp&gt;.',\n        ),\n    ],\n    data_dir: Annotated[Path, typer.Option('--data_dir', help='Path to data processing dir')] = Path('data'),\n    log_dir: Annotated[Path, typer.Option('--log_dir', help='Path to log dir')] = Path('logs'),\n    config: Annotated[Path, typer.Option('--config', '-c', help='Specify run config to use.')] = Path('config.yml'),\n    resume: Annotated[\n        str,\n        typer.Option(\n            '--resume',\n            '-r',\n            help='Resume from the specified checkpoint. Can be either a run-id (e.g. \"2020-06-29_18-12-03\") to select the last. Overrides the resume option in the config file if given.',\n        ),\n    ] = None,\n    summary: Annotated[bool, typer.Option('--summary', '-s', help='Only print model summary and return.')] = False,\n    wandb_project: Annotated[\n        str, typer.Option('--wandb_project', '-wp', help='Set a project name for weights and biases')\n    ] = 'thaw-slump-segmentation',\n    wandb_name: Annotated[\n        str, typer.Option('--wandb_name', '-wn', help='Set a run name for weights and biases')\n    ] = None,\n):\n    \"\"\"Training script\"\"\"\n    engine = Engine(config, data_dir, name, log_dir, resume, summary, wandb_project, wandb_name)\n    engine.run()\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/utils/data/","title":"Data","text":""},{"location":"reference/src/thaw_slump_segmentation/utils/data/#src.thaw_slump_segmentation.utils.data.Augment","title":"<code>Augment</code>","text":"<p>               Bases: <code>Dataset</code></p> Source code in <code>src/thaw_slump_segmentation/utils/data.py</code> <pre><code>class Augment(Dataset):\n    def __init__(self, dataset, augment_types=None):\n        self.dataset = dataset\n        if not augment_types:\n            self.augment_types = ['HorizontalFlip', 'VerticalFlip', 'Blur', 'RandomRotate90']\n        else:\n            self.augment_types = augment_types\n\n    def __getitem__(self, idx):\n        idx, (flipx, flipy, transpose) = self._augmented_idx_and_ops(idx)\n        base = self.dataset[idx]\n\n        # add Augmentation types\n        augment_list = []\n        if self.augment_types:\n            for aug_type in self.augment_types:\n                augment_list.append(getattr(A, aug_type)())\n        else:\n            return base\n        # scale data\n        transform = A.Compose(augment_list)\n\n        augmented = transform(image=np.array(base[0].permute(1,2,0)), mask=np.array(base[1]))\n        data = torch.from_numpy(np.ascontiguousarray(augmented['image']).transpose(2,0,1).copy())\n        mask = torch.from_numpy(np.ascontiguousarray(augmented['mask']).copy())\n\n        return (data, mask)\n\n    def _augmented_idx_and_ops(self, idx):\n        idx, carry = divmod(idx, 8)\n        carry, flipx = divmod(carry, 2)\n        transpose, flipy = divmod(carry, 2)\n\n        return idx, (flipx, flipy, transpose)\n\n    def _get_nth_tensor_raw(self, idx, n):\n        \"\"\"Hacky way of transparently accessing the underlying get_nth_tensor of a PTDataset\"\"\"\n        idx, ops = self._augmented_idx_and_ops(idx)\n        return self.dataset.get_nth_tensor(idx, n)\n\n    def __len__(self):\n        return len(self.dataset) * 8\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/utils/data/#src.thaw_slump_segmentation.utils.data.Augment.augment_types","title":"<code>augment_types = ['HorizontalFlip', 'VerticalFlip', 'Blur', 'RandomRotate90']</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/utils/data/#src.thaw_slump_segmentation.utils.data.Augment.dataset","title":"<code>dataset = dataset</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/utils/data/#src.thaw_slump_segmentation.utils.data.Augment.__getitem__","title":"<code>__getitem__(idx)</code>","text":"Source code in <code>src/thaw_slump_segmentation/utils/data.py</code> <pre><code>def __getitem__(self, idx):\n    idx, (flipx, flipy, transpose) = self._augmented_idx_and_ops(idx)\n    base = self.dataset[idx]\n\n    # add Augmentation types\n    augment_list = []\n    if self.augment_types:\n        for aug_type in self.augment_types:\n            augment_list.append(getattr(A, aug_type)())\n    else:\n        return base\n    # scale data\n    transform = A.Compose(augment_list)\n\n    augmented = transform(image=np.array(base[0].permute(1,2,0)), mask=np.array(base[1]))\n    data = torch.from_numpy(np.ascontiguousarray(augmented['image']).transpose(2,0,1).copy())\n    mask = torch.from_numpy(np.ascontiguousarray(augmented['mask']).copy())\n\n    return (data, mask)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/utils/data/#src.thaw_slump_segmentation.utils.data.Augment.__init__","title":"<code>__init__(dataset, augment_types=None)</code>","text":"Source code in <code>src/thaw_slump_segmentation/utils/data.py</code> <pre><code>def __init__(self, dataset, augment_types=None):\n    self.dataset = dataset\n    if not augment_types:\n        self.augment_types = ['HorizontalFlip', 'VerticalFlip', 'Blur', 'RandomRotate90']\n    else:\n        self.augment_types = augment_types\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/utils/data/#src.thaw_slump_segmentation.utils.data.Augment.__len__","title":"<code>__len__()</code>","text":"Source code in <code>src/thaw_slump_segmentation/utils/data.py</code> <pre><code>def __len__(self):\n    return len(self.dataset) * 8\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/utils/data/#src.thaw_slump_segmentation.utils.data.H5Dataset","title":"<code>H5Dataset</code>","text":"<p>               Bases: <code>Dataset</code></p> Source code in <code>src/thaw_slump_segmentation/utils/data.py</code> <pre><code>class H5Dataset(Dataset):\n    def __init__(self, dataset_path, data_sources):\n        super().__init__()\n        self.dataset_path = dataset_path\n        self.sources = [src.name for src in data_sources]\n        self.h5 = None\n        self.length = None\n\n    def assert_open(self):\n        # Needed for multi-threading to work\n        if self.h5 is None:\n            self.h5 = h5py.File(self.dataset_path, 'r',\n                rdcc_nbytes = 2*(1&lt;&lt;30), # 2 GiB\n                rdcc_nslots = 200003,\n            )\n\n    def close_fd(self):\n        # We need to close the h5 file descriptor before going multi-threaded\n        # to allow for pickling on windows\n        self.h5.close()\n        del self.h5 \n        self.h5 = None\n\n    def __getitem__(self, idx):\n        self.assert_open()\n        features = [self.h5[source][idx] for source in self.sources]\n        features = torch.from_numpy(np.concatenate(features, axis=0))\n\n        mask = torch.from_numpy(self.h5['mask'][idx])[0]\n        return features, mask\n\n    def __len__(self):\n        # Three branches for this, because __getitem__ should be the only\n        # operation to open the h5 in the long-term (otherwise, threading will fail on windows)\n        # 1. length was already determined -&gt; just use that\n        # 2. we don't know the length &amp; h5 is open -&gt; get length from open h5\n        # 3. we don't know the length &amp; h5 is closed -&gt; open h5, get length and CLOSE AGAIN!\n        if self.length is None:\n            if self.h5 is None:\n                self.assert_open()\n                self.length = self.h5['mask'].shape[0]\n                self.close_fd()\n            else:\n                self.length = self.h5['mask'].shape[0]\n        return self.length\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/utils/data/#src.thaw_slump_segmentation.utils.data.H5Dataset.dataset_path","title":"<code>dataset_path = dataset_path</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/utils/data/#src.thaw_slump_segmentation.utils.data.H5Dataset.h5","title":"<code>h5 = None</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/utils/data/#src.thaw_slump_segmentation.utils.data.H5Dataset.length","title":"<code>length = None</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/utils/data/#src.thaw_slump_segmentation.utils.data.H5Dataset.sources","title":"<code>sources = [src.name for src in data_sources]</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/utils/data/#src.thaw_slump_segmentation.utils.data.H5Dataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"Source code in <code>src/thaw_slump_segmentation/utils/data.py</code> <pre><code>def __getitem__(self, idx):\n    self.assert_open()\n    features = [self.h5[source][idx] for source in self.sources]\n    features = torch.from_numpy(np.concatenate(features, axis=0))\n\n    mask = torch.from_numpy(self.h5['mask'][idx])[0]\n    return features, mask\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/utils/data/#src.thaw_slump_segmentation.utils.data.H5Dataset.__init__","title":"<code>__init__(dataset_path, data_sources)</code>","text":"Source code in <code>src/thaw_slump_segmentation/utils/data.py</code> <pre><code>def __init__(self, dataset_path, data_sources):\n    super().__init__()\n    self.dataset_path = dataset_path\n    self.sources = [src.name for src in data_sources]\n    self.h5 = None\n    self.length = None\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/utils/data/#src.thaw_slump_segmentation.utils.data.H5Dataset.__len__","title":"<code>__len__()</code>","text":"Source code in <code>src/thaw_slump_segmentation/utils/data.py</code> <pre><code>def __len__(self):\n    # Three branches for this, because __getitem__ should be the only\n    # operation to open the h5 in the long-term (otherwise, threading will fail on windows)\n    # 1. length was already determined -&gt; just use that\n    # 2. we don't know the length &amp; h5 is open -&gt; get length from open h5\n    # 3. we don't know the length &amp; h5 is closed -&gt; open h5, get length and CLOSE AGAIN!\n    if self.length is None:\n        if self.h5 is None:\n            self.assert_open()\n            self.length = self.h5['mask'].shape[0]\n            self.close_fd()\n        else:\n            self.length = self.h5['mask'].shape[0]\n    return self.length\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/utils/data/#src.thaw_slump_segmentation.utils.data.H5Dataset.assert_open","title":"<code>assert_open()</code>","text":"Source code in <code>src/thaw_slump_segmentation/utils/data.py</code> <pre><code>def assert_open(self):\n    # Needed for multi-threading to work\n    if self.h5 is None:\n        self.h5 = h5py.File(self.dataset_path, 'r',\n            rdcc_nbytes = 2*(1&lt;&lt;30), # 2 GiB\n            rdcc_nslots = 200003,\n        )\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/utils/data/#src.thaw_slump_segmentation.utils.data.H5Dataset.close_fd","title":"<code>close_fd()</code>","text":"Source code in <code>src/thaw_slump_segmentation/utils/data.py</code> <pre><code>def close_fd(self):\n    # We need to close the h5 file descriptor before going multi-threaded\n    # to allow for pickling on windows\n    self.h5.close()\n    del self.h5 \n    self.h5 = None\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/utils/data/#src.thaw_slump_segmentation.utils.data.PTDataset","title":"<code>PTDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> Random access Dataset for datasets of pytorch tensors stored like this <p>data/images/file1.pt data/images/file1.pt ... data/masks/file1.pt data/masks/file2.pt ...</p> Source code in <code>src/thaw_slump_segmentation/utils/data.py</code> <pre><code>class PTDataset(Dataset):\n    \"\"\"\n    Random access Dataset for datasets of pytorch tensors stored like this:\n        data/images/file1.pt\n        data/images/file1.pt\n        ...\n        data/masks/file1.pt\n        data/masks/file2.pt\n        ...\n    \"\"\"\n    def __init__(self, root, parts, suffix='.pt'):\n        self.root = Path(root)\n        self.parts = parts\n\n        first = self.root / parts[0]\n        filenames = list(sorted([x.name for x in first.glob('*' + suffix)]))\n        self.index = [[self.root / p / x for p in parts] for x in filenames]\n\n    def __getitem__(self, idx):\n        files = self.index[idx]\n        data = [torch.load(f) for f in files]\n        return data\n\n    def get_nth_tensor(self, idx, n):\n        \"\"\"Loads just the n-th of the tensors belonging to idx, untransformed!\"\"\"\n        files = self.index[idx]\n        return torch.load(files[n])\n\n    def __len__(self):\n        return len(self.index)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/utils/data/#src.thaw_slump_segmentation.utils.data.PTDataset.index","title":"<code>index = [[self.root / p / x for p in parts] for x in filenames]</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/utils/data/#src.thaw_slump_segmentation.utils.data.PTDataset.parts","title":"<code>parts = parts</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/utils/data/#src.thaw_slump_segmentation.utils.data.PTDataset.root","title":"<code>root = Path(root)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/utils/data/#src.thaw_slump_segmentation.utils.data.PTDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"Source code in <code>src/thaw_slump_segmentation/utils/data.py</code> <pre><code>def __getitem__(self, idx):\n    files = self.index[idx]\n    data = [torch.load(f) for f in files]\n    return data\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/utils/data/#src.thaw_slump_segmentation.utils.data.PTDataset.__init__","title":"<code>__init__(root, parts, suffix='.pt')</code>","text":"Source code in <code>src/thaw_slump_segmentation/utils/data.py</code> <pre><code>def __init__(self, root, parts, suffix='.pt'):\n    self.root = Path(root)\n    self.parts = parts\n\n    first = self.root / parts[0]\n    filenames = list(sorted([x.name for x in first.glob('*' + suffix)]))\n    self.index = [[self.root / p / x for p in parts] for x in filenames]\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/utils/data/#src.thaw_slump_segmentation.utils.data.PTDataset.__len__","title":"<code>__len__()</code>","text":"Source code in <code>src/thaw_slump_segmentation/utils/data.py</code> <pre><code>def __len__(self):\n    return len(self.index)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/utils/data/#src.thaw_slump_segmentation.utils.data.PTDataset.get_nth_tensor","title":"<code>get_nth_tensor(idx, n)</code>","text":"<p>Loads just the n-th of the tensors belonging to idx, untransformed!</p> Source code in <code>src/thaw_slump_segmentation/utils/data.py</code> <pre><code>def get_nth_tensor(self, idx, n):\n    \"\"\"Loads just the n-th of the tensors belonging to idx, untransformed!\"\"\"\n    files = self.index[idx]\n    return torch.load(files[n])\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/utils/data/#src.thaw_slump_segmentation.utils.data.Scaling","title":"<code>Scaling</code>","text":"<p>Scales tensors by predefined normalization factors</p> Source code in <code>src/thaw_slump_segmentation/utils/data.py</code> <pre><code>class Scaling():\n    \"Scales tensors by predefined normalization factors\"\n    def __init__(self, normalize):\n        self.normalize = normalize\n        # TODO: Also allow for a shifting factor?\n\n    def __call__(self, sample):\n        sample = list(sample)\n        # Imagery is sample[0]\n        sample[0] = sample[0].to(torch.float) * self.normalize\n        return sample\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/utils/data/#src.thaw_slump_segmentation.utils.data.Scaling.normalize","title":"<code>normalize = normalize</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/utils/data/#src.thaw_slump_segmentation.utils.data.Scaling.__call__","title":"<code>__call__(sample)</code>","text":"Source code in <code>src/thaw_slump_segmentation/utils/data.py</code> <pre><code>def __call__(self, sample):\n    sample = list(sample)\n    # Imagery is sample[0]\n    sample[0] = sample[0].to(torch.float) * self.normalize\n    return sample\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/utils/data/#src.thaw_slump_segmentation.utils.data.Scaling.__init__","title":"<code>__init__(normalize)</code>","text":"Source code in <code>src/thaw_slump_segmentation/utils/data.py</code> <pre><code>def __init__(self, normalize):\n    self.normalize = normalize\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/utils/data/#src.thaw_slump_segmentation.utils.data.Transformed","title":"<code>Transformed</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Wrap a dataset and apply a given transformation to every sample (e.g. Scaling)</p> Source code in <code>src/thaw_slump_segmentation/utils/data.py</code> <pre><code>class Transformed(Dataset):\n    \"Wrap a dataset and apply a given transformation to every sample (e.g. Scaling)\"\n    def __init__(self, dataset, transform):\n        self.dataset = dataset\n        self.transform = transform\n\n    def __getitem__(self, idx):\n        sample = self.dataset[idx]\n        return self.transform(sample)\n\n    def __len__(self):\n        return len(self.dataset)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/utils/data/#src.thaw_slump_segmentation.utils.data.Transformed.dataset","title":"<code>dataset = dataset</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/utils/data/#src.thaw_slump_segmentation.utils.data.Transformed.transform","title":"<code>transform = transform</code>  <code>instance-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/utils/data/#src.thaw_slump_segmentation.utils.data.Transformed.__getitem__","title":"<code>__getitem__(idx)</code>","text":"Source code in <code>src/thaw_slump_segmentation/utils/data.py</code> <pre><code>def __getitem__(self, idx):\n    sample = self.dataset[idx]\n    return self.transform(sample)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/utils/data/#src.thaw_slump_segmentation.utils.data.Transformed.__init__","title":"<code>__init__(dataset, transform)</code>","text":"Source code in <code>src/thaw_slump_segmentation/utils/data.py</code> <pre><code>def __init__(self, dataset, transform):\n    self.dataset = dataset\n    self.transform = transform\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/utils/data/#src.thaw_slump_segmentation.utils.data.Transformed.__len__","title":"<code>__len__()</code>","text":"Source code in <code>src/thaw_slump_segmentation/utils/data.py</code> <pre><code>def __len__(self):\n    return len(self.dataset)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/utils/geometry/","title":"Geometry","text":""},{"location":"reference/src/thaw_slump_segmentation/utils/geometry/#src.thaw_slump_segmentation.utils.geometry.make_sdf","title":"<code>make_sdf(mask, truncation=16)</code>","text":"<p>Calculates a truncated signed distance field on the given binary mask</p> Source code in <code>src/thaw_slump_segmentation/utils/geometry.py</code> <pre><code>@jit(nopython=True, parallel=True)\ndef make_sdf(mask, truncation=16):\n    \"\"\"\n    Calculates a truncated signed distance field on the given binary mask\n    \"\"\"\n    _, H, W = mask.shape\n    sdf = np.zeros((1, H, W), np.float32)\n\n    for y in range(0, H):\n        for x in prange(0, W):\n            base = mask[0, y, x]\n\n            lo_y = max(0, y - truncation)\n            hi_y = min(H, y + 1 + truncation)  # exclusive\n            lo_x = max(0, x - truncation)\n            hi_x = min(W, x + 1 + truncation)  # exclusive\n\n            best = truncation * truncation\n            for y2 in range(lo_y, hi_y):\n                for x2 in range(lo_x, hi_x):\n                    if not base == mask[0, y2, x2]:\n                        dy = y - y2\n                        dx = x - x2\n                        best = min(best, dy * dy + dx * dx)\n\n            best = np.sqrt(best)\n            if not base:\n                best = -best\n            sdf[0, y, x] = best\n\n    return sdf / truncation\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/utils/images/","title":"Images","text":""},{"location":"reference/src/thaw_slump_segmentation/utils/images/#src.thaw_slump_segmentation.utils.images.extract_patches","title":"<code>extract_patches(images, patch_size=512, max_nodata=0.0, stride=None)</code>","text":"<p>Extracts corresponding patches from the given images. Use as e.g. extract_patches([image, mask]). Assumes the first image to be the source image for nodata-filtering.</p> Source code in <code>src/thaw_slump_segmentation/utils/images.py</code> <pre><code>def extract_patches(images, patch_size=512, max_nodata=0.0, stride=None):\n    \"\"\"\n    Extracts corresponding patches from the given images.\n    Use as e.g. extract_patches([image, mask]).\n    Assumes the first image to be the source image for nodata-filtering.\n    \"\"\"\n    if type(images) is not list:\n        images = [images]\n    if stride is None:\n        stride = patch_size // 2\n\n    base = images[0]\n    yvals = np.arange(0, base.shape[-2] - patch_size, stride)\n    xvals = np.arange(0, base.shape[-1] - patch_size, stride)\n\n    for y_i, y0 in enumerate(yvals):\n        y1 = y0 + patch_size\n        for x_i, x0 in enumerate(xvals):\n            x1 = x0 + patch_size\n            cutouts = [x[..., y0:y1, x0:x1] for x in images]\n            nodata = cutouts[0] == 0\n            while len(nodata.shape) &gt; 2:\n                nodata = nodata.all(axis=0)\n            if nodata.mean() &lt;= max_nodata:\n                yield [x_i, y_i, *cutouts]\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/utils/io/","title":"Io","text":""},{"location":"reference/src/thaw_slump_segmentation/utils/io/#src.thaw_slump_segmentation.utils.io.download","title":"<code>download(url, dst, **args)</code>","text":"<p>Downloads a file from a given url to a destination path. Resumes partial downloads and skips alrady downloaded files. Cf. https://gist.github.com/wy193777/0e2a4932e81afc6aa4c8f7a2984f34e2</p> <p>@param: <code>url</code> to download file @param: <code>dst</code> place to put the file @param: <code>args</code> additional keyword arguments for <code>requests.get</code></p> Source code in <code>src/thaw_slump_segmentation/utils/io.py</code> <pre><code>def download(url, dst, **args):\n    \"\"\"\n    Downloads a file from a given url to a destination path.\n    Resumes partial downloads and skips alrady downloaded files.\n    Cf. https://gist.github.com/wy193777/0e2a4932e81afc6aa4c8f7a2984f34e2\n\n    @param: `url` to download file\n    @param: `dst` place to put the file\n    @param: `args` additional keyword arguments for `requests.get`\n    \"\"\"\n    file_size = int(requests.head(url, **args).headers[\"content-length\"])\n    if os.path.exists(dst):\n        first_byte = os.path.getsize(dst)\n    else:\n        first_byte = 0\n    if first_byte &gt;= file_size:\n        return file_size\n    header = {\"Range\": \"bytes=%s-%s\" % (first_byte, file_size)}\n    pbar = tqdm(\n        total=file_size, initial=first_byte,\n        unit='B', unit_scale=True, desc=url.split('/')[-1])\n    req = requests.get(url, headers=header, stream=True, **args)\n    with(open(dst, 'ab')) as f:\n        for chunk in req.iter_content(chunk_size=1024):\n            if chunk:\n                f.write(chunk)\n                pbar.update(1024)\n    pbar.close()\n    return file_size\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/utils/logging/","title":"Logging","text":""},{"location":"reference/src/thaw_slump_segmentation/utils/logging/#src.thaw_slump_segmentation.utils.logging.ISO8601LoggingFormatter","title":"<code>ISO8601LoggingFormatter</code>","text":"<p>               Bases: <code>Formatter</code></p> Source code in <code>src/thaw_slump_segmentation/utils/logging.py</code> <pre><code>class ISO8601LoggingFormatter(logging.Formatter):\n    def __init__(self, format_string):\n        super().__init__(format_string)\n\n    def formatTime(self, record, datefmt=None):\n        dt = datetime.fromtimestamp(record.created).astimezone()\n        return dt.isoformat(timespec='milliseconds')\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/utils/logging/#src.thaw_slump_segmentation.utils.logging.ISO8601LoggingFormatter.__init__","title":"<code>__init__(format_string)</code>","text":"Source code in <code>src/thaw_slump_segmentation/utils/logging.py</code> <pre><code>def __init__(self, format_string):\n    super().__init__(format_string)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/utils/logging/#src.thaw_slump_segmentation.utils.logging.ISO8601LoggingFormatter.formatTime","title":"<code>formatTime(record, datefmt=None)</code>","text":"Source code in <code>src/thaw_slump_segmentation/utils/logging.py</code> <pre><code>def formatTime(self, record, datefmt=None):\n    dt = datetime.fromtimestamp(record.created).astimezone()\n    return dt.isoformat(timespec='milliseconds')\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/utils/logging/#src.thaw_slump_segmentation.utils.logging.get_logger","title":"<code>get_logger(name)</code>","text":"Source code in <code>src/thaw_slump_segmentation/utils/logging.py</code> <pre><code>def get_logger(name):\n    return logging.getLogger(f'uc2.{name}')\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/utils/logging/#src.thaw_slump_segmentation.utils.logging.init_logging","title":"<code>init_logging(log_file)</code>","text":"Source code in <code>src/thaw_slump_segmentation/utils/logging.py</code> <pre><code>def init_logging(log_file):\n    root_logger = logging.getLogger('uc2')\n    if not root_logger.handlers:\n        file_handler = logging.FileHandler(log_file)\n        file_handler.setLevel(logging.DEBUG)\n        stdout_handler = logging.StreamHandler()\n        stdout_handler.setLevel(logging.INFO)  # Hide DEBUG messages from terminal output\n        formatter = ISO8601LoggingFormatter('[%(asctime)s] %(name)s %(levelname)s: %(message)s')\n        file_handler.setFormatter(formatter)\n        stdout_handler.setFormatter(formatter)\n        root_logger.addHandler(file_handler)\n        root_logger.addHandler(stdout_handler)\n        root_logger.setLevel(logging.DEBUG)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/utils/logging/#src.thaw_slump_segmentation.utils.logging.log_run","title":"<code>log_run(shell_command, logger)</code>","text":"Source code in <code>src/thaw_slump_segmentation/utils/logging.py</code> <pre><code>def log_run(shell_command, logger):\n    proc = subprocess.run(shell_command, shell=True, capture_output=True)\n    logger.info(f'Called `{proc.args}`')\n    if proc.stdout:\n        stdout = proc.stdout.decode('utf8').strip()\n        for line in stdout.split('\\n'):\n            logger.info(f'&gt; {line}')\n    if proc.stderr:\n        stderr = proc.stderr.decode('utf8').strip()\n        for line in stderr.split('\\n'):\n            logger.error(f'&gt; {line}')\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/utils/plot_info/","title":"Plot info","text":""},{"location":"reference/src/thaw_slump_segmentation/utils/plot_info/#src.thaw_slump_segmentation.utils.plot_info.FLATUI","title":"<code>FLATUI = {'Turquoise': (0.10196078431372549, 0.7372549019607844, 0.611764705882353), 'Emerald': (0.1803921568627451, 0.8, 0.44313725490196076), 'Peter River': (0.20392156862745098, 0.596078431372549, 0.8588235294117647), 'Amethyst': (0.6078431372549019, 0.34901960784313724, 0.7137254901960784), 'Wet Asphalt': (0.20392156862745098, 0.28627450980392155, 0.3686274509803922), 'Green Sea': (0.08627450980392157, 0.6274509803921569, 0.5215686274509804), 'Nephritis': (0.15294117647058825, 0.6823529411764706, 0.3764705882352941), 'Belize Hole': (0.1607843137254902, 0.5019607843137255, 0.7254901960784313), 'Wisteria': (0.5568627450980392, 0.26666666666666666, 0.6784313725490196), 'Midnight Blue': (0.17254901960784313, 0.24313725490196078, 0.3137254901960784), 'Sun Flower': (0.9450980392156862, 0.7686274509803922, 0.058823529411764705), 'Carrot': (0.9019607843137255, 0.49411764705882355, 0.13333333333333333), 'Alizarin': (0.9058823529411765, 0.2980392156862745, 0.23529411764705882), 'Clouds': (0.9254901960784314, 0.9411764705882353, 0.9450980392156862), 'Concrete': (0.5843137254901961, 0.6470588235294118, 0.6509803921568628), 'Orange': (0.9529411764705882, 0.611764705882353, 0.07058823529411765), 'Pumpkin': (0.8274509803921568, 0.32941176470588235, 0.0), 'Pomegranate': (0.7529411764705882, 0.2235294117647059, 0.16862745098039217), 'Silver': (0.7411764705882353, 0.7647058823529411, 0.7803921568627451), 'Asbestos': (0.4980392156862745, 0.5490196078431373, 0.5529411764705883)}</code>  <code>module-attribute</code>","text":""},{"location":"reference/src/thaw_slump_segmentation/utils/plot_info/#src.thaw_slump_segmentation.utils.plot_info.flatui_cmap","title":"<code>flatui_cmap(*colors)</code>","text":"Source code in <code>src/thaw_slump_segmentation/utils/plot_info.py</code> <pre><code>def flatui_cmap(*colors):\n    ts = np.linspace(0, 1, len(colors))\n\n    segmentdata = dict(\n        red=[[t, FLATUI[col][0], FLATUI[col][0]] for col, t in zip(colors, ts)],\n        green=[[t, FLATUI[col][1], FLATUI[col][1]] for col, t in zip(colors, ts)],\n        blue=[[t, FLATUI[col][2], FLATUI[col][2]] for col, t in zip(colors, ts)],\n    )\n\n    return LinearSegmentedColormap('flatui', segmentdata=segmentdata, N=256)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/utils/plot_info/#src.thaw_slump_segmentation.utils.plot_info.get_channel_offset","title":"<code>get_channel_offset(data_sources, target_source)</code>","text":"Source code in <code>src/thaw_slump_segmentation/utils/plot_info.py</code> <pre><code>def get_channel_offset(data_sources, target_source):\n    offset = 0\n    for src in data_sources:\n        if src.name == target_source:\n            break\n        offset += src.channels\n    return offset\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/utils/plot_info/#src.thaw_slump_segmentation.utils.plot_info.imageize","title":"<code>imageize(tensor)</code>","text":"Source code in <code>src/thaw_slump_segmentation/utils/plot_info.py</code> <pre><code>def imageize(tensor):\n    return np.clip(tensor.cpu().numpy().transpose(1, 2, 0), 0, 1)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/utils/plot_info/#src.thaw_slump_segmentation.utils.plot_info.plot_metrics","title":"<code>plot_metrics(train_metrics, val_metrics, outdir='.')</code>","text":"Source code in <code>src/thaw_slump_segmentation/utils/plot_info.py</code> <pre><code>def plot_metrics(train_metrics, val_metrics, outdir='.'):\n    metrics = (set(train_metrics) | set(val_metrics)) - set(['step', 'epoch'])\n    for metric in metrics:\n        outfile = os.path.join(outdir, f'{metric}.png')\n        ax = plt.subplot()\n        fig = ax.get_figure()\n\n        if metric in train_metrics:\n            ax.plot(train_metrics['epoch'], train_metrics[metric], c='C0', label='Train')\n        if metric in val_metrics:\n            ax.plot(val_metrics['epoch'], val_metrics[metric], c='C1', label='Val')\n        ax.set_xlabel('Epoch')\n        ax.get_xaxis().set_major_locator(MaxNLocator(integer=True))\n        ax.set_ylabel(metric)\n        ax.legend()\n        ax.grid()\n        fig.savefig(outfile, bbox_inches='tight')\n        plt.close()\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/utils/plot_info/#src.thaw_slump_segmentation.utils.plot_info.plot_precision_recall","title":"<code>plot_precision_recall(train_metrics, val_metrics, outdir='.')</code>","text":"Source code in <code>src/thaw_slump_segmentation/utils/plot_info.py</code> <pre><code>def plot_precision_recall(train_metrics, val_metrics, outdir='.'):\n    fig = plt.figure(figsize=(10, 3))\n    ax1, ax2 = fig.subplots(1, 2)\n\n    ax1.plot(train_metrics['epoch'], train_metrics['Precision'])\n    ax1.plot(train_metrics['epoch'], train_metrics['Recall'])\n    ax1.set_title('Train')\n    ax2.plot(val_metrics['epoch'], val_metrics['Precision'])\n    ax2.plot(val_metrics['epoch'], val_metrics['Recall'])\n    ax2.set_title('Val')\n\n    for ax in [ax1, ax2]:\n        ax.set_xlabel('Epoch')\n        ax.get_xaxis().set_major_locator(MaxNLocator(integer=True))\n        ax.legend(['Precision', 'Recall'])\n        ax.grid()\n\n    fig.tight_layout()\n\n    outfile = os.path.join(outdir, 'precision_recall.png')\n    fig.savefig(outfile)\n    fig.clear()\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/utils/plot_info/#src.thaw_slump_segmentation.utils.plot_info.read_metrics_file","title":"<code>read_metrics_file(file_path)</code>","text":"Source code in <code>src/thaw_slump_segmentation/utils/plot_info.py</code> <pre><code>def read_metrics_file(file_path):\n    with open(file_path) as src:\n        lines = src.readlines()\n    data = []\n    for line in lines:\n        if line.startswith('Phase'):\n            continue\n        epoch, remain = line.replace(' ', '').replace('\\n', '').split('-')\n        val_type = remain.split(',')[0].split(':')[0].strip()\n        vals = remain.replace('Train:', '').replace('Val:', '').split(',')\n        acc_vals = [float(v.split(':')[1]) for v in vals]\n\n        data.append([int(epoch.replace('Epoch', '')), str(val_type), *acc_vals])\n\n    df = pd.DataFrame(columns=['epoch', 'val_type', 'accuracy', 'precision', 'recall', 'f1', 'iou', 'loss'], data=data)\n    return df\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/utils/plot_info/#src.thaw_slump_segmentation.utils.plot_info.showexample","title":"<code>showexample(data, preds, filename, data_sources, step)</code>","text":"Source code in <code>src/thaw_slump_segmentation/utils/plot_info.py</code> <pre><code>def showexample(data, preds, filename, data_sources, step):\n    # First plot\n    ROWS = 6\n    m = 0.02\n    gridspec_kw = dict(left=m, right=1 - m, top=1 - m, bottom=m, hspace=0.12, wspace=m)\n    N = 1 + int(np.ceil(len(preds) / ROWS))\n    fig, ax = plt.subplots(ROWS, N, figsize=(3 * N, 3 * ROWS), gridspec_kw=gridspec_kw)\n    ax = ax.T.reshape(-1)\n\n    heatmap_args = dict(cmap=plt.cm.Greys_r, vmin=0, vmax=1)\n\n    img, target = data\n    img = img.to(torch.float)\n\n    # Clear all axes\n    for axis in ax:\n        axis.imshow(np.ones([1, 1, 3]))\n        axis.axis('off')\n\n    ds_names = set(src.name for src in data_sources)\n    if 'planet' in ds_names:\n        offset = get_channel_offset(data_sources, 'planet')\n        b, g, r, nir = np.arange(4) + offset\n        bgnir = imageize(img[[nir, b, g]])\n        ax[0].imshow(bgnir)\n        ax[0].set_title('NIR-R-G')\n    if 'ndvi' in ds_names:\n        c = get_channel_offset(data_sources, 'ndvi')\n        ndvi = img[[c, c, c]].cpu().numpy()\n        ax[1].imshow(ndvi[0], cmap=plt.cm.RdYlGn, vmin=0, vmax=1)\n        ax[1].set_title('NDVI')\n    if 'tcvis' in ds_names:\n        offset = get_channel_offset(data_sources, 'tcvis')\n        r, g, b = np.arange(3) + offset\n        tcvis = imageize(img[[r, g, b]])\n        ax[2].imshow(tcvis)\n        ax[2].set_title('TCVIS')\n    if 'relative_elevation' in ds_names:\n        c = get_channel_offset(data_sources, 'relative_elevation')\n        dem = img[[c, c, c]].cpu().numpy()\n        ax[3].imshow(np.clip(dem[0], 0, 1), cmap='RdBu', vmin=0, vmax=1)\n        ax[3].set_title('DEM')\n    if 'slope' in ds_names:\n        c = get_channel_offset(data_sources, 'slope')\n        # TODO: loading predominantly nan values\n        slope = img[[c, c, c]].cpu().numpy()\n        ax[4].imshow(np.clip(slope[0], 0, 1), cmap='Reds', vmin=0, vmax=1)\n        ax[4].set_title('Slope')\n\n    ax[5].imshow(target, **heatmap_args)\n    ax[5].set_title('Target')\n\n    for i, pred in enumerate(preds):\n        p = torch.sigmoid(pred).cpu()\n        ax[i + ROWS].imshow(p, **heatmap_args)\n        ax[i + ROWS].set_title(f'Epoch {i + 1} Prediction')\n\n    filename.parent.mkdir(exist_ok=True)\n    plt.savefig(filename, bbox_inches='tight')\n    plt.close()\n    fig, ax = plt.subplots(1, 3, figsize=(9, 4), gridspec_kw=gridspec_kw)\n    if 'planet' in ds_names:\n        offset = get_channel_offset(data_sources, 'planet')\n        b, g, r, nir = np.arange(4) + offset\n        bgnir = imageize(img[[nir, b, g]])\n        ax[0].imshow(bgnir)\n        ax[0].set_title('NIR-R-G')\n    ax[1].imshow(target.cpu(), **heatmap_args)\n    ax[1].set_title('Ground Truth')\n\n    pred = torch.sigmoid(preds[-1])\n    ax[2].imshow(pred, **heatmap_args)\n    ax[2].set_title('Prediction')\n    for axis in ax:\n        axis.axis('off')\n    wandb.log({filename.stem: wandb.Image(fig)}, step=step)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/utils/plot_statistics/","title":"Plot statistics","text":""},{"location":"reference/src/thaw_slump_segmentation/utils/plot_statistics/#src.thaw_slump_segmentation.utils.plot_statistics.channel_histograms","title":"<code>channel_histograms(data)</code>","text":"Source code in <code>src/thaw_slump_segmentation/utils/plot_statistics.py</code> <pre><code>def channel_histograms(data):\n    data = data.cpu().numpy()\n    C = data.shape[1]\n    COLS = 4\n    ROWS = int(math.ceil(C / COLS))\n    fig, ax = plt.subplots(ROWS, COLS, figsize=(3*COLS, 3*ROWS))\n    ax = ax.reshape(-1)\n    for i in range(C):\n        ax[i].hist(data[:, i].reshape(-1))\n    plt.show()\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/utils/stats/","title":"Stats","text":""},{"location":"reference/src/thaw_slump_segmentation/utils/stats/#src.thaw_slump_segmentation.utils.stats.focal_loss_with_logits","title":"<code>focal_loss_with_logits(y_hat_log, y, gamma=2)</code>","text":"Source code in <code>src/thaw_slump_segmentation/utils/stats.py</code> <pre><code>def focal_loss_with_logits(y_hat_log, y, gamma=2):\n    log0 = F.logsigmoid(-y_hat_log)\n    log1 = F.logsigmoid(y_hat_log)\n\n    gamma0 = torch.pow(torch.abs(1 - y - torch.exp(log0)), gamma)\n    gamma1 = torch.pow(torch.abs(y - torch.exp(log1)), gamma)\n\n    return torch.mean(-(1 - y) * gamma0 * log0 - y * gamma1 * log1)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/utils/stats/#src.thaw_slump_segmentation.utils.stats.gamma_nll","title":"<code>gamma_nll(alpha, beta, target)</code>","text":"<p>NLL for a Gamma(alpha, beta) distributed RV.</p> Source code in <code>src/thaw_slump_segmentation/utils/stats.py</code> <pre><code>def gamma_nll(alpha, beta, target):\n    \"\"\"\n    NLL for a Gamma(alpha, beta) distributed RV.\n    \"\"\"\n    # Likelihood     = \u03b2^\u03b1 / \u0393(\u03b1) x^(\u03b1-1) e^(-\u03b2x)\n    # Log-Likelihood = \u03b1 log(\u03b2) - log(\u0393(\u03b1)) + (\u03b1-1) log(x) - \u03b2x\\\n    ll = alpha * torch.log(beta) \\\n        - torch.lgamma(alpha) \\\n        + (alpha - 1) * torch.log(target) \\\n        - beta * target\n    return -torch.mean(ll)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/utils/stats/#src.thaw_slump_segmentation.utils.stats.gamma_pdf","title":"<code>gamma_pdf(alpha, beta, x)</code>","text":"<p>PDF for a Gamma(alpha, beta) distributed RV.</p> Source code in <code>src/thaw_slump_segmentation/utils/stats.py</code> <pre><code>def gamma_pdf(alpha, beta, x):\n    \"\"\"\n    PDF for a Gamma(alpha, beta) distributed RV.\n    \"\"\"\n    return torch.pow(beta, alpha) / torch.exp(torch.lgamma(alpha)) \\\n        * torch.pow(x, alpha - 1) * torch.exp(-beta * x)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/utils/stats/#src.thaw_slump_segmentation.utils.stats.lp_gamma_nll","title":"<code>lp_gamma_nll(log_alpha, log_beta, target)</code>","text":"<p>Negative log-likelihood of a log-parametrized gamma distribution</p> Source code in <code>src/thaw_slump_segmentation/utils/stats.py</code> <pre><code>def lp_gamma_nll(log_alpha, log_beta, target):\n    \"\"\"\n    Negative log-likelihood of a log-parametrized gamma distribution\n    \"\"\"\n    # Log-Likelihood = \u03b1 log(\u03b2) - log(\u0393(\u03b1)) + (\u03b1-1) log(x) - \u03b2x\\\n    alpha = torch.exp(log_alpha)\n    beta = torch.exp(log_beta)\n    ll = alpha * log_beta \\\n        - torch.lgamma(alpha) \\\n        + (alpha - 1) * torch.log(target + 1e-3) \\\n        - beta * target\n    return -torch.mean(ll)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/utils/stats/#src.thaw_slump_segmentation.utils.stats.lp_gamma_pdf","title":"<code>lp_gamma_pdf(log_alpha, log_beta, x)</code>","text":"<p>PDF of a log-parametrized gamma distribution</p> Source code in <code>src/thaw_slump_segmentation/utils/stats.py</code> <pre><code>def lp_gamma_pdf(log_alpha, log_beta, x):\n    \"\"\"\n    PDF of a log-parametrized gamma distribution\n    \"\"\"\n    alpha = torch.exp(log_alpha)\n    beta = torch.exp(log_beta)\n\n    return torch.pow(beta, alpha) / torch.exp(torch.lgamma(alpha)) \\\n        * torch.pow(x, alpha - 1) * torch.exp(-beta * x)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/utils/yaml_custom/","title":"Yaml custom","text":""},{"location":"reference/src/thaw_slump_segmentation/utils/yaml_custom/#src.thaw_slump_segmentation.utils.yaml_custom.HesitantIntResolver","title":"<code>HesitantIntResolver</code>","text":"<p>               Bases: <code>BaseResolver</code></p> <p>A YAML value resolver that won't parse any tokens containint underscores as ints, because that is not what we want!</p> Source code in <code>src/thaw_slump_segmentation/utils/yaml_custom.py</code> <pre><code>class HesitantIntResolver(BaseResolver):\n    \"\"\"A YAML value resolver that won't parse any tokens containint underscores\n    as ints, because that is not what we want!\"\"\"\n    pass\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/utils/yaml_custom/#src.thaw_slump_segmentation.utils.yaml_custom.SaneYAMLLoader","title":"<code>SaneYAMLLoader</code>","text":"<p>               Bases: <code>Reader</code>, <code>Scanner</code>, <code>Parser</code>, <code>Composer</code>, <code>SafeConstructor</code>, <code>HesitantIntResolver</code></p> Source code in <code>src/thaw_slump_segmentation/utils/yaml_custom.py</code> <pre><code>class SaneYAMLLoader(Reader, Scanner, Parser, Composer, SafeConstructor, HesitantIntResolver):\n    def __init__(self, stream):\n        Reader.__init__(self, stream)\n        Scanner.__init__(self)\n        Parser.__init__(self)\n        Composer.__init__(self)\n        SafeConstructor.__init__(self)\n        HesitantIntResolver.__init__(self)\n</code></pre>"},{"location":"reference/src/thaw_slump_segmentation/utils/yaml_custom/#src.thaw_slump_segmentation.utils.yaml_custom.SaneYAMLLoader.__init__","title":"<code>__init__(stream)</code>","text":"Source code in <code>src/thaw_slump_segmentation/utils/yaml_custom.py</code> <pre><code>def __init__(self, stream):\n    Reader.__init__(self, stream)\n    Scanner.__init__(self)\n    Parser.__init__(self)\n    Composer.__init__(self)\n    SafeConstructor.__init__(self)\n    HesitantIntResolver.__init__(self)\n</code></pre>"}]}